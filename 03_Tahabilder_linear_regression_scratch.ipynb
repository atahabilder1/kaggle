{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Your Own Linear Regression from Scratch\n",
    "\n",
    "**Author:** Anik Tahabilder  \n",
    "**Project:** 3 of 22 - Kaggle ML Portfolio  \n",
    "**Dataset:** Student Performance  \n",
    "**Difficulty:** 5/10 | **Learning Value:** 9/10\n",
    "\n",
    "---\n",
    "\n",
    "## Why Build from Scratch?\n",
    "\n",
    "Using `sklearn.linear_model.LinearRegression` is just one line of code. But do you actually understand what's happening under the hood?\n",
    "\n",
    "In this notebook, we'll:\n",
    "- **Derive the math** behind linear regression\n",
    "- **Implement everything using only NumPy** (no sklearn for the model)\n",
    "- **Visualize** how the algorithm learns\n",
    "- **Apply it** to predict student performance based on study habits\n",
    "- **Verify** our implementation matches sklearn\n",
    "\n",
    "By the end, you'll truly understand:\n",
    "- What a cost function is and why we use Mean Squared Error\n",
    "- How gradient descent finds optimal parameters\n",
    "- Why feature scaling matters\n",
    "- The closed-form solution (Normal Equation)\n",
    "\n",
    "---\n",
    "\n",
    "## The Problem: Predicting Student Performance\n",
    "\n",
    "We have data about students including:\n",
    "- Hours studied\n",
    "- Previous scores\n",
    "- Sleep hours\n",
    "- Practice papers completed\n",
    "\n",
    "**Goal:** Predict the **Performance Index** (final score) using linear regression.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: What is Linear Regression?](#part1)\n",
    "2. [Part 2: Load and Explore the Data](#part2)\n",
    "3. [Part 3: The Cost Function](#part3)\n",
    "4. [Part 4: Gradient Descent - The Learning Algorithm](#part4)\n",
    "5. [Part 5: Implementation from Scratch](#part5)\n",
    "6. [Part 6: The Normal Equation (Closed-Form Solution)](#part6)\n",
    "7. [Part 7: Feature Scaling - Why It Matters](#part7)\n",
    "8. [Part 8: Training on Student Performance Data](#part8)\n",
    "9. [Part 9: Evaluation Metrics from Scratch](#part9)\n",
    "10. [Part 10: Verification Against sklearn](#part10)\n",
    "11. [Part 11: Summary and Key Takeaways](#part11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part1'></a>\n",
    "# Part 1: What is Linear Regression?\n",
    "\n",
    "## The Intuition\n",
    "\n",
    "Linear regression is the **simplest supervised learning algorithm**. Given input features, it predicts a continuous output by fitting a straight line (or hyperplane) through the data.\n",
    "\n",
    "### Real-World Examples:\n",
    "- Predicting house prices based on square footage\n",
    "- Estimating salary based on years of experience\n",
    "- **Predicting student scores based on study hours** (our problem!)\n",
    "\n",
    "## The Mathematical Model\n",
    "\n",
    "### Simple Linear Regression (One Feature)\n",
    "\n",
    "The equation of a line:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}$ = predicted value (output)\n",
    "- $x$ = input feature\n",
    "- $w_0$ = **bias** (y-intercept) - where the line crosses the y-axis\n",
    "- $w_1$ = **weight** (slope) - how much y changes when x increases by 1\n",
    "\n",
    "### Multiple Linear Regression (Multiple Features)\n",
    "\n",
    "With multiple features:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n$$\n",
    "\n",
    "For our student data:\n",
    "$$\\text{Performance} = w_0 + w_1 \\cdot \\text{Hours} + w_2 \\cdot \\text{PrevScores} + w_3 \\cdot \\text{Sleep} + ...$$\n",
    "\n",
    "### Matrix Notation (Compact Form)\n",
    "\n",
    "We can write this more elegantly using matrices:\n",
    "\n",
    "$$\\hat{y} = X \\cdot w$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the **feature matrix** of shape (m, n+1) - m samples, n features + 1 bias column\n",
    "- $w$ is the **weight vector** of shape (n+1, 1)\n",
    "\n",
    "**The Bias Trick:** We add a column of 1s to X so that $w_0 \\cdot 1 = w_0$ (the bias term).\n",
    "\n",
    "## The Goal\n",
    "\n",
    "Find the values of $w_0, w_1, ..., w_n$ that make our predictions $\\hat{y}$ as close as possible to the actual values $y$.\n",
    "\n",
    "But how do we measure \"close\"? That's where the **cost function** comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Setup - Only NumPy and Matplotlib (NO sklearn for the model)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For nice plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete! Using only NumPy for the model.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part2'></a>\n",
    "# Part 2: Load and Explore the Data\n",
    "\n",
    "Let's load the Student Performance dataset and understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Student_Performance.csv')\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(f\"\\nWe have {df.shape[0]} students and {df.shape[1]} features.\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"First 10 rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print(\"Dataset Info:\")\n",
    "print(\"=\"*50)\n",
    "df.info()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nNo missing values - great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the features\n",
    "print(\"Feature Analysis:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Feature':<35} {'Type':<15} {'Range'}\")\n",
    "print(\"-\"*60)\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        unique = df[col].unique()\n",
    "        print(f\"{col:<35} {'Categorical':<15} {unique}\")\n",
    "    else:\n",
    "        print(f\"{col:<35} {'Numeric':<15} [{df[col].min():.1f} - {df[col].max():.1f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TARGET: Performance Index (what we want to predict)\")\n",
    "print(\"FEATURES: All other columns (what we use to predict)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Distribution of target variable\n",
    "axes[0, 0].hist(df['Performance Index'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Performance Index')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Performance Index (Target)')\n",
    "\n",
    "# Scatter plots of numeric features vs target\n",
    "numeric_features = ['Hours Studied', 'Previous Scores', 'Sleep Hours', 'Sample Question Papers Practiced']\n",
    "\n",
    "for i, feature in enumerate(numeric_features):\n",
    "    row, col = (i + 1) // 3, (i + 1) % 3\n",
    "    axes[row, col].scatter(df[feature], df['Performance Index'], alpha=0.5, edgecolors='black', linewidth=0.3)\n",
    "    axes[row, col].set_xlabel(feature)\n",
    "    axes[row, col].set_ylabel('Performance Index')\n",
    "    axes[row, col].set_title(f'{feature} vs Performance')\n",
    "\n",
    "# Extracurricular activities boxplot\n",
    "df.boxplot(column='Performance Index', by='Extracurricular Activities', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Performance by Extracurricular Activities')\n",
    "axes[1, 2].set_xlabel('Extracurricular Activities')\n",
    "\n",
    "plt.suptitle('Exploratory Data Analysis', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Hours Studied shows STRONG positive correlation with Performance\")\n",
    "print(\"- Previous Scores also shows positive correlation\")\n",
    "print(\"- These are the features linear regression will learn from!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start simple: predict Performance using just ONE feature (Hours Studied)\n",
    "# This makes it easier to visualize and understand the math\n",
    "\n",
    "X_simple = df['Hours Studied'].values.reshape(-1, 1)\n",
    "y = df['Performance Index'].values.reshape(-1, 1)\n",
    "\n",
    "print(\"Simple Linear Regression Setup:\")\n",
    "print(f\"X (Hours Studied) shape: {X_simple.shape}\")\n",
    "print(f\"y (Performance Index) shape: {y.shape}\")\n",
    "print(f\"\\nWe'll predict Performance = w0 + w1 * Hours_Studied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the simple regression problem\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_simple, y, alpha=0.5, edgecolors='black', linewidth=0.3, label='Students')\n",
    "plt.xlabel('Hours Studied')\n",
    "plt.ylabel('Performance Index')\n",
    "plt.title('Can We Find a Line That Predicts Performance from Study Hours?')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Our goal: Find the BEST line through this data!\")\n",
    "print(\"The line equation: Performance = w0 + w1 * Hours_Studied\")\n",
    "print(\"\\nWe need to find optimal values for w0 (bias) and w1 (slope).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part3'></a>\n",
    "# Part 3: The Cost Function\n",
    "\n",
    "## What is a Cost Function?\n",
    "\n",
    "A **cost function** (also called loss function or objective function) measures how wrong our predictions are. It gives us a single number that represents the \"badness\" of our model.\n",
    "\n",
    "**Goal:** Find parameters that **minimize** the cost function.\n",
    "\n",
    "## Mean Squared Error (MSE)\n",
    "\n",
    "The most common cost function for linear regression is **Mean Squared Error**:\n",
    "\n",
    "$$J(w) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "Where:\n",
    "- $m$ = number of training examples\n",
    "- $\\hat{y}^{(i)}$ = prediction for example $i$\n",
    "- $y^{(i)}$ = actual value for example $i$\n",
    "- The $\\frac{1}{2}$ is a convenience factor (makes the derivative cleaner)\n",
    "\n",
    "### Why Squared?\n",
    "\n",
    "1. **Penalizes large errors more** - An error of 10 contributes 100 to the cost (vs. 10 for absolute error)\n",
    "2. **Differentiable everywhere** - Unlike absolute error which has a kink at 0\n",
    "3. **Convex function** - Has a single global minimum (no local minima to get stuck in)\n",
    "\n",
    "### In Matrix Form\n",
    "\n",
    "$$J(w) = \\frac{1}{2m} (Xw - y)^T (Xw - y)$$\n",
    "\n",
    "This is equivalent to the summation but uses matrix operations (faster in NumPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w):\n",
    "    \"\"\"\n",
    "    Compute the Mean Squared Error cost function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy array of shape (m, n+1)\n",
    "        Feature matrix with bias column\n",
    "    y : numpy array of shape (m, 1)\n",
    "        Target values\n",
    "    w : numpy array of shape (n+1, 1)\n",
    "        Weight vector (including bias)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cost : float\n",
    "        The MSE cost\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    predictions = X.dot(w)           # y_hat = Xw\n",
    "    errors = predictions - y          # (y_hat - y)\n",
    "    cost = (1 / (2 * m)) * np.sum(errors ** 2)  # (1/2m) * sum((y_hat - y)^2)\n",
    "    return cost\n",
    "\n",
    "print(\"Cost function defined!\")\n",
    "print(\"\\nFormula: J(w) = (1/2m) * sum((y_hat - y)^2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias column to X (column of 1s)\n",
    "m = len(X_simple)\n",
    "X_b = np.c_[np.ones((m, 1)), X_simple]  # X_b = [1, x] for each sample\n",
    "\n",
    "print(\"Original X shape:\", X_simple.shape)\n",
    "print(\"X with bias column shape:\", X_b.shape)\n",
    "print(\"\\nFirst 5 rows of X_b:\")\n",
    "print(X_b[:5])\n",
    "print(\"\\n^ First column is all 1s (for the bias term)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how cost changes with different weight values\n",
    "# Try some random weights and see their costs\n",
    "\n",
    "test_weights = [\n",
    "    np.array([[0], [0]]),      # Zero weights (predicting 0 for everyone)\n",
    "    np.array([[20], [5]]),     # Random guess\n",
    "    np.array([[0], [10]]),     # Reasonable guess (10 points per hour)\n",
    "    np.array([[-10], [12]]),   # Another guess\n",
    "]\n",
    "\n",
    "print(\"Cost for different weight values:\")\n",
    "print(\"=\"*50)\n",
    "for w in test_weights:\n",
    "    cost = compute_cost(X_b, y, w)\n",
    "    print(f\"w0={w[0,0]:>6.1f}, w1={w[1,0]:>5.1f} --> Cost = {cost:>10.2f}\")\n",
    "\n",
    "print(\"\\nWe need to find the weights that give the LOWEST cost!\")\n",
    "print(\"That's where gradient descent comes in...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cost function surface (3D plot)\n",
    "# This shows how cost changes for different combinations of w0 and w1\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create a grid of w0 and w1 values\n",
    "w0_vals = np.linspace(-30, 30, 100)\n",
    "w1_vals = np.linspace(-5, 20, 100)\n",
    "W0, W1 = np.meshgrid(w0_vals, w1_vals)\n",
    "\n",
    "# Compute cost for each combination\n",
    "costs = np.zeros_like(W0)\n",
    "for i in range(len(w0_vals)):\n",
    "    for j in range(len(w1_vals)):\n",
    "        w = np.array([[W0[i, j]], [W1[i, j]]])\n",
    "        costs[i, j] = compute_cost(X_b, y, w)\n",
    "\n",
    "# 3D Surface Plot\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Surface plot\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(W0, W1, costs, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('w0 (bias)')\n",
    "ax1.set_ylabel('w1 (slope)')\n",
    "ax1.set_zlabel('Cost J(w)')\n",
    "ax1.set_title('Cost Function Surface')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(W0, W1, costs, levels=50, cmap='viridis')\n",
    "ax2.set_xlabel('w0 (bias)')\n",
    "ax2.set_ylabel('w1 (slope)')\n",
    "ax2.set_title('Cost Function Contour Plot')\n",
    "plt.colorbar(contour, ax=ax2, label='Cost')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe cost surface is a BOWL shape (convex).\")\n",
    "print(\"There's only ONE minimum - at the bottom of the bowl.\")\n",
    "print(\"Gradient descent will 'roll down' this bowl to find the minimum!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part4'></a>\n",
    "# Part 4: Gradient Descent - The Learning Algorithm\n",
    "\n",
    "## The Intuition\n",
    "\n",
    "Imagine you're blindfolded on a hilly terrain and want to reach the lowest point. What would you do?\n",
    "\n",
    "1. **Feel the slope** under your feet (compute the gradient)\n",
    "2. **Take a step downhill** (update parameters in opposite direction of gradient)\n",
    "3. **Repeat** until you reach the bottom (convergence)\n",
    "\n",
    "This is exactly what gradient descent does!\n",
    "\n",
    "## The Math\n",
    "\n",
    "### What is a Gradient?\n",
    "\n",
    "The **gradient** is a vector of partial derivatives. It points in the direction of steepest INCREASE of a function.\n",
    "\n",
    "To minimize the function, we move in the **opposite** direction of the gradient.\n",
    "\n",
    "### Deriving the Gradient for MSE\n",
    "\n",
    "Our cost function:\n",
    "$$J(w) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "Where $\\hat{y}^{(i)} = w_0 + w_1 x^{(i)}$\n",
    "\n",
    "Taking partial derivatives:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_0} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_1} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) \\cdot x^{(i)}$$\n",
    "\n",
    "### In Matrix Form (Much Cleaner!)\n",
    "\n",
    "$$\\nabla J(w) = \\frac{1}{m} X^T (Xw - y)$$\n",
    "\n",
    "This single line computes all partial derivatives at once!\n",
    "\n",
    "### The Update Rule\n",
    "\n",
    "$$w := w - \\alpha \\cdot \\nabla J(w)$$\n",
    "\n",
    "Or expanded:\n",
    "\n",
    "$$w := w - \\frac{\\alpha}{m} X^T (Xw - y)$$\n",
    "\n",
    "Where $\\alpha$ is the **learning rate** - how big of a step we take.\n",
    "\n",
    "## Learning Rate: The Critical Hyperparameter\n",
    "\n",
    "- **Too small**: Very slow convergence (takes forever)\n",
    "- **Too large**: Overshoots the minimum (may never converge or diverge)\n",
    "- **Just right**: Fast convergence to the minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the cost function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy array of shape (m, n+1)\n",
    "        Feature matrix with bias column\n",
    "    y : numpy array of shape (m, 1)\n",
    "        Target values\n",
    "    w : numpy array of shape (n+1, 1)\n",
    "        Current weight vector\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    gradient : numpy array of shape (n+1, 1)\n",
    "        The gradient vector\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    predictions = X.dot(w)            # y_hat = Xw\n",
    "    errors = predictions - y          # (y_hat - y)\n",
    "    gradient = (1 / m) * X.T.dot(errors)  # (1/m) * X^T(Xw - y)\n",
    "    return gradient\n",
    "\n",
    "print(\"Gradient function defined!\")\n",
    "print(\"\\nFormula: gradient = (1/m) * X^T(Xw - y)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the gradient looks like at different points\n",
    "\n",
    "test_points = [\n",
    "    np.array([[0], [0]]),      # Far from optimum\n",
    "    np.array([[10], [5]]),     # Getting closer\n",
    "    np.array([[-5], [10]]),    # Near optimum maybe?\n",
    "]\n",
    "\n",
    "print(\"Gradient at different points:\")\n",
    "print(\"=\"*60)\n",
    "for w in test_points:\n",
    "    grad = compute_gradient(X_b, y, w)\n",
    "    cost = compute_cost(X_b, y, w)\n",
    "    print(f\"w = [{w[0,0]:>6.1f}, {w[1,0]:>5.1f}]\")\n",
    "    print(f\"  Gradient = [{grad[0,0]:>8.2f}, {grad[1,0]:>8.2f}]\")\n",
    "    print(f\"  Cost = {cost:.2f}\")\n",
    "    print()\n",
    "\n",
    "print(\"The gradient tells us which direction to move to decrease cost!\")\n",
    "print(\"Negative gradient = we should increase that weight.\")\n",
    "print(\"Positive gradient = we should decrease that weight.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part5'></a>\n",
    "# Part 5: Implementation from Scratch\n",
    "\n",
    "Now let's put it all together into a complete linear regression class using only NumPy!\n",
    "\n",
    "## The Algorithm\n",
    "\n",
    "```\n",
    "1. Initialize weights randomly (or with zeros)\n",
    "2. Repeat for n_iterations:\n",
    "   a. Compute predictions: y_hat = Xw\n",
    "   b. Compute cost: J(w)\n",
    "   c. Compute gradient: gradient(J)\n",
    "   d. Update weights: w = w - alpha * gradient\n",
    "   e. Store cost for plotting\n",
    "3. Return learned weights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionScratch:\n",
    "    \"\"\"\n",
    "    Linear Regression implemented from scratch using only NumPy.\n",
    "    \n",
    "    This implementation uses batch gradient descent to find optimal weights.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate : float, default=0.01\n",
    "        The step size for gradient descent\n",
    "    n_iterations : int, default=1000\n",
    "        Number of iterations to run gradient descent\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    weights : numpy array\n",
    "        Learned weights (including bias)\n",
    "    cost_history : list\n",
    "        Cost at each iteration (for plotting)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.cost_history = []\n",
    "    \n",
    "    def _add_bias(self, X):\n",
    "        \"\"\"Add a column of 1s to X for the bias term.\"\"\"\n",
    "        return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    \n",
    "    def _compute_cost(self, X, y):\n",
    "        \"\"\"Compute Mean Squared Error cost.\"\"\"\n",
    "        m = len(y)\n",
    "        predictions = X.dot(self.weights)\n",
    "        cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)\n",
    "        return cost\n",
    "    \n",
    "    def _compute_gradient(self, X, y):\n",
    "        \"\"\"Compute gradient of cost function.\"\"\"\n",
    "        m = len(y)\n",
    "        predictions = X.dot(self.weights)\n",
    "        gradient = (1 / m) * X.T.dot(predictions - y)\n",
    "        return gradient\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy array of shape (m, n)\n",
    "            Training features\n",
    "        y : numpy array of shape (m, 1)\n",
    "            Target values\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self for method chaining\n",
    "        \"\"\"\n",
    "        # Add bias column\n",
    "        X_b = self._add_bias(X)\n",
    "        m, n = X_b.shape\n",
    "        \n",
    "        # Initialize weights to zeros\n",
    "        self.weights = np.zeros((n, 1))\n",
    "        self.cost_history = []\n",
    "        \n",
    "        # Gradient descent loop\n",
    "        for i in range(self.n_iterations):\n",
    "            # Compute and store cost\n",
    "            cost = self._compute_cost(X_b, y)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Compute gradient\n",
    "            gradient = self._compute_gradient(X_b, y)\n",
    "            \n",
    "            # Update weights\n",
    "            self.weights = self.weights - self.learning_rate * gradient\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using learned weights.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy array of shape (m, n)\n",
    "            Features to predict\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : numpy array of shape (m, 1)\n",
    "            Predicted values\n",
    "        \"\"\"\n",
    "        X_b = self._add_bias(X)\n",
    "        return X_b.dot(self.weights)\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"Return bias and weights separately.\"\"\"\n",
    "        return {\n",
    "            'bias': self.weights[0, 0],\n",
    "            'coefficients': self.weights[1:].flatten()\n",
    "        }\n",
    "\n",
    "print(\"LinearRegressionScratch class defined!\")\n",
    "print(\"\\nMethods:\")\n",
    "print(\"  - fit(X, y): Train the model using gradient descent\")\n",
    "print(\"  - predict(X): Make predictions\")\n",
    "print(\"  - get_params(): Get learned bias and coefficients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our model on the simple case (Hours Studied -> Performance)\n",
    "model_simple = LinearRegressionScratch(learning_rate=0.01, n_iterations=1000)\n",
    "model_simple.fit(X_simple, y)\n",
    "\n",
    "# Get learned parameters\n",
    "params = model_simple.get_params()\n",
    "\n",
    "print(\"Training complete! (Simple Linear Regression)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Learned equation: Performance = {params['bias']:.2f} + {params['coefficients'][0]:.2f} * Hours_Studied\")\n",
    "print()\n",
    "print(f\"Interpretation:\")\n",
    "print(f\"  - Base performance (0 hours studied): {params['bias']:.2f}\")\n",
    "print(f\"  - Each additional hour of study adds: {params['coefficients'][0]:.2f} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learning process\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Cost over iterations\n",
    "axes[0].plot(model_simple.cost_history)\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Cost')\n",
    "axes[0].set_title('Cost Function Over Training')\n",
    "\n",
    "# Plot 2: Data with learned line\n",
    "axes[1].scatter(X_simple, y, alpha=0.5, edgecolors='black', linewidth=0.3, label='Students')\n",
    "X_line = np.array([[0], [9]])\n",
    "y_pred_line = model_simple.predict(X_line)\n",
    "axes[1].plot(X_line, y_pred_line, 'r-', linewidth=2, \n",
    "             label=f'Learned: y = {params[\"bias\"]:.1f} + {params[\"coefficients\"][0]:.1f}x')\n",
    "axes[1].set_xlabel('Hours Studied')\n",
    "axes[1].set_ylabel('Performance Index')\n",
    "axes[1].set_title('Learned Regression Line')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: Cost decreases as the model learns\")\n",
    "print(\"Right: The red line is what our model learned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient descent path on the cost surface\n",
    "\n",
    "# Track weight history during training\n",
    "weights = np.zeros((2, 1))\n",
    "weight_history = [weights.copy()]\n",
    "\n",
    "for i in range(100):  # Fewer iterations to see the path clearly\n",
    "    gradient = (1 / m) * X_b.T.dot(X_b.dot(weights) - y)\n",
    "    weights = weights - 0.01 * gradient\n",
    "    weight_history.append(weights.copy())\n",
    "\n",
    "weight_history = np.array(weight_history).squeeze()\n",
    "\n",
    "# Plot the path on contour\n",
    "plt.figure(figsize=(10, 8))\n",
    "contour = plt.contour(W0, W1, costs, levels=30, cmap='viridis')\n",
    "plt.colorbar(contour, label='Cost')\n",
    "\n",
    "# Plot the gradient descent path\n",
    "plt.plot(weight_history[:, 0], weight_history[:, 1], 'ro-', markersize=3, linewidth=1, label='Gradient Descent Path')\n",
    "plt.plot(weight_history[0, 0], weight_history[0, 1], 'go', markersize=12, label='Start (0, 0)')\n",
    "plt.plot(weight_history[-1, 0], weight_history[-1, 1], 'r*', markersize=15, label='End (Optimal)')\n",
    "\n",
    "plt.xlabel('w0 (bias)')\n",
    "plt.ylabel('w1 (slope)')\n",
    "plt.title('Gradient Descent Path on Cost Surface')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Watch how gradient descent navigates the cost surface!\")\n",
    "print(\"Starting from (0,0), it follows the steepest descent to reach the minimum.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part6'></a>\n",
    "# Part 6: The Normal Equation (Closed-Form Solution)\n",
    "\n",
    "## An Alternative to Gradient Descent\n",
    "\n",
    "Instead of iteratively searching for the minimum, we can solve for it directly using calculus!\n",
    "\n",
    "## Deriving the Normal Equation\n",
    "\n",
    "To find the minimum, we set the gradient to zero:\n",
    "\n",
    "$$\\nabla J(w) = 0$$\n",
    "\n",
    "$$\\frac{1}{m} X^T (Xw - y) = 0$$\n",
    "\n",
    "$$X^T Xw = X^T y$$\n",
    "\n",
    "$$w = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "This is the **Normal Equation** - it gives us the optimal weights directly!\n",
    "\n",
    "## Gradient Descent vs Normal Equation\n",
    "\n",
    "| Aspect | Gradient Descent | Normal Equation |\n",
    "|--------|-----------------|------------------|\n",
    "| **Iterative** | Yes | No |\n",
    "| **Learning rate** | Needs tuning | Not needed |\n",
    "| **Complexity** | O(kn²) | O(n³) for matrix inversion |\n",
    "| **Large n (features)** | Works well | Slow (matrix inversion) |\n",
    "| **Large m (samples)** | Works well | Works well |\n",
    "\n",
    "**Rule of thumb:** Use Normal Equation when n < 10,000 features. Use Gradient Descent for larger feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(X, y):\n",
    "    \"\"\"\n",
    "    Compute optimal weights using the Normal Equation.\n",
    "    \n",
    "    Formula: w = (X^T X)^(-1) X^T y\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy array of shape (m, n)\n",
    "        Feature matrix (without bias column)\n",
    "    y : numpy array of shape (m, 1)\n",
    "        Target values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w : numpy array of shape (n+1, 1)\n",
    "        Optimal weights (including bias)\n",
    "    \"\"\"\n",
    "    # Add bias column\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    \n",
    "    # Normal equation: w = (X^T X)^(-1) X^T y\n",
    "    w = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "    \n",
    "    return w\n",
    "\n",
    "# Solve using normal equation\n",
    "w_normal = normal_equation(X_simple, y)\n",
    "\n",
    "print(\"Normal Equation Solution\")\n",
    "print(\"=\"*50)\n",
    "print(f\"w = (X^T X)^(-1) X^T y\")\n",
    "print()\n",
    "print(f\"Computed bias (w0): {w_normal[0, 0]:.6f}\")\n",
    "print(f\"Computed slope (w1): {w_normal[1, 0]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: Gradient Descent vs Normal Equation\n",
    "\n",
    "# Gradient Descent result\n",
    "gd_params = model_simple.get_params()\n",
    "\n",
    "print(\"Comparison: Gradient Descent vs Normal Equation\")\n",
    "print(\"=\"*55)\n",
    "print(f\"{'Method':<20} {'Bias (w0)':<15} {'Slope (w1)':<15}\")\n",
    "print(\"-\"*55)\n",
    "print(f\"{'Gradient Descent':<20} {gd_params['bias']:<15.6f} {gd_params['coefficients'][0]:<15.6f}\")\n",
    "print(f\"{'Normal Equation':<20} {w_normal[0,0]:<15.6f} {w_normal[1,0]:<15.6f}\")\n",
    "print()\n",
    "print(\"Both methods find essentially the same solution!\")\n",
    "print(\"(Small differences are due to gradient descent not fully converging)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part7'></a>\n",
    "# Part 7: Feature Scaling - Why It Matters\n",
    "\n",
    "## The Problem\n",
    "\n",
    "When features have very different scales, gradient descent can behave poorly:\n",
    "\n",
    "- The cost surface becomes elongated (elliptical instead of circular)\n",
    "- Gradient descent takes a zigzag path\n",
    "- Convergence is slow or may not happen\n",
    "\n",
    "## The Solution: Feature Scaling\n",
    "\n",
    "### Standardization (Z-score normalization)\n",
    "\n",
    "$$x' = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ = mean of the feature\n",
    "- $\\sigma$ = standard deviation of the feature\n",
    "\n",
    "Result: Features have mean = 0 and std = 1\n",
    "\n",
    "## When to Scale\n",
    "\n",
    "- **Gradient Descent**: Always scale features!\n",
    "- **Normal Equation**: Not strictly necessary (but doesn't hurt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement standardization from scratch\n",
    "\n",
    "def standardize(X):\n",
    "    \"\"\"\n",
    "    Standardize features to have mean=0 and std=1.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy array of shape (m, n)\n",
    "        Features to standardize\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_scaled : numpy array of shape (m, n)\n",
    "        Standardized features\n",
    "    mu : numpy array of shape (n,)\n",
    "        Mean of each feature\n",
    "    sigma : numpy array of shape (n,)\n",
    "        Std of each feature\n",
    "    \"\"\"\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "    X_scaled = (X - mu) / sigma\n",
    "    return X_scaled, mu, sigma\n",
    "\n",
    "print(\"Standardization function defined!\")\n",
    "print(\"\\nFormula: x' = (x - mean) / std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare multiple features for the full model\n",
    "# We'll encode 'Extracurricular Activities' as 1/0\n",
    "\n",
    "df_encoded = df.copy()\n",
    "df_encoded['Extracurricular Activities'] = (df['Extracurricular Activities'] == 'Yes').astype(int)\n",
    "\n",
    "# Select features and target\n",
    "feature_cols = ['Hours Studied', 'Previous Scores', 'Sleep Hours', \n",
    "                'Sample Question Papers Practiced', 'Extracurricular Activities']\n",
    "X_multi = df_encoded[feature_cols].values\n",
    "y = df_encoded['Performance Index'].values.reshape(-1, 1)\n",
    "\n",
    "print(\"Multiple features prepared:\")\n",
    "print(f\"X shape: {X_multi.shape} (5 features)\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature scales before standardization\n",
    "print(\"Feature Scales BEFORE Standardization:\")\n",
    "print(\"=\"*60)\n",
    "for i, col in enumerate(feature_cols):\n",
    "    print(f\"{col:<40} mean={X_multi[:, i].mean():.2f}, std={X_multi[:, i].std():.2f}\")\n",
    "\n",
    "print(\"\\nNotice: Previous Scores has much larger values than others!\")\n",
    "print(\"This can cause problems for gradient descent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "X_scaled, mu, sigma = standardize(X_multi)\n",
    "\n",
    "print(\"Feature Scales AFTER Standardization:\")\n",
    "print(\"=\"*60)\n",
    "for i, col in enumerate(feature_cols):\n",
    "    print(f\"{col:<40} mean={X_scaled[:, i].mean():.4f}, std={X_scaled[:, i].std():.4f}\")\n",
    "\n",
    "print(\"\\nAll features now have mean ~0 and std ~1!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part8'></a>\n",
    "# Part 8: Training on Student Performance Data\n",
    "\n",
    "Now let's train our from-scratch model on all features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets manually\n",
    "np.random.seed(42)\n",
    "m = len(X_scaled)\n",
    "indices = np.random.permutation(m)\n",
    "split_idx = int(0.8 * m)\n",
    "\n",
    "train_idx, test_idx = indices[:split_idx], indices[split_idx:]\n",
    "\n",
    "X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our from-scratch model!\n",
    "model = LinearRegressionScratch(learning_rate=0.1, n_iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get learned parameters\n",
    "params = model.get_params()\n",
    "\n",
    "print(\"Training complete! (Multiple Linear Regression)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nLearned bias: {params['bias']:.4f}\")\n",
    "print(f\"\\nLearned coefficients (standardized scale):\")\n",
    "for i, col in enumerate(feature_cols):\n",
    "    print(f\"  {col:<40}: {params['coefficients'][i]:>8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(model.cost_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function During Training (Multiple Features)')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial cost: {model.cost_history[0]:.2f}\")\n",
    "print(f\"Final cost: {model.cost_history[-1]:.2f}\")\n",
    "print(f\"Cost reduction: {(1 - model.cost_history[-1]/model.cost_history[0])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (coefficient magnitudes)\n",
    "# Note: Because we standardized, coefficients are comparable\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['green' if c > 0 else 'red' for c in params['coefficients']]\n",
    "plt.barh(feature_cols, params['coefficients'], color=colors, alpha=0.7)\n",
    "plt.xlabel('Coefficient Value (Standardized)')\n",
    "plt.title('Feature Importance (Coefficient Magnitude)')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Green bars: Positive impact on performance\")\n",
    "print(\"- Red bars: Negative impact on performance\")\n",
    "print(\"- Larger magnitude = stronger effect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part9'></a>\n",
    "# Part 9: Evaluation Metrics from Scratch\n",
    "\n",
    "How do we measure how good our model is? Let's implement common regression metrics.\n",
    "\n",
    "## Mean Squared Error (MSE)\n",
    "\n",
    "$$MSE = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "## Root Mean Squared Error (RMSE)\n",
    "\n",
    "$$RMSE = \\sqrt{MSE}$$\n",
    "\n",
    "## Mean Absolute Error (MAE)\n",
    "\n",
    "$$MAE = \\frac{1}{m} \\sum_{i=1}^{m} |y_i - \\hat{y}_i|$$\n",
    "\n",
    "## R-squared (Coefficient of Determination)\n",
    "\n",
    "$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement evaluation metrics from scratch\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Squared Error.\"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"Calculate Root Mean Squared Error.\"\"\"\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Error.\"\"\"\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    \"\"\"Calculate R-squared (coefficient of determination).\"\"\"\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)  # Residual sum of squares\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)  # Total sum of squares\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(\"Evaluation metrics defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Model Evaluation\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<25} {'Train':<15} {'Test':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'MSE':<25} {mean_squared_error(y_train, y_pred_train):<15.4f} {mean_squared_error(y_test, y_pred_test):<15.4f}\")\n",
    "print(f\"{'RMSE':<25} {root_mean_squared_error(y_train, y_pred_train):<15.4f} {root_mean_squared_error(y_test, y_pred_test):<15.4f}\")\n",
    "print(f\"{'MAE':<25} {mean_absolute_error(y_train, y_pred_train):<15.4f} {mean_absolute_error(y_test, y_pred_test):<15.4f}\")\n",
    "print(f\"{'R-squared':<25} {r_squared(y_train, y_pred_train):<15.4f} {r_squared(y_test, y_pred_test):<15.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"- Our model explains {r_squared(y_test, y_pred_test)*100:.1f}% of the variance in student performance\")\n",
    "print(f\"- Average prediction error: {mean_absolute_error(y_test, y_pred_test):.2f} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[0].scatter(y_test, y_pred_test, alpha=0.5, edgecolors='black', linewidth=0.3)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Performance')\n",
    "axes[0].set_ylabel('Predicted Performance')\n",
    "axes[0].set_title(f'Actual vs Predicted (R² = {r_squared(y_test, y_pred_test):.4f})')\n",
    "axes[0].legend()\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - y_pred_test\n",
    "axes[1].scatter(y_pred_test, residuals, alpha=0.5, edgecolors='black', linewidth=0.3)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Performance')\n",
    "axes[1].set_ylabel('Residuals (Actual - Predicted)')\n",
    "axes[1].set_title('Residual Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: Points close to the red line = accurate predictions\")\n",
    "print(\"Right: Residuals randomly scattered around 0 = good model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part10'></a>\n",
    "# Part 10: Verification Against sklearn\n",
    "\n",
    "Let's verify that our implementation is correct by comparing with sklearn's LinearRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sklearn for verification only\n",
    "from sklearn.linear_model import LinearRegression as SklearnLR\n",
    "from sklearn.metrics import mean_squared_error as sklearn_mse, r2_score\n",
    "\n",
    "# Train sklearn model\n",
    "sklearn_model = SklearnLR()\n",
    "sklearn_model.fit(X_train, y_train.ravel())\n",
    "\n",
    "# Get sklearn predictions\n",
    "sklearn_pred_test = sklearn_model.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "print(\"Comparison: Our Implementation vs sklearn\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<20} {'Our Model':<20} {'sklearn':<20}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Test MSE':<20} {mean_squared_error(y_test, y_pred_test):<20.6f} {sklearn_mse(y_test, sklearn_pred_test):<20.6f}\")\n",
    "print(f\"{'Test R²':<20} {r_squared(y_test, y_pred_test):<20.6f} {r2_score(y_test, sklearn_pred_test):<20.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coefficients\n",
    "print(\"\\nCoefficient Comparison:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Feature':<40} {'Our Model':<15} {'sklearn':<15}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Bias':<40} {params['bias']:<15.6f} {sklearn_model.intercept_:<15.6f}\")\n",
    "for i, col in enumerate(feature_cols):\n",
    "    print(f\"{col:<40} {params['coefficients'][i]:<15.6f} {sklearn_model.coef_[i]:<15.6f}\")\n",
    "\n",
    "print(\"\\nOur implementation matches sklearn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part11'></a>\n",
    "# Part 11: Summary and Key Takeaways\n",
    "\n",
    "## What We Built\n",
    "\n",
    "We implemented **Linear Regression from scratch** using only NumPy, and applied it to predict student performance!\n",
    "\n",
    "### Components Implemented:\n",
    "\n",
    "1. **Cost Function (MSE)**: Measures how wrong our predictions are\n",
    "2. **Gradient Descent**: Iteratively updates weights to minimize cost\n",
    "3. **Normal Equation**: Closed-form solution for optimal weights\n",
    "4. **Feature Scaling**: Standardization for better gradient descent performance\n",
    "5. **Evaluation Metrics**: MSE, RMSE, MAE, R² implemented from scratch\n",
    "\n",
    "## Key Mathematical Concepts\n",
    "\n",
    "| Concept | Formula |\n",
    "|---------|----------|\n",
    "| Model | $\\hat{y} = Xw$ |\n",
    "| Cost Function | $J(w) = \\frac{1}{2m}(Xw - y)^T(Xw - y)$ |\n",
    "| Gradient | $\\nabla J(w) = \\frac{1}{m}X^T(Xw - y)$ |\n",
    "| Update Rule | $w := w - \\alpha \\nabla J(w)$ |\n",
    "| Normal Equation | $w = (X^TX)^{-1}X^Ty$ |\n",
    "\n",
    "## Key Insights from Student Performance Data\n",
    "\n",
    "- **Hours Studied** and **Previous Scores** are the strongest predictors\n",
    "- Our model achieves ~98% R² (explains 98% of variance in performance)\n",
    "- Average prediction error is only ~2 points\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "- **Regularization**: Add L1/L2 penalties to prevent overfitting\n",
    "- **Polynomial Features**: Model non-linear relationships\n",
    "- **Logistic Regression**: Apply similar concepts to classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary visualization\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Simple regression (Hours vs Performance)\n",
    "axes[0, 0].scatter(X_simple, y, alpha=0.3, edgecolors='black', linewidth=0.2)\n",
    "X_line = np.array([[0], [9]])\n",
    "y_line = model_simple.predict(X_line)\n",
    "axes[0, 0].plot(X_line, y_line, 'r-', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Hours Studied')\n",
    "axes[0, 0].set_ylabel('Performance Index')\n",
    "axes[0, 0].set_title('Simple Linear Regression')\n",
    "\n",
    "# 2. Cost convergence\n",
    "axes[0, 1].plot(model.cost_history)\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Cost')\n",
    "axes[0, 1].set_title('Gradient Descent Convergence')\n",
    "\n",
    "# 3. Actual vs Predicted\n",
    "axes[1, 0].scatter(y_test, y_pred_test, alpha=0.5, edgecolors='black', linewidth=0.3)\n",
    "axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Actual')\n",
    "axes[1, 0].set_ylabel('Predicted')\n",
    "axes[1, 0].set_title(f'Predictions (R² = {r_squared(y_test, y_pred_test):.4f})')\n",
    "\n",
    "# 4. Feature importance\n",
    "colors = ['green' if c > 0 else 'red' for c in params['coefficients']]\n",
    "axes[1, 1].barh(feature_cols, params['coefficients'], color=colors, alpha=0.7)\n",
    "axes[1, 1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1, 1].set_xlabel('Coefficient Value')\n",
    "axes[1, 1].set_title('Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LINEAR REGRESSION FROM SCRATCH - COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nYou now understand the math behind one of ML's most\")\n",
    "print(\"fundamental algorithms!\")\n",
    "print(\"\\nThis knowledge transfers to:\")\n",
    "print(\"  - Logistic Regression (classification)\")\n",
    "print(\"  - Neural Networks (multi-layer linear + activation)\")\n",
    "print(\"  - Any gradient-based optimization\")\n",
    "print(\"\\nThe same concepts of cost functions, gradients, and\")\n",
    "print(\"optimization apply throughout machine learning!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
