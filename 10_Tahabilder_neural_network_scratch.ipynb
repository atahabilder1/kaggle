{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "\n",
    "**Author:** Anik Tahabilder  \n",
    "**Project:** 10 of 22 - Kaggle ML Portfolio  \n",
    "**Dataset:** MNIST Handwritten Digits (Kaggle: digit-recognizer)  \n",
    "**Difficulty:** 8/10 | **Learning Value:** 10/10\n",
    "\n",
    "---\n",
    "\n",
    "## What Will You Learn?\n",
    "\n",
    "This notebook builds a **Neural Network completely from scratch** using only NumPy. No TensorFlow, no PyTorch - just pure math and Python!\n",
    "\n",
    "| Concept | What You'll Understand |\n",
    "|---------|------------------------|\n",
    "| **Neurons & Perceptrons** | The basic building block of neural networks |\n",
    "| **Activation Functions** | Sigmoid, ReLU, Tanh, Softmax - why we need them |\n",
    "| **Forward Propagation** | How data flows through the network |\n",
    "| **Loss Functions** | Measuring how wrong our predictions are |\n",
    "| **Backpropagation** | The magic algorithm that makes learning possible |\n",
    "| **Gradient Descent** | How weights are updated to minimize error |\n",
    "| **Full Implementation** | A working neural network class from scratch |\n",
    "\n",
    "### Why Build from Scratch?\n",
    "\n",
    "```\n",
    "Using TensorFlow without understanding backprop is like\n",
    "driving a car without knowing how engines work.\n",
    "You can do it, but you'll never truly master it.\n",
    "```\n",
    "\n",
    "### Dataset: MNIST Handwritten Digits\n",
    "\n",
    "| Info | Value |\n",
    "|------|-------|\n",
    "| **Kaggle** | digit-recognizer competition |\n",
    "| **Task** | Classify digits 0-9 |\n",
    "| **Samples** | 42,000 training images |\n",
    "| **Features** | 784 pixels (28x28) |\n",
    "| **Classes** | 10 |\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: The Neuron - Building Block of Neural Networks](#part1)\n",
    "2. [Part 2: Activation Functions](#part2)\n",
    "3. [Part 3: Neural Network Architecture](#part3)\n",
    "4. [Part 4: Forward Propagation](#part4)\n",
    "5. [Part 5: Loss Functions](#part5)\n",
    "6. [Part 6: Backpropagation - The Math](#part6)\n",
    "7. [Part 7: Gradient Descent Optimization](#part7)\n",
    "8. [Part 8: Building the Neural Network Class](#part8)\n",
    "9. [Part 9: Training on MNIST](#part9)\n",
    "10. [Part 10: CNN Concepts Overview](#part10)\n",
    "11. [Part 11: Summary and Key Takeaways](#part11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part1'></a>\n",
    "# Part 1: The Neuron - Building Block of Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 What is a Neuron?\n",
    "\n",
    "A **neuron** (or perceptron) is inspired by biological neurons in the brain.\n",
    "\n",
    "### Biological vs Artificial Neuron:\n",
    "\n",
    "| Biological Neuron | Artificial Neuron |\n",
    "|-------------------|-------------------|\n",
    "| Dendrites receive signals | Inputs (x1, x2, ..., xn) |\n",
    "| Cell body processes | Weighted sum + bias |\n",
    "| Axon transmits output | Activation function |\n",
    "| Synapses connect neurons | Weights (w1, w2, ..., wn) |\n",
    "\n",
    "### Mathematical Representation:\n",
    "\n",
    "```\n",
    "        x1 ---w1-->\\\n",
    "        x2 ---w2--->\\ \n",
    "        x3 ---w3---->[ \u03a3 + b ]---> f(z) ---> output\n",
    "        ...        /\n",
    "        xn ---wn-->/\n",
    "\n",
    "Where:\n",
    "  z = w1*x1 + w2*x2 + ... + wn*xn + b  (weighted sum + bias)\n",
    "  output = f(z)  (activation function)\n",
    "```\n",
    "\n",
    "### In Matrix Form:\n",
    "\n",
    "$$z = \\mathbf{w}^T \\mathbf{x} + b = \\sum_{i=1}^{n} w_i x_i + b$$\n",
    "\n",
    "$$\\text{output} = f(z)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml, make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"NEURAL NETWORK FROM SCRATCH\")\n",
    "print(\"=\"*70)\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(\"\\nNo TensorFlow, No PyTorch - Just Pure NumPy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SINGLE NEURON (PERCEPTRON) IMPLEMENTATION\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"SINGLE NEURON IMPLEMENTATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"\n",
    "    A single neuron (perceptron).\n",
    "    \n",
    "    Components:\n",
    "    - weights: Connection strengths for each input\n",
    "    - bias: Threshold for activation\n",
    "    - activation: Function to introduce non-linearity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, activation='sigmoid'):\n",
    "        # Initialize weights randomly (small values)\n",
    "        self.weights = np.random.randn(n_inputs) * 0.01\n",
    "        self.bias = 0.0\n",
    "        self.activation = activation\n",
    "        \n",
    "    def _activate(self, z):\n",
    "        \"\"\"Apply activation function\"\"\"\n",
    "        if self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(0, z)\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(z)\n",
    "        else:\n",
    "            return z  # Linear\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Compute output: f(w*x + b)\"\"\"\n",
    "        # Step 1: Weighted sum\n",
    "        z = np.dot(self.weights, inputs) + self.bias\n",
    "        # Step 2: Activation\n",
    "        output = self._activate(z)\n",
    "        return output\n",
    "\n",
    "# Example: Single neuron with 3 inputs\n",
    "print(\"\\nExample: Single Neuron with 3 inputs\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "neuron = Neuron(n_inputs=3, activation='sigmoid')\n",
    "print(f\"Weights: {neuron.weights}\")\n",
    "print(f\"Bias: {neuron.bias}\")\n",
    "\n",
    "# Sample input\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "output = neuron.forward(x)\n",
    "\n",
    "print(f\"\\nInput: {x}\")\n",
    "print(f\"Weighted Sum (z): {np.dot(neuron.weights, x) + neuron.bias:.4f}\")\n",
    "print(f\"Output (after sigmoid): {output:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"The neuron takes inputs, multiplies by weights, adds bias,\")\n",
    "print(\"and applies activation function to produce output!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Why Do We Need Activation Functions?\n",
    "\n",
    "Without activation functions, a neural network is just **linear regression**!\n",
    "\n",
    "| Without Activation | With Activation |\n",
    "|-------------------|------------------|\n",
    "| Only linear transformations | Can learn non-linear patterns |\n",
    "| Multiple layers = 1 layer | Each layer adds complexity |\n",
    "| Can't solve XOR problem | Can solve any function (Universal Approximation) |\n",
    "\n",
    "### Proof: Why Multiple Linear Layers = 1 Layer\n",
    "\n",
    "If we have two layers without activation:\n",
    "- Layer 1: $z_1 = W_1 x + b_1$\n",
    "- Layer 2: $z_2 = W_2 z_1 + b_2 = W_2 (W_1 x + b_1) + b_2 = (W_2 W_1) x + (W_2 b_1 + b_2)$\n",
    "\n",
    "This is just another linear transformation! **Activation functions break this linearity.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part2'></a>\n",
    "# Part 2: Activation Functions\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Common Activation Functions\n",
    "\n",
    "| Function | Formula | Range | Use Case |\n",
    "|----------|---------|-------|----------|\n",
    "| **Sigmoid** | $\\sigma(z) = \\frac{1}{1+e^{-z}}$ | (0, 1) | Binary classification output |\n",
    "| **Tanh** | $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ | (-1, 1) | Hidden layers (centered) |\n",
    "| **ReLU** | $\\max(0, z)$ | [0, \u221e) | Hidden layers (most popular) |\n",
    "| **Leaky ReLU** | $\\max(0.01z, z)$ | (-\u221e, \u221e) | Prevents dying ReLU |\n",
    "| **Softmax** | $\\frac{e^{z_i}}{\\sum_j e^{z_j}}$ | (0, 1), sum=1 | Multi-class output |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ACTIVATION FUNCTIONS IMPLEMENTATION\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"ACTIVATION FUNCTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class ActivationFunctions:\n",
    "    \"\"\"\n",
    "    Collection of activation functions and their derivatives.\n",
    "    Derivatives are needed for backpropagation!\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========== SIGMOID ==========\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"Sigmoid: squashes values to (0, 1)\"\"\"\n",
    "        # Clip to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        \"\"\"Derivative of sigmoid: \u03c3(z) * (1 - \u03c3(z))\"\"\"\n",
    "        s = ActivationFunctions.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    # ========== TANH ==========\n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        \"\"\"Tanh: squashes values to (-1, 1)\"\"\"\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        \"\"\"Derivative of tanh: 1 - tanh\u00b2(z)\"\"\"\n",
    "        return 1 - np.tanh(z) ** 2\n",
    "    \n",
    "    # ========== ReLU ==========\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        \"\"\"ReLU: max(0, z)\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        \"\"\"Derivative of ReLU: 1 if z > 0, else 0\"\"\"\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    # ========== LEAKY ReLU ==========\n",
    "    @staticmethod\n",
    "    def leaky_relu(z, alpha=0.01):\n",
    "        \"\"\"Leaky ReLU: max(\u03b1z, z)\"\"\"\n",
    "        return np.where(z > 0, z, alpha * z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def leaky_relu_derivative(z, alpha=0.01):\n",
    "        \"\"\"Derivative of Leaky ReLU\"\"\"\n",
    "        return np.where(z > 0, 1, alpha)\n",
    "    \n",
    "    # ========== SOFTMAX ==========\n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        \"\"\"Softmax: converts to probability distribution\"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        exp_z = np.exp(z - np.max(z, axis=-1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n",
    "\n",
    "# Create shorthand\n",
    "AF = ActivationFunctions\n",
    "\n",
    "print(\"\\nActivation functions implemented:\")\n",
    "print(\"  - Sigmoid (and derivative)\")\n",
    "print(\"  - Tanh (and derivative)\")\n",
    "print(\"  - ReLU (and derivative)\")\n",
    "print(\"  - Leaky ReLU (and derivative)\")\n",
    "print(\"  - Softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions\n",
    "print(\"=\"*70)\n",
    "print(\"VISUALIZING ACTIVATION FUNCTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# 1. Sigmoid\n",
    "ax = axes[0, 0]\n",
    "ax.plot(z, AF.sigmoid(z), 'b-', linewidth=2, label='Sigmoid')\n",
    "ax.plot(z, AF.sigmoid_derivative(z), 'r--', linewidth=2, label='Derivative')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axhline(y=0.5, color='gray', linewidth=0.5, linestyle=':')\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_title('Sigmoid: \u03c3(z) = 1/(1+e^{-z})', fontweight='bold')\n",
    "ax.set_xlabel('z')\n",
    "ax.set_ylabel('Output')\n",
    "ax.legend()\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "\n",
    "# 2. Tanh\n",
    "ax = axes[0, 1]\n",
    "ax.plot(z, AF.tanh(z), 'b-', linewidth=2, label='Tanh')\n",
    "ax.plot(z, AF.tanh_derivative(z), 'r--', linewidth=2, label='Derivative')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_title('Tanh: (e^z - e^{-z})/(e^z + e^{-z})', fontweight='bold')\n",
    "ax.set_xlabel('z')\n",
    "ax.legend()\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "# 3. ReLU\n",
    "ax = axes[0, 2]\n",
    "ax.plot(z, AF.relu(z), 'b-', linewidth=2, label='ReLU')\n",
    "ax.plot(z, AF.relu_derivative(z), 'r--', linewidth=2, label='Derivative')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_title('ReLU: max(0, z)', fontweight='bold')\n",
    "ax.set_xlabel('z')\n",
    "ax.legend()\n",
    "ax.set_ylim(-1, 5)\n",
    "\n",
    "# 4. Leaky ReLU\n",
    "ax = axes[1, 0]\n",
    "ax.plot(z, AF.leaky_relu(z), 'b-', linewidth=2, label='Leaky ReLU')\n",
    "ax.plot(z, AF.leaky_relu_derivative(z), 'r--', linewidth=2, label='Derivative')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_title('Leaky ReLU: max(0.01z, z)', fontweight='bold')\n",
    "ax.set_xlabel('z')\n",
    "ax.legend()\n",
    "ax.set_ylim(-1, 5)\n",
    "\n",
    "# 5. Comparison\n",
    "ax = axes[1, 1]\n",
    "ax.plot(z, AF.sigmoid(z), linewidth=2, label='Sigmoid')\n",
    "ax.plot(z, AF.tanh(z), linewidth=2, label='Tanh')\n",
    "ax.plot(z, AF.relu(z), linewidth=2, label='ReLU')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_title('Comparison of Activation Functions', fontweight='bold')\n",
    "ax.set_xlabel('z')\n",
    "ax.legend()\n",
    "ax.set_ylim(-1.5, 5)\n",
    "\n",
    "# 6. Softmax example\n",
    "ax = axes[1, 2]\n",
    "z_example = np.array([2.0, 1.0, 0.1])\n",
    "softmax_out = AF.softmax(z_example)\n",
    "bars = ax.bar(['Class 0', 'Class 1', 'Class 2'], softmax_out, \n",
    "              color=['steelblue', 'lightblue', 'lightgray'], edgecolor='black')\n",
    "ax.set_title(f'Softmax: z={list(z_example)}', fontweight='bold')\n",
    "ax.set_ylabel('Probability')\n",
    "for bar, val in zip(bars, softmax_out):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "            f'{val:.3f}', ha='center', fontweight='bold')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axhline(y=1/3, color='red', linestyle='--', alpha=0.5, label='Equal prob')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"  - Sigmoid: Output (0,1), derivative max at z=0\")\n",
    "print(\"  - Tanh: Output (-1,1), zero-centered, stronger gradients\")\n",
    "print(\"  - ReLU: Fast computation, can 'die' (output 0 forever)\")\n",
    "print(\"  - Softmax: Outputs sum to 1 (probability distribution)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 When to Use Which Activation?\n",
    "\n",
    "| Layer Type | Recommended Activation | Why |\n",
    "|------------|----------------------|-----|\n",
    "| **Hidden Layers** | ReLU | Fast, avoids vanishing gradient |\n",
    "| **Hidden (if ReLU dies)** | Leaky ReLU, ELU | Prevents dead neurons |\n",
    "| **Binary Output** | Sigmoid | Output is probability (0-1) |\n",
    "| **Multi-class Output** | Softmax | Outputs are probabilities summing to 1 |\n",
    "| **Regression Output** | Linear (none) | No squashing needed |\n",
    "\n",
    "### The Vanishing Gradient Problem\n",
    "\n",
    "| Activation | Max Derivative | Problem |\n",
    "|------------|---------------|----------|\n",
    "| Sigmoid | 0.25 (at z=0) | Gradients shrink exponentially in deep networks |\n",
    "| Tanh | 1.0 (at z=0) | Better but still shrinks |\n",
    "| ReLU | 1.0 (for z>0) | No shrinking! But can \"die\" |\n",
    "\n",
    "**ReLU solved the vanishing gradient problem**, enabling deep networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part3'></a>\n",
    "# Part 3: Neural Network Architecture\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Layers of a Neural Network\n",
    "\n",
    "```\n",
    "    INPUT LAYER          HIDDEN LAYERS           OUTPUT LAYER\n",
    "    (features)           (learned repr.)         (predictions)\n",
    "    \n",
    "        x1  ----\\                                    /---- y1\n",
    "                 \\       [h1] --- [h4]              /\n",
    "        x2  ------\\     /    \\   /    \\            /------ y2\n",
    "                   \\   /      \\ /      \\          /\n",
    "        x3  --------[h1]------[h3]------[h5]-----/-------- y3\n",
    "                   /   \\      / \\      /          \\\n",
    "        x4  ------/     \\    /   \\    /            \\------ y4\n",
    "                 /       [h2] --- [h4]              \\\n",
    "        xn  ----/                                    \\---- yk\n",
    "    \n",
    "    Layer 0             Layer 1    Layer 2          Layer 3\n",
    "    (n neurons)         (4 neurons)(3 neurons)      (k neurons)\n",
    "```\n",
    "\n",
    "### Layer Definitions:\n",
    "\n",
    "| Layer | Purpose | Activation |\n",
    "|-------|---------|------------|\n",
    "| **Input** | Receives raw features | None |\n",
    "| **Hidden** | Learns representations | ReLU, Tanh |\n",
    "| **Output** | Makes predictions | Sigmoid/Softmax |\n",
    "\n",
    "### Notation:\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|  \n",
    "| $L$ | Number of layers (not counting input) |\n",
    "| $n^{[l]}$ | Number of neurons in layer $l$ |\n",
    "| $W^{[l]}$ | Weight matrix for layer $l$, shape $(n^{[l]}, n^{[l-1]})$ |\n",
    "| $b^{[l]}$ | Bias vector for layer $l$, shape $(n^{[l]}, 1)$ |\n",
    "| $a^{[l]}$ | Activations of layer $l$ |\n",
    "| $z^{[l]}$ | Pre-activation (before applying activation function) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZE NEURAL NETWORK ARCHITECTURE\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"NEURAL NETWORK ARCHITECTURE VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def draw_neural_network(layer_sizes, ax, title=\"Neural Network\"):\n",
    "    \"\"\"\n",
    "    Draw a neural network diagram.\n",
    "    layer_sizes: list of neurons per layer [input, hidden1, ..., output]\n",
    "    \"\"\"\n",
    "    n_layers = len(layer_sizes)\n",
    "    v_spacing = 1.0\n",
    "    h_spacing = 2.0\n",
    "    \n",
    "    # Calculate positions\n",
    "    layer_positions = []\n",
    "    for i, size in enumerate(layer_sizes):\n",
    "        x = i * h_spacing\n",
    "        y_start = (max(layer_sizes) - size) * v_spacing / 2\n",
    "        positions = [(x, y_start + j * v_spacing) for j in range(size)]\n",
    "        layer_positions.append(positions)\n",
    "    \n",
    "    # Draw connections\n",
    "    for i in range(n_layers - 1):\n",
    "        for pos1 in layer_positions[i]:\n",
    "            for pos2 in layer_positions[i + 1]:\n",
    "                ax.plot([pos1[0], pos2[0]], [pos1[1], pos2[1]], \n",
    "                       'b-', alpha=0.3, linewidth=0.5)\n",
    "    \n",
    "    # Draw neurons\n",
    "    colors = ['#FFD700', '#87CEEB', '#87CEEB', '#FFB6C1']  # Gold, Sky Blue, Pink\n",
    "    labels = ['Input', 'Hidden', 'Hidden', 'Output']\n",
    "    \n",
    "    for i, positions in enumerate(layer_positions):\n",
    "        color = colors[min(i, len(colors)-1)] if i < n_layers - 1 else colors[-1]\n",
    "        for pos in positions:\n",
    "            circle = plt.Circle(pos, 0.3, color=color, ec='black', linewidth=2, zorder=10)\n",
    "            ax.add_patch(circle)\n",
    "        \n",
    "        # Label\n",
    "        label = 'Input' if i == 0 else ('Output' if i == n_layers - 1 else f'Hidden {i}')\n",
    "        ax.text(i * h_spacing, -1.5, f'{label}\\n({layer_sizes[i]} neurons)', \n",
    "               ha='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(-1, (n_layers - 1) * h_spacing + 1)\n",
    "    ax.set_ylim(-2.5, max(layer_sizes) * v_spacing)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "\n",
    "# Example architectures\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "\n",
    "# Simple network\n",
    "draw_neural_network([3, 4, 2], axes[0], \"Simple: [3, 4, 2]\")\n",
    "\n",
    "# Medium network\n",
    "draw_neural_network([4, 5, 5, 3], axes[1], \"Medium: [4, 5, 5, 3]\")\n",
    "\n",
    "# Deep network\n",
    "draw_neural_network([5, 4, 4, 4, 2], axes[2], \"Deep: [5, 4, 4, 4, 2]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nArchitecture Notation:\")\n",
    "print(\"  [3, 4, 2] means:\")\n",
    "print(\"    - 3 input features\")\n",
    "print(\"    - 1 hidden layer with 4 neurons\")\n",
    "print(\"    - 2 output neurons (e.g., binary classification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Weight Matrices and Dimensions\n",
    "\n",
    "For a network with architecture `[3, 4, 2]`:\n",
    "\n",
    "| Layer | Weight Shape | Bias Shape | Total Parameters |\n",
    "|-------|--------------|------------|------------------|\n",
    "| 1 (Hidden) | (4, 3) | (4, 1) | 4\u00d73 + 4 = 16 |\n",
    "| 2 (Output) | (2, 4) | (2, 1) | 2\u00d74 + 2 = 10 |\n",
    "| **Total** | | | **26 parameters** |\n",
    "\n",
    "### Why These Shapes?\n",
    "\n",
    "- $W^{[l]}$ has shape $(n^{[l]}, n^{[l-1]})$\n",
    "- This allows: $z^{[l]} = W^{[l]} \\cdot a^{[l-1]} + b^{[l]}$\n",
    "- Where $a^{[l-1]}$ has shape $(n^{[l-1]}, m)$ for $m$ samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# WEIGHT INITIALIZATION\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"WEIGHT INITIALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def initialize_parameters(layer_sizes, init_method='he'):\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for all layers.\n",
    "    \n",
    "    Parameters:\n",
    "    - layer_sizes: list like [input_size, hidden1_size, ..., output_size]\n",
    "    - init_method: 'random', 'xavier', 'he'\n",
    "    \n",
    "    Returns:\n",
    "    - parameters: dict with W1, b1, W2, b2, ...\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_sizes) - 1  # Number of layers (not counting input)\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        n_current = layer_sizes[l]\n",
    "        n_prev = layer_sizes[l - 1]\n",
    "        \n",
    "        if init_method == 'random':\n",
    "            # Small random values\n",
    "            W = np.random.randn(n_current, n_prev) * 0.01\n",
    "        elif init_method == 'xavier':\n",
    "            # Xavier initialization (good for tanh)\n",
    "            W = np.random.randn(n_current, n_prev) * np.sqrt(1 / n_prev)\n",
    "        elif init_method == 'he':\n",
    "            # He initialization (good for ReLU)\n",
    "            W = np.random.randn(n_current, n_prev) * np.sqrt(2 / n_prev)\n",
    "        \n",
    "        b = np.zeros((n_current, 1))\n",
    "        \n",
    "        parameters[f'W{l}'] = W\n",
    "        parameters[f'b{l}'] = b\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Example\n",
    "layer_sizes = [784, 128, 64, 10]  # MNIST example\n",
    "params = initialize_parameters(layer_sizes, init_method='he')\n",
    "\n",
    "print(f\"\\nNetwork Architecture: {layer_sizes}\")\n",
    "print(f\"\\nInitialized Parameters:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "total_params = 0\n",
    "for l in range(1, len(layer_sizes)):\n",
    "    W_shape = params[f'W{l}'].shape\n",
    "    b_shape = params[f'b{l}'].shape\n",
    "    layer_params = W_shape[0] * W_shape[1] + b_shape[0]\n",
    "    total_params += layer_params\n",
    "    print(f\"Layer {l}: W{l} shape = {W_shape}, b{l} shape = {b_shape}, params = {layer_params:,}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INITIALIZATION METHODS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "| Method | Formula | Best For |\n",
    "|--------|---------|----------|\n",
    "| Random | W * 0.01 | Simple cases |\n",
    "| Xavier | W * sqrt(1/n_prev) | Sigmoid, Tanh |\n",
    "| He     | W * sqrt(2/n_prev) | ReLU |\n",
    "\n",
    "Why initialization matters:\n",
    "- Too large: Exploding gradients, activations saturate\n",
    "- Too small: Vanishing gradients, slow learning\n",
    "- Just right: Stable training!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part4'></a>\n",
    "# Part 4: Forward Propagation\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 What is Forward Propagation?\n",
    "\n",
    "Forward propagation is the process of **passing input through the network** to get output.\n",
    "\n",
    "### Algorithm:\n",
    "\n",
    "```\n",
    "For each layer l = 1, 2, ..., L:\n",
    "    1. Compute linear combination:  z[l] = W[l] \u00b7 a[l-1] + b[l]\n",
    "    2. Apply activation:            a[l] = g[l](z[l])\n",
    "    \n",
    "Where:\n",
    "    - a[0] = X (input)\n",
    "    - a[L] = \u0177 (output/prediction)\n",
    "    - g[l] is the activation function for layer l\n",
    "```\n",
    "\n",
    "### Visual Representation:\n",
    "\n",
    "```\n",
    "Input (X)                                          Output (\u0177)\n",
    "    \u2193                                                  \u2191\n",
    " a[0] = X                                          a[L] = \u0177\n",
    "    \u2193                                                  \u2191\n",
    " z[1] = W[1]\u00b7a[0] + b[1]  \u2192  a[1] = g(z[1])  \u2192  ... \u2192  a[L]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FORWARD PROPAGATION IMPLEMENTATION\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"FORWARD PROPAGATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def forward_propagation(X, parameters, activations):\n",
    "    \"\"\"\n",
    "    Perform forward propagation through the network.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Input data, shape (n_features, n_samples)\n",
    "    - parameters: dict with W1, b1, W2, b2, ...\n",
    "    - activations: list of activation functions per layer\n",
    "    \n",
    "    Returns:\n",
    "    - A_final: Output predictions\n",
    "    - cache: dict storing all intermediate values (needed for backprop)\n",
    "    \"\"\"\n",
    "    cache = {'A0': X}  # Store input\n",
    "    A = X\n",
    "    L = len(activations)  # Number of layers\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        A_prev = A\n",
    "        W = parameters[f'W{l}']\n",
    "        b = parameters[f'b{l}']\n",
    "        \n",
    "        # Step 1: Linear transformation\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        \n",
    "        # Step 2: Activation\n",
    "        activation = activations[l - 1]\n",
    "        if activation == 'sigmoid':\n",
    "            A = AF.sigmoid(Z)\n",
    "        elif activation == 'relu':\n",
    "            A = AF.relu(Z)\n",
    "        elif activation == 'tanh':\n",
    "            A = AF.tanh(Z)\n",
    "        elif activation == 'softmax':\n",
    "            A = AF.softmax(Z.T).T  # Softmax along correct axis\n",
    "        else:\n",
    "            A = Z  # Linear\n",
    "        \n",
    "        # Store in cache for backpropagation\n",
    "        cache[f'Z{l}'] = Z\n",
    "        cache[f'A{l}'] = A\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "# Example: Forward pass\n",
    "print(\"\\nExample Forward Propagation:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Simple network: 3 inputs -> 4 hidden -> 2 outputs\n",
    "layer_sizes = [3, 4, 2]\n",
    "params = initialize_parameters(layer_sizes, 'he')\n",
    "activations = ['relu', 'sigmoid']  # ReLU for hidden, Sigmoid for output\n",
    "\n",
    "# Sample input (3 features, 5 samples)\n",
    "X = np.random.randn(3, 5)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Network: {layer_sizes}\")\n",
    "print(f\"Activations: {activations}\")\n",
    "\n",
    "# Forward pass\n",
    "output, cache = forward_propagation(X, params, activations)\n",
    "\n",
    "print(f\"\\nLayer-by-layer shapes:\")\n",
    "print(f\"  Input (A0): {cache['A0'].shape}\")\n",
    "print(f\"  Z1: {cache['Z1'].shape} -> A1: {cache['A1'].shape}\")\n",
    "print(f\"  Z2: {cache['Z2'].shape} -> A2: {cache['A2'].shape}\")\n",
    "\n",
    "print(f\"\\nOutput (predictions):\")\n",
    "print(output.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part5'></a>\n",
    "# Part 5: Loss Functions\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 What is a Loss Function?\n",
    "\n",
    "A **loss function** (or cost function) measures **how wrong** our predictions are.\n",
    "\n",
    "| Task | Loss Function | Formula |\n",
    "|------|--------------|--------|\n",
    "| **Binary Classification** | Binary Cross-Entropy | $-\\frac{1}{m}\\sum[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]$ |\n",
    "| **Multi-class Classification** | Categorical Cross-Entropy | $-\\frac{1}{m}\\sum\\sum y_k\\log(\\hat{y}_k)$ |\n",
    "| **Regression** | Mean Squared Error | $\\frac{1}{m}\\sum(y - \\hat{y})^2$ |\n",
    "\n",
    "### Goal of Training:\n",
    "\n",
    "**Minimize the loss function** by adjusting weights and biases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOSS FUNCTIONS IMPLEMENTATION\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"LOSS FUNCTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class LossFunctions:\n",
    "    \"\"\"\n",
    "    Collection of loss functions and their derivatives.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_crossentropy(y_true, y_pred, epsilon=1e-15):\n",
    "        \"\"\"\n",
    "        Binary Cross-Entropy Loss.\n",
    "        Used for binary classification with sigmoid output.\n",
    "        \n",
    "        Formula: -1/m * sum(y*log(\u0177) + (1-y)*log(1-\u0177))\n",
    "        \"\"\"\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        m = y_true.shape[1]  # Number of samples\n",
    "        \n",
    "        loss = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / m\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def categorical_crossentropy(y_true, y_pred, epsilon=1e-15):\n",
    "        \"\"\"\n",
    "        Categorical Cross-Entropy Loss.\n",
    "        Used for multi-class classification with softmax output.\n",
    "        \n",
    "        Formula: -1/m * sum(sum(y_k * log(\u0177_k)))\n",
    "        \"\"\"\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        m = y_true.shape[1]\n",
    "        \n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / m\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Mean Squared Error.\n",
    "        Used for regression.\n",
    "        \n",
    "        Formula: 1/m * sum((y - \u0177)\u00b2)\n",
    "        \"\"\"\n",
    "        m = y_true.shape[1]\n",
    "        loss = np.sum((y_true - y_pred) ** 2) / m\n",
    "        return loss\n",
    "\n",
    "LF = LossFunctions\n",
    "\n",
    "# Example: Binary Cross-Entropy\n",
    "print(\"\\nExample: Binary Cross-Entropy\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "y_true = np.array([[1, 0, 1, 0, 1]])  # True labels\n",
    "y_pred_good = np.array([[0.9, 0.1, 0.8, 0.2, 0.9]])  # Good predictions\n",
    "y_pred_bad = np.array([[0.5, 0.5, 0.5, 0.5, 0.5]])   # Bad predictions\n",
    "\n",
    "loss_good = LF.binary_crossentropy(y_true, y_pred_good)\n",
    "loss_bad = LF.binary_crossentropy(y_true, y_pred_bad)\n",
    "\n",
    "print(f\"True labels:      {y_true[0]}\")\n",
    "print(f\"Good predictions: {y_pred_good[0]} -> Loss: {loss_good:.4f}\")\n",
    "print(f\"Bad predictions:  {y_pred_bad[0]} -> Loss: {loss_bad:.4f}\")\n",
    "print(f\"\\nLower loss = better predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loss landscape\n",
    "print(\"=\"*70)\n",
    "print(\"VISUALIZING LOSS FUNCTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Binary Cross-Entropy for y=1\n",
    "y_pred_range = np.linspace(0.001, 0.999, 100)\n",
    "loss_y1 = -np.log(y_pred_range)  # When y=1\n",
    "loss_y0 = -np.log(1 - y_pred_range)  # When y=0\n",
    "\n",
    "axes[0].plot(y_pred_range, loss_y1, 'b-', linewidth=2, label='y=1: -log(\u0177)')\n",
    "axes[0].plot(y_pred_range, loss_y0, 'r-', linewidth=2, label='y=0: -log(1-\u0177)')\n",
    "axes[0].set_xlabel('Predicted Probability (\u0177)')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Binary Cross-Entropy Loss', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 5)\n",
    "axes[0].axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 2. MSE Loss\n",
    "y_true_val = 1.0\n",
    "y_pred_range = np.linspace(-1, 3, 100)\n",
    "mse_loss = (y_true_val - y_pred_range) ** 2\n",
    "\n",
    "axes[1].plot(y_pred_range, mse_loss, 'g-', linewidth=2)\n",
    "axes[1].scatter([y_true_val], [0], color='red', s=100, zorder=5, label='True value')\n",
    "axes[1].set_xlabel('Predicted Value (\u0177)')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Mean Squared Error (y=1)', fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. Loss surface (2D)\n",
    "w1_range = np.linspace(-3, 3, 100)\n",
    "w2_range = np.linspace(-3, 3, 100)\n",
    "W1, W2 = np.meshgrid(w1_range, w2_range)\n",
    "\n",
    "# Simple quadratic loss surface\n",
    "Loss = (W1 - 1)**2 + (W2 + 0.5)**2\n",
    "\n",
    "contour = axes[2].contour(W1, W2, Loss, levels=20, cmap='viridis')\n",
    "axes[2].scatter([1], [-0.5], color='red', s=100, zorder=5, marker='*', label='Minimum')\n",
    "axes[2].set_xlabel('Weight 1')\n",
    "axes[2].set_ylabel('Weight 2')\n",
    "axes[2].set_title('Loss Surface (Gradient Descent Target)', fontweight='bold')\n",
    "axes[2].legend()\n",
    "plt.colorbar(contour, ax=axes[2], label='Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"  - BCE heavily penalizes confident wrong predictions\")\n",
    "print(\"  - MSE has a smooth parabolic shape (easy to optimize)\")\n",
    "print(\"  - Gradient descent finds the minimum of the loss surface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part6'></a>\n",
    "# Part 6: Backpropagation - The Math\n",
    "\n",
    "---\n",
    "\n",
    "## 6.1 What is Backpropagation?\n",
    "\n",
    "**Backpropagation** is the algorithm that computes gradients (derivatives) of the loss with respect to each weight.\n",
    "\n",
    "### The Big Picture:\n",
    "\n",
    "```\n",
    "Forward Pass:  X \u2192 Layer 1 \u2192 Layer 2 \u2192 ... \u2192 \u0177 \u2192 Loss\n",
    "Backward Pass: X \u2190 Layer 1 \u2190 Layer 2 \u2190 ... \u2190 \u0177 \u2190 Loss\n",
    "                   (compute gradients going backward)\n",
    "```\n",
    "\n",
    "### Chain Rule - The Foundation:\n",
    "\n",
    "If $L = f(g(h(x)))$, then:\n",
    "\n",
    "$$\\frac{dL}{dx} = \\frac{dL}{df} \\cdot \\frac{df}{dg} \\cdot \\frac{dg}{dh} \\cdot \\frac{dh}{dx}$$\n",
    "\n",
    "This is how we propagate gradients backward through layers!\n",
    "\n",
    "## 6.2 Backpropagation Equations\n",
    "\n",
    "For layer $l$:\n",
    "\n",
    "| What We Compute | Formula | Purpose |\n",
    "|-----------------|---------|--------|\n",
    "| $dZ^{[l]}$ | $dA^{[l]} \\times g'^{[l]}(Z^{[l]})$ | Gradient of pre-activation |\n",
    "| $dW^{[l]}$ | $\\frac{1}{m} dZ^{[l]} \\cdot A^{[l-1]T}$ | Gradient for weights |\n",
    "| $db^{[l]}$ | $\\frac{1}{m} \\sum dZ^{[l]}$ | Gradient for biases |\n",
    "| $dA^{[l-1]}$ | $W^{[l]T} \\cdot dZ^{[l]}$ | Pass gradient to previous layer |\n",
    "\n",
    "Where $g'$ is the derivative of the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BACKPROPAGATION IMPLEMENTATION\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"BACKPROPAGATION - THE LEARNING ALGORITHM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def backward_propagation(Y, cache, parameters, activations):\n",
    "    \"\"\"\n",
    "    Perform backward propagation to compute gradients.\n",
    "    \n",
    "    Parameters:\n",
    "    - Y: True labels, shape (n_classes, n_samples)\n",
    "    - cache: dict with Z1, A1, Z2, A2, ... from forward pass\n",
    "    - parameters: dict with W1, b1, W2, b2, ...\n",
    "    - activations: list of activation functions per layer\n",
    "    \n",
    "    Returns:\n",
    "    - gradients: dict with dW1, db1, dW2, db2, ...\n",
    "    \"\"\"\n",
    "    gradients = {}\n",
    "    L = len(activations)  # Number of layers\n",
    "    m = Y.shape[1]  # Number of samples\n",
    "    \n",
    "    # Get final layer output\n",
    "    AL = cache[f'A{L}']\n",
    "    \n",
    "    # ========== OUTPUT LAYER GRADIENT ==========\n",
    "    # For cross-entropy loss with sigmoid/softmax: dZ = A - Y\n",
    "    # This is a simplified form that combines loss derivative and activation derivative\n",
    "    dZ = AL - Y\n",
    "    \n",
    "    # ========== BACKWARD THROUGH EACH LAYER ==========\n",
    "    for l in reversed(range(1, L + 1)):\n",
    "        A_prev = cache[f'A{l-1}']\n",
    "        W = parameters[f'W{l}']\n",
    "        \n",
    "        # Compute gradients\n",
    "        dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "        db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        \n",
    "        # Store gradients\n",
    "        gradients[f'dW{l}'] = dW\n",
    "        gradients[f'db{l}'] = db\n",
    "        \n",
    "        # Compute dA for previous layer (if not input layer)\n",
    "        if l > 1:\n",
    "            dA_prev = np.dot(W.T, dZ)\n",
    "            \n",
    "            # Apply activation derivative\n",
    "            Z_prev = cache[f'Z{l-1}']\n",
    "            activation = activations[l - 2]\n",
    "            \n",
    "            if activation == 'sigmoid':\n",
    "                dZ = dA_prev * AF.sigmoid_derivative(Z_prev)\n",
    "            elif activation == 'relu':\n",
    "                dZ = dA_prev * AF.relu_derivative(Z_prev)\n",
    "            elif activation == 'tanh':\n",
    "                dZ = dA_prev * AF.tanh_derivative(Z_prev)\n",
    "            else:\n",
    "                dZ = dA_prev  # Linear\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Example: Backpropagation\n",
    "print(\"\\nExample: Computing Gradients\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Simple network\n",
    "layer_sizes = [3, 4, 2]\n",
    "params = initialize_parameters(layer_sizes, 'he')\n",
    "activations = ['relu', 'sigmoid']\n",
    "\n",
    "# Sample data\n",
    "X = np.random.randn(3, 5)  # 3 features, 5 samples\n",
    "Y = np.random.randint(0, 2, (2, 5))  # Binary labels (2 classes, 5 samples)\n",
    "\n",
    "# Forward pass\n",
    "output, cache = forward_propagation(X, params, activations)\n",
    "\n",
    "# Backward pass\n",
    "grads = backward_propagation(Y, cache, params, activations)\n",
    "\n",
    "print(\"Computed gradients:\")\n",
    "for key, value in grads.items():\n",
    "    print(f\"  {key}: shape = {value.shape}, mean = {value.mean():.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"These gradients tell us HOW to adjust weights to reduce loss!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Visualizing Backpropagation\n",
    "\n",
    "### The Backward Flow:\n",
    "\n",
    "```\n",
    "FORWARD:   X \u2500\u2500\u2192 Z1 \u2500\u2500\u2192 A1 \u2500\u2500\u2192 Z2 \u2500\u2500\u2192 A2 \u2500\u2500\u2192 Loss\n",
    "                 \u2191            \u2191            \u2191\n",
    "              W1, b1       W2, b2         (Y)\n",
    "\n",
    "BACKWARD:  dX \u2190\u2500\u2500 dZ1 \u2190\u2500\u2500 dA1 \u2190\u2500\u2500 dZ2 \u2190\u2500\u2500 dA2 \u2190\u2500\u2500 dLoss\n",
    "                 \u2193            \u2193\n",
    "             dW1, db1     dW2, db2\n",
    "```\n",
    "\n",
    "### Key Intuition:\n",
    "\n",
    "1. **Output layer**: Error = (prediction - true label)\n",
    "2. **Propagate backward**: Each layer receives error from layer above\n",
    "3. **Compute gradients**: Use chain rule at each layer\n",
    "4. **Update weights**: Move in opposite direction of gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part7'></a>\n",
    "# Part 7: Gradient Descent Optimization\n",
    "\n",
    "---\n",
    "\n",
    "## 7.1 The Gradient Descent Algorithm\n",
    "\n",
    "**Gradient Descent** updates weights to minimize the loss function.\n",
    "\n",
    "### Update Rule:\n",
    "\n",
    "$$W_{new} = W_{old} - \\alpha \\cdot \\frac{\\partial J}{\\partial W}$$\n",
    "\n",
    "$$b_{new} = b_{old} - \\alpha \\cdot \\frac{\\partial J}{\\partial b}$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = **learning rate** (step size)\n",
    "- $\\frac{\\partial J}{\\partial W}$ = gradient of loss with respect to weights\n",
    "\n",
    "### Analogy: Hiking Down a Mountain\n",
    "\n",
    "| Mountain Hiking | Gradient Descent |\n",
    "|-----------------|------------------|\n",
    "| You're at some position | Current weights |\n",
    "| Look for steepest downhill | Compute gradient |\n",
    "| Take a step downhill | Update weights |\n",
    "| Repeat until valley | Repeat until minimum loss |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GRADIENT DESCENT IMPLEMENTATION\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"GRADIENT DESCENT OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent.\n",
    "    \n",
    "    W_new = W_old - learning_rate * dW\n",
    "    b_new = b_old - learning_rate * db\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2  # Number of layers\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        parameters[f'W{l}'] -= learning_rate * gradients[f'dW{l}']\n",
    "        parameters[f'b{l}'] -= learning_rate * gradients[f'db{l}']\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Visualize gradient descent\n",
    "print(\"\\nVisualizing Gradient Descent on 2D Loss Surface:\")\n",
    "\n",
    "def loss_function(w1, w2):\n",
    "    \"\"\"Simple quadratic loss for visualization\"\"\"\n",
    "    return (w1 - 1)**2 + 2*(w2 + 0.5)**2\n",
    "\n",
    "def gradient(w1, w2):\n",
    "    \"\"\"Gradient of the loss\"\"\"\n",
    "    dw1 = 2 * (w1 - 1)\n",
    "    dw2 = 4 * (w2 + 0.5)\n",
    "    return dw1, dw2\n",
    "\n",
    "# Gradient descent simulation\n",
    "w1, w2 = -2.0, 2.0  # Starting point\n",
    "learning_rate = 0.1\n",
    "path = [(w1, w2)]\n",
    "\n",
    "for _ in range(50):\n",
    "    dw1, dw2 = gradient(w1, w2)\n",
    "    w1 -= learning_rate * dw1\n",
    "    w2 -= learning_rate * dw2\n",
    "    path.append((w1, w2))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Loss surface\n",
    "w1_range = np.linspace(-3, 3, 100)\n",
    "w2_range = np.linspace(-3, 3, 100)\n",
    "W1, W2 = np.meshgrid(w1_range, w2_range)\n",
    "Loss = loss_function(W1, W2)\n",
    "\n",
    "contour = axes[0].contour(W1, W2, Loss, levels=30, cmap='viridis')\n",
    "path = np.array(path)\n",
    "axes[0].plot(path[:, 0], path[:, 1], 'r.-', linewidth=2, markersize=8, label='GD Path')\n",
    "axes[0].scatter([path[0, 0]], [path[0, 1]], color='red', s=100, marker='o', zorder=5, label='Start')\n",
    "axes[0].scatter([1], [-0.5], color='green', s=200, marker='*', zorder=5, label='Minimum')\n",
    "axes[0].set_xlabel('Weight 1')\n",
    "axes[0].set_ylabel('Weight 2')\n",
    "axes[0].set_title('Gradient Descent Path', fontweight='bold', fontsize=14)\n",
    "axes[0].legend()\n",
    "plt.colorbar(contour, ax=axes[0], label='Loss')\n",
    "\n",
    "# Loss over iterations\n",
    "losses = [loss_function(p[0], p[1]) for p in path]\n",
    "axes[1].plot(losses, 'b-', linewidth=2)\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Loss Decrease Over Iterations', fontweight='bold', fontsize=14)\n",
    "axes[1].axhline(y=0, color='green', linestyle='--', alpha=0.5, label='Minimum')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStarting point: w1={path[0, 0]:.2f}, w2={path[0, 1]:.2f}\")\n",
    "print(f\"Final point:    w1={path[-1, 0]:.4f}, w2={path[-1, 1]:.4f}\")\n",
    "print(f\"True minimum:   w1=1.0, w2=-0.5\")\n",
    "print(f\"Initial loss:   {losses[0]:.4f}\")\n",
    "print(f\"Final loss:     {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Learning Rate - The Most Important Hyperparameter\n",
    "\n",
    "| Learning Rate | Effect |\n",
    "|---------------|--------|\n",
    "| **Too Large** | Overshoots minimum, may diverge |\n",
    "| **Too Small** | Very slow convergence |\n",
    "| **Just Right** | Smooth convergence to minimum |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of learning rate\n",
    "print(\"=\"*70)\n",
    "print(\"EFFECT OF LEARNING RATE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.5]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, lr in enumerate(learning_rates):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Gradient descent\n",
    "    w1, w2 = -2.0, 2.0\n",
    "    path = [(w1, w2)]\n",
    "    \n",
    "    for _ in range(30):\n",
    "        dw1, dw2 = gradient(w1, w2)\n",
    "        w1 -= lr * dw1\n",
    "        w2 -= lr * dw2\n",
    "        # Clip to prevent divergence\n",
    "        w1, w2 = np.clip(w1, -5, 5), np.clip(w2, -5, 5)\n",
    "        path.append((w1, w2))\n",
    "    \n",
    "    path = np.array(path)\n",
    "    \n",
    "    # Plot\n",
    "    contour = ax.contour(W1, W2, Loss, levels=30, cmap='viridis', alpha=0.7)\n",
    "    ax.plot(path[:, 0], path[:, 1], 'r.-', linewidth=2, markersize=8)\n",
    "    ax.scatter([path[0, 0]], [path[0, 1]], color='red', s=100, marker='o', zorder=5)\n",
    "    ax.scatter([1], [-0.5], color='green', s=200, marker='*', zorder=5)\n",
    "    ax.set_xlabel('Weight 1')\n",
    "    ax.set_ylabel('Weight 2')\n",
    "    \n",
    "    status = \"Too slow\" if lr < 0.05 else (\"Good\" if lr < 1.0 else \"Diverging!\")\n",
    "    ax.set_title(f'Learning Rate = {lr} ({status})', fontweight='bold')\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLearning Rate Guidelines:\")\n",
    "print(\"  - Start with 0.001 or 0.01\")\n",
    "print(\"  - If loss decreases smoothly, learning rate is good\")\n",
    "print(\"  - If loss oscillates wildly, reduce learning rate\")\n",
    "print(\"  - If loss decreases too slowly, increase learning rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part8'></a>\n",
    "# Part 8: Building the Complete Neural Network Class\n",
    "\n",
    "---\n",
    "\n",
    "Now we combine everything into a single, reusable Neural Network class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPLETE NEURAL NETWORK CLASS FROM SCRATCH\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"BUILDING COMPLETE NEURAL NETWORK FROM SCRATCH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A fully-connected neural network built from scratch.\n",
    "    \n",
    "    Supports:\n",
    "    - Multiple hidden layers\n",
    "    - Different activation functions (relu, sigmoid, tanh, softmax)\n",
    "    - Binary and multi-class classification\n",
    "    - He/Xavier weight initialization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, activations, learning_rate=0.01, init_method='he'):\n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        - layer_sizes: list like [input_size, hidden1, hidden2, ..., output_size]\n",
    "        - activations: list of activations for each layer (excluding input)\n",
    "        - learning_rate: step size for gradient descent\n",
    "        - init_method: 'random', 'xavier', or 'he'\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activations = activations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.parameters = self._initialize_parameters(init_method)\n",
    "        self.history = {'loss': [], 'accuracy': []}\n",
    "        \n",
    "    def _initialize_parameters(self, init_method):\n",
    "        \"\"\"Initialize weights and biases.\"\"\"\n",
    "        parameters = {}\n",
    "        L = len(self.layer_sizes) - 1\n",
    "        \n",
    "        for l in range(1, L + 1):\n",
    "            n_current = self.layer_sizes[l]\n",
    "            n_prev = self.layer_sizes[l - 1]\n",
    "            \n",
    "            if init_method == 'he':\n",
    "                W = np.random.randn(n_current, n_prev) * np.sqrt(2 / n_prev)\n",
    "            elif init_method == 'xavier':\n",
    "                W = np.random.randn(n_current, n_prev) * np.sqrt(1 / n_prev)\n",
    "            else:\n",
    "                W = np.random.randn(n_current, n_prev) * 0.01\n",
    "            \n",
    "            b = np.zeros((n_current, 1))\n",
    "            parameters[f'W{l}'] = W\n",
    "            parameters[f'b{l}'] = b\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def _activate(self, Z, activation):\n",
    "        \"\"\"Apply activation function.\"\"\"\n",
    "        if activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
    "        elif activation == 'relu':\n",
    "            return np.maximum(0, Z)\n",
    "        elif activation == 'tanh':\n",
    "            return np.tanh(Z)\n",
    "        elif activation == 'softmax':\n",
    "            exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "            return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "        return Z\n",
    "    \n",
    "    def _activate_derivative(self, Z, activation):\n",
    "        \"\"\"Compute derivative of activation function.\"\"\"\n",
    "        if activation == 'sigmoid':\n",
    "            s = self._activate(Z, 'sigmoid')\n",
    "            return s * (1 - s)\n",
    "        elif activation == 'relu':\n",
    "            return (Z > 0).astype(float)\n",
    "        elif activation == 'tanh':\n",
    "            return 1 - np.tanh(Z) ** 2\n",
    "        return np.ones_like(Z)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation.\"\"\"\n",
    "        self.cache = {'A0': X}\n",
    "        A = X\n",
    "        L = len(self.activations)\n",
    "        \n",
    "        for l in range(1, L + 1):\n",
    "            W = self.parameters[f'W{l}']\n",
    "            b = self.parameters[f'b{l}']\n",
    "            \n",
    "            Z = np.dot(W, A) + b\n",
    "            A = self._activate(Z, self.activations[l - 1])\n",
    "            \n",
    "            self.cache[f'Z{l}'] = Z\n",
    "            self.cache[f'A{l}'] = A\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, Y):\n",
    "        \"\"\"Backward propagation.\"\"\"\n",
    "        gradients = {}\n",
    "        L = len(self.activations)\n",
    "        m = Y.shape[1]\n",
    "        \n",
    "        AL = self.cache[f'A{L}']\n",
    "        \n",
    "        # Output layer gradient (cross-entropy with sigmoid/softmax)\n",
    "        dZ = AL - Y\n",
    "        \n",
    "        for l in reversed(range(1, L + 1)):\n",
    "            A_prev = self.cache[f'A{l-1}']\n",
    "            W = self.parameters[f'W{l}']\n",
    "            \n",
    "            gradients[f'dW{l}'] = (1/m) * np.dot(dZ, A_prev.T)\n",
    "            gradients[f'db{l}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            \n",
    "            if l > 1:\n",
    "                dA_prev = np.dot(W.T, dZ)\n",
    "                Z_prev = self.cache[f'Z{l-1}']\n",
    "                dZ = dA_prev * self._activate_derivative(Z_prev, self.activations[l-2])\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, gradients):\n",
    "        \"\"\"Update parameters using gradient descent.\"\"\"\n",
    "        L = len(self.activations)\n",
    "        \n",
    "        for l in range(1, L + 1):\n",
    "            self.parameters[f'W{l}'] -= self.learning_rate * gradients[f'dW{l}']\n",
    "            self.parameters[f'b{l}'] -= self.learning_rate * gradients[f'db{l}']\n",
    "    \n",
    "    def compute_loss(self, Y, Y_pred):\n",
    "        \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "        m = Y.shape[1]\n",
    "        epsilon = 1e-15\n",
    "        Y_pred = np.clip(Y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.sum(Y * np.log(Y_pred)) / m\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, Y, epochs=1000, verbose=True, print_every=100):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Training data, shape (n_features, n_samples)\n",
    "        - Y: Labels, shape (n_classes, n_samples)\n",
    "        - epochs: Number of training iterations\n",
    "        - verbose: Print progress\n",
    "        - print_every: Print frequency\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            Y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(Y, Y_pred)\n",
    "            \n",
    "            # Backward propagation\n",
    "            gradients = self.backward(Y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.update_parameters(gradients)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            predictions = np.argmax(Y_pred, axis=0)\n",
    "            labels = np.argmax(Y, axis=0)\n",
    "            accuracy = np.mean(predictions == labels)\n",
    "            \n",
    "            # Store history\n",
    "            self.history['loss'].append(loss)\n",
    "            self.history['accuracy'].append(accuracy)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch % print_every == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch:5d}/{epochs} - Loss: {loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        Y_pred = self.forward(X)\n",
    "        return np.argmax(Y_pred, axis=0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get probability predictions.\"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "print(\"\\nNeuralNetwork class created!\")\n",
    "print(\"\\nFeatures:\")\n",
    "print(\"  - Custom architecture (any number of layers)\")\n",
    "print(\"  - Multiple activation functions (relu, sigmoid, tanh, softmax)\")\n",
    "print(\"  - He/Xavier weight initialization\")\n",
    "print(\"  - Gradient descent optimization\")\n",
    "print(\"  - Training history tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part9'></a>\n",
    "# Part 9: Training on MNIST\n",
    "\n",
    "---\n",
    "\n",
    "Let's test our neural network on the famous **MNIST dataset** (handwritten digits)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD AND PREPARE MNIST DATA (KAGGLE)\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING MNIST DATASET FROM KAGGLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# DATA SOURCE OPTIONS\n",
    "# ============================================================\n",
    "# Option 1: Kaggle Dataset (when running on Kaggle)\n",
    "# Option 2: sklearn fetch_openml (when running locally)\n",
    "# ============================================================\n",
    "\n",
    "USE_KAGGLE = True  # Set to False if running locally\n",
    "\n",
    "if USE_KAGGLE:\n",
    "    # ============================================================\n",
    "    # KAGGLE: Load from CSV files\n",
    "    # ============================================================\n",
    "    # Dataset: https://www.kaggle.com/competitions/digit-recognizer\n",
    "    \n",
    "    KAGGLE_PATH = '/kaggle/input/digit-recognizer'\n",
    "    \n",
    "    print(\"\\nLoading from Kaggle dataset...\")\n",
    "    print(f\"Path: {KAGGLE_PATH}\")\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    try:\n",
    "        # Load training data\n",
    "        train_df = pd.read_csv(f'{KAGGLE_PATH}/train.csv')\n",
    "        \n",
    "        # Separate features and labels\n",
    "        y = train_df['label'].values\n",
    "        X = train_df.drop('label', axis=1).values\n",
    "        \n",
    "        print(f\"\\nDataset loaded from Kaggle!\")\n",
    "        print(f\"  Samples: {X.shape[0]}\")\n",
    "        print(f\"  Features: {X.shape[1]} (28x28 pixels)\")\n",
    "        print(f\"  Classes: {len(np.unique(y))} (digits 0-9)\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"ERROR: Kaggle dataset not found!\")\n",
    "        print(\"Make sure you've added the 'digit-recognizer' dataset to your notebook.\")\n",
    "        print(\"Go to: Add Data -> Competition Data -> digit-recognizer\")\n",
    "        USE_KAGGLE = False\n",
    "\n",
    "if not USE_KAGGLE:\n",
    "    # ============================================================\n",
    "    # LOCAL: Load using sklearn\n",
    "    # ============================================================\n",
    "    print(\"\\nLoading from sklearn (local)...\")\n",
    "    print(\"Downloading MNIST (this may take a moment)...\")\n",
    "    \n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    X, y = mnist.data, mnist.target.astype(int)\n",
    "    \n",
    "    print(f\"\\nDataset loaded!\")\n",
    "    print(f\"  Samples: {X.shape[0]}\")\n",
    "    print(f\"  Features: {X.shape[1]} (28x28 pixels)\")\n",
    "    print(f\"  Classes: {len(np.unique(y))} (digits 0-9)\")\n",
    "\n",
    "# ============================================================\n",
    "# PREPARE DATA\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPARING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use subset for faster training (optional)\n",
    "n_samples = min(20000, len(X))  # Use up to 20,000 samples\n",
    "X = X[:n_samples]\n",
    "y = y[:n_samples]\n",
    "\n",
    "print(f\"\\nUsing {n_samples} samples for training\")\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "X = X / 255.0\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Transpose for our neural network (features x samples)\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "\n",
    "# One-hot encode labels\n",
    "def one_hot_encode(y, num_classes=10):\n",
    "    \"\"\"Convert labels to one-hot encoding.\"\"\"\n",
    "    m = y.shape[0]\n",
    "    Y = np.zeros((num_classes, m))\n",
    "    Y[y, np.arange(m)] = 1\n",
    "    return Y\n",
    "\n",
    "Y_train = one_hot_encode(y_train)\n",
    "Y_test = one_hot_encode(y_test)\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  X_train: {X_train.shape} (features x samples)\")\n",
    "print(f\"  Y_train: {Y_train.shape} (classes x samples)\")\n",
    "print(f\"  X_test:  {X_test.shape}\")\n",
    "print(f\"  Y_test:  {Y_test.shape}\")\n",
    "\n",
    "print(f\"\\nTrain samples: {X_train.shape[1]}\")\n",
    "print(f\"Test samples:  {X_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "print(\"=\"*70)\n",
    "print(\"SAMPLE IMAGES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = X_train[:, i].reshape(28, 28)\n",
    "    label = y_train[i]\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'Label: {label}', fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample MNIST Images', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN NEURAL NETWORK ON MNIST\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING NEURAL NETWORK ON MNIST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define network architecture\n",
    "# 784 inputs -> 128 hidden -> 64 hidden -> 10 outputs\n",
    "layer_sizes = [784, 128, 64, 10]\n",
    "activations = ['relu', 'relu', 'softmax']\n",
    "\n",
    "print(f\"\\nArchitecture: {layer_sizes}\")\n",
    "print(f\"Activations: {activations}\")\n",
    "\n",
    "# Create and train network\n",
    "nn = NeuralNetwork(\n",
    "    layer_sizes=layer_sizes,\n",
    "    activations=activations,\n",
    "    learning_rate=0.1,\n",
    "    init_method='he'\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal parameters: {sum(nn.parameters[f'W{l}'].size + nn.parameters[f'b{l}'].size for l in range(1, len(layer_sizes)))}\")\n",
    "print(f\"Learning rate: {nn.learning_rate}\")\n",
    "print(\"\\nTraining...\\n\")\n",
    "\n",
    "# Train\n",
    "nn.fit(X_train, Y_train, epochs=500, verbose=True, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING PROGRESS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(nn.history['loss'], 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss', fontweight='bold', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(nn.history['accuracy'], 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training Accuracy', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Training Loss: {nn.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Training Accuracy: {nn.history['accuracy'][-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUATION ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_test = nn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = np.mean(y_pred_test == y_test)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=range(10), yticklabels=range(10))\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "ax.set_title('Confusion Matrix', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_test, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "print(\"=\"*70)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Random samples\n",
    "n_show = 15\n",
    "indices = np.random.choice(X_test.shape[1], n_show, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(12, 8))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = indices[i]\n",
    "    img = X_test[:, idx].reshape(28, 28)\n",
    "    true_label = y_test[idx]\n",
    "    pred_label = y_pred_test[idx]\n",
    "    \n",
    "    ax.imshow(img, cmap='gray')\n",
    "    \n",
    "    color = 'green' if true_label == pred_label else 'red'\n",
    "    ax.set_title(f'True: {true_label}, Pred: {pred_label}', \n",
    "                 fontweight='bold', color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample Predictions (Green=Correct, Red=Wrong)', \n",
    "             fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part10'></a>\n",
    "# Part 10: CNN Concepts Overview\n",
    "\n",
    "---\n",
    "\n",
    "## 10.1 Why CNNs for Images?\n",
    "\n",
    "Our fully-connected network treats each pixel independently. But images have **spatial structure**!\n",
    "\n",
    "| Fully-Connected | CNN |\n",
    "|-----------------|-----|\n",
    "| All pixels connected to all neurons | Local connections (kernels) |\n",
    "| No spatial awareness | Exploits spatial patterns |\n",
    "| Many parameters | Fewer parameters (weight sharing) |\n",
    "| For tabular data | For images, video, spatial data |\n",
    "\n",
    "## 10.2 Key CNN Components\n",
    "\n",
    "| Component | Purpose | How It Works |\n",
    "|-----------|---------|-------------|\n",
    "| **Convolution** | Extract features | Slide kernel over image, compute dot product |\n",
    "| **Pooling** | Reduce size | Take max/average of local region |\n",
    "| **ReLU** | Non-linearity | Same as before: max(0, x) |\n",
    "| **Fully Connected** | Classification | Same as our neural network! |\n",
    "\n",
    "## 10.3 Convolution Operation\n",
    "\n",
    "A **kernel** (filter) slides over the image, performing element-wise multiplication and sum:\n",
    "\n",
    "```\n",
    "Input Image (5x5)           Kernel (3x3)            Output (3x3)\n",
    "[1 2 3 0 1]                 [1 0 1]                 \n",
    "[0 1 2 3 0]     *           [0 1 0]      =         [? ? ?]\n",
    "[1 0 1 2 1]                 [1 0 1]                 [? ? ?]\n",
    "[2 1 0 1 0]                                         [? ? ?]\n",
    "[0 1 2 0 1]\n",
    "```\n",
    "\n",
    "### Common Kernels:\n",
    "\n",
    "| Kernel Type | Effect |\n",
    "|-------------|--------|\n",
    "| Edge Detection | Highlights edges |\n",
    "| Blur | Smooths image |\n",
    "| Sharpen | Enhances details |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CNN CONCEPTS: CONVOLUTION DEMONSTRATION\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"CONVOLUTION DEMONSTRATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def convolve2d(image, kernel):\n",
    "    \"\"\"Simple 2D convolution (no padding).\"\"\"\n",
    "    h, w = image.shape\n",
    "    kh, kw = kernel.shape\n",
    "    output = np.zeros((h - kh + 1, w - kw + 1))\n",
    "    \n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            region = image[i:i+kh, j:j+kw]\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Sample image (MNIST digit)\n",
    "sample_img = X_train[:, 0].reshape(28, 28)\n",
    "\n",
    "# Define kernels\n",
    "kernels = {\n",
    "    'Edge Detection': np.array([[-1, -1, -1],\n",
    "                                 [-1,  8, -1],\n",
    "                                 [-1, -1, -1]]),\n",
    "    \n",
    "    'Box Blur': np.array([[1, 1, 1],\n",
    "                          [1, 1, 1],\n",
    "                          [1, 1, 1]]) / 9,\n",
    "    \n",
    "    'Sharpen': np.array([[ 0, -1,  0],\n",
    "                         [-1,  5, -1],\n",
    "                         [ 0, -1,  0]]),\n",
    "    \n",
    "    'Horizontal Edge': np.array([[-1, -1, -1],\n",
    "                                  [ 0,  0,  0],\n",
    "                                  [ 1,  1,  1]]),\n",
    "    \n",
    "    'Vertical Edge': np.array([[-1, 0, 1],\n",
    "                                [-1, 0, 1],\n",
    "                                [-1, 0, 1]])\n",
    "}\n",
    "\n",
    "# Apply kernels\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 10))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(sample_img, cmap='gray')\n",
    "axes[0, 0].set_title('Original Image', fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Apply each kernel\n",
    "for idx, (name, kernel) in enumerate(kernels.items()):\n",
    "    row, col = (idx + 1) // 3, (idx + 1) % 3\n",
    "    \n",
    "    convolved = convolve2d(sample_img, kernel)\n",
    "    \n",
    "    axes[row, col].imshow(convolved, cmap='gray')\n",
    "    axes[row, col].set_title(f'{name}', fontweight='bold')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle('Convolution with Different Kernels', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCNN learns these kernels automatically during training!\")\n",
    "print(\"Early layers learn edges, later layers learn complex patterns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max pooling demonstration\n",
    "print(\"=\"*70)\n",
    "print(\"MAX POOLING DEMONSTRATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def max_pool2d(image, pool_size=2):\n",
    "    \"\"\"Max pooling operation.\"\"\"\n",
    "    h, w = image.shape\n",
    "    new_h, new_w = h // pool_size, w // pool_size\n",
    "    output = np.zeros((new_h, new_w))\n",
    "    \n",
    "    for i in range(new_h):\n",
    "        for j in range(new_w):\n",
    "            region = image[i*pool_size:(i+1)*pool_size, \n",
    "                          j*pool_size:(j+1)*pool_size]\n",
    "            output[i, j] = np.max(region)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Apply max pooling\n",
    "pooled = max_pool2d(sample_img, pool_size=2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(sample_img, cmap='gray')\n",
    "axes[0].set_title(f'Original: {sample_img.shape}', fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(pooled, cmap='gray')\n",
    "axes[1].set_title(f'After Max Pooling: {pooled.shape}', fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle('Max Pooling (2x2)', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMax Pooling:\")\n",
    "print(f\"  - Reduces spatial dimensions by {pooled.shape[0]/sample_img.shape[0]:.0%}\")\n",
    "print(\"  - Provides translation invariance\")\n",
    "print(\"  - Reduces computation and overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 CNN Architecture Summary\n",
    "\n",
    "```\n",
    "Input Image (28x28x1)\n",
    "       \u2193\n",
    "Conv Layer (32 filters, 3x3) \u2192 ReLU \u2192 (26x26x32)\n",
    "       \u2193\n",
    "Max Pool (2x2) \u2192 (13x13x32)\n",
    "       \u2193\n",
    "Conv Layer (64 filters, 3x3) \u2192 ReLU \u2192 (11x11x64)\n",
    "       \u2193\n",
    "Max Pool (2x2) \u2192 (5x5x64)\n",
    "       \u2193\n",
    "Flatten \u2192 (1600)\n",
    "       \u2193\n",
    "Fully Connected (128) \u2192 ReLU\n",
    "       \u2193\n",
    "Fully Connected (10) \u2192 Softmax\n",
    "       \u2193\n",
    "Output (10 classes)\n",
    "```\n",
    "\n",
    "**For CNNs, use TensorFlow/PyTorch!** They have optimized implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part11'></a>\n",
    "# Part 11: Summary and Key Takeaways\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"=\"*70)\n",
    "print(\"NEURAL NETWORK FROM SCRATCH - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "WHAT WE BUILT:\n",
    "==============\n",
    "A complete neural network from scratch using only NumPy!\n",
    "\n",
    "KEY COMPONENTS:\n",
    "===============\n",
    "1. NEURONS: Basic unit that computes weighted sum + activation\n",
    "   z = Wx + b, a = g(z)\n",
    "\n",
    "2. ACTIVATION FUNCTIONS:\n",
    "   - Sigmoid: 1/(1+e^-z) \u2192 Output layer (binary)\n",
    "   - ReLU: max(0,z) \u2192 Hidden layers (most popular)\n",
    "   - Softmax: e^zi/\u03a3e^zj \u2192 Output layer (multi-class)\n",
    "\n",
    "3. FORWARD PROPAGATION:\n",
    "   X \u2192 Z1 \u2192 A1 \u2192 Z2 \u2192 A2 \u2192 ... \u2192 \u0177\n",
    "\n",
    "4. LOSS FUNCTIONS:\n",
    "   - Cross-Entropy: -\u03a3(y\u00b7log(\u0177)) \u2192 Classification\n",
    "   - MSE: \u03a3(y-\u0177)\u00b2 \u2192 Regression\n",
    "\n",
    "5. BACKPROPAGATION:\n",
    "   Compute gradients using chain rule, backward through network\n",
    "   dW = (1/m) dZ \u00b7 A_prev^T\n",
    "   db = (1/m) \u03a3 dZ\n",
    "\n",
    "6. GRADIENT DESCENT:\n",
    "   W_new = W_old - \u03b1 \u00b7 dW\n",
    "   b_new = b_old - \u03b1 \u00b7 db\n",
    "\n",
    "RESULTS ON MNIST:\n",
    "=================\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Architecture: {layer_sizes}\")\n",
    "print(f\"Training Accuracy: {nn.history['accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "print(\"\"\"\n",
    "CNN CONCEPTS:\n",
    "=============\n",
    "- Convolution: Extract spatial features with kernels\n",
    "- Pooling: Reduce dimensions, add translation invariance\n",
    "- Architecture: Conv \u2192 ReLU \u2192 Pool \u2192 ... \u2192 FC \u2192 Softmax\n",
    "\n",
    "WHAT'S NEXT:\n",
    "============\n",
    "- Use TensorFlow/PyTorch for production\n",
    "- Add regularization (dropout, L2)\n",
    "- Try advanced optimizers (Adam, RMSprop)\n",
    "- Build CNNs for image classification\n",
    "- Explore RNNs for sequences\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Formulas Cheat Sheet\n",
    "\n",
    "### Forward Propagation\n",
    "| Step | Formula |\n",
    "|------|---------|\n",
    "| Linear | $Z^{[l]} = W^{[l]} \\cdot A^{[l-1]} + b^{[l]}$ |\n",
    "| Activation | $A^{[l]} = g(Z^{[l]})$ |\n",
    "\n",
    "### Backpropagation\n",
    "| Step | Formula |\n",
    "|------|---------|\n",
    "| Output gradient | $dZ^{[L]} = A^{[L]} - Y$ |\n",
    "| Weight gradient | $dW^{[l]} = \\frac{1}{m} dZ^{[l]} \\cdot A^{[l-1]T}$ |\n",
    "| Bias gradient | $db^{[l]} = \\frac{1}{m} \\sum dZ^{[l]}$ |\n",
    "| Previous layer | $dA^{[l-1]} = W^{[l]T} \\cdot dZ^{[l]}$ |\n",
    "| Activation deriv | $dZ^{[l-1]} = dA^{[l-1]} * g'(Z^{[l-1]})$ |\n",
    "\n",
    "### Gradient Descent\n",
    "| Update | Formula |\n",
    "|--------|---------|\n",
    "| Weights | $W := W - \\alpha \\cdot dW$ |\n",
    "| Biases | $b := b - \\alpha \\cdot db$ |\n",
    "\n",
    "---\n",
    "\n",
    "## Checklist\n",
    "\n",
    "- [x] Understood neuron structure (weights, bias, activation)\n",
    "- [x] Implemented activation functions (Sigmoid, ReLU, Tanh, Softmax)\n",
    "- [x] Learned activation function derivatives\n",
    "- [x] Built neural network architecture\n",
    "- [x] Implemented forward propagation\n",
    "- [x] Understood loss functions (Cross-Entropy, MSE)\n",
    "- [x] Implemented backpropagation with chain rule\n",
    "- [x] Implemented gradient descent\n",
    "- [x] Built complete NeuralNetwork class from scratch\n",
    "- [x] Trained on MNIST dataset\n",
    "- [x] Understood CNN concepts (convolution, pooling)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You now understand how neural networks work at a fundamental level!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}