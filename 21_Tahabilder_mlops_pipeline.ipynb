{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 21: MLOps Pipeline\n",
    "\n",
    "**Implement model versioning, monitoring, and retraining**\n",
    "\n",
    "In this tutorial, we'll build a complete MLOps framework that handles the entire ML lifecycle:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                        MLOps Pipeline                               │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│  ┌──────────┐   ┌──────────┐   ┌──────────┐   ┌──────────────────┐ │\n",
    "│  │   Data   │──▶│  Model   │──▶│  Model   │──▶│   Deployment     │ │\n",
    "│  │  Prep    │   │ Training │   │ Registry │   │   (Staging/Prod) │ │\n",
    "│  └──────────┘   └──────────┘   └──────────┘   └──────────────────┘ │\n",
    "│       │              │              │                   │          │\n",
    "│       ▼              ▼              ▼                   ▼          │\n",
    "│  ┌──────────┐   ┌──────────┐   ┌──────────┐   ┌──────────────────┐ │\n",
    "│  │ Feature  │   │Experiment│   │ Version  │   │   Monitoring     │ │\n",
    "│  │  Store   │   │ Tracking │   │ Control  │   │   & Alerting     │ │\n",
    "│  └──────────┘   └──────────┘   └──────────┘   └──────────────────┘ │\n",
    "│                                                       │            │\n",
    "│                      ┌────────────────────────────────┘            │\n",
    "│                      ▼                                             │\n",
    "│              ┌──────────────┐                                      │\n",
    "│              │  Auto        │◀── Drift Detection                   │\n",
    "│              │  Retrain     │◀── Performance Decay                 │\n",
    "│              └──────────────┘                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Components we'll build:**\n",
    "1. Experiment Tracker (like MLflow)\n",
    "2. Model Registry & Versioning\n",
    "3. Feature Store\n",
    "4. Data Drift Detection\n",
    "5. Model Performance Monitoring\n",
    "6. Automated Retraining Pipeline\n",
    "7. A/B Testing Framework\n",
    "8. Complete MLOps Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Installation](#1-setup-and-installation)\n",
    "2. [MLOps Concepts Overview](#2-mlops-concepts-overview)\n",
    "3. [Experiment Tracker](#3-experiment-tracker)\n",
    "4. [Model Registry](#4-model-registry)\n",
    "5. [Feature Store](#5-feature-store)\n",
    "6. [Data Drift Detection](#6-data-drift-detection)\n",
    "7. [Model Performance Monitoring](#7-model-performance-monitoring)\n",
    "8. [Automated Retraining Pipeline](#8-automated-retraining-pipeline)\n",
    "9. [A/B Testing Framework](#9-ab-testing-framework)\n",
    "10. [Complete MLOps Pipeline](#10-complete-mlops-pipeline)\n",
    "11. [Demo: Full MLOps Simulation](#11-demo-full-mlops-simulation)\n",
    "12. [Summary](#12-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All standard libraries - no special installation needed!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "import pickle\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any, Tuple, Union\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from enum import Enum\n",
    "import uuid\n",
    "import warnings\n",
    "import time\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Statistical tests for drift detection\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp, chi2_contingency, wasserstein_distance\n",
    "\n",
    "# Set seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Create MLOps directory\n",
    "MLOPS_DIR = Path('./mlops_artifacts')\n",
    "MLOPS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"MLOps Pipeline - All libraries loaded!\")\n",
    "print(f\"Artifacts directory: {MLOPS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLOps Concepts Overview\n",
    "\n",
    "### What is MLOps?\n",
    "\n",
    "MLOps (Machine Learning Operations) is a set of practices that combines ML, DevOps, and Data Engineering to deploy and maintain ML systems in production reliably and efficiently.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "| Component | Purpose | Production Tools |\n",
    "|-----------|---------|------------------|\n",
    "| **Experiment Tracking** | Log parameters, metrics, artifacts | MLflow, W&B, Neptune |\n",
    "| **Model Registry** | Version and manage models | MLflow, SageMaker |\n",
    "| **Feature Store** | Store and serve features | Feast, Tecton |\n",
    "| **Drift Detection** | Detect data/concept drift | Evidently, WhyLabs |\n",
    "| **Monitoring** | Track model performance | Prometheus, Grafana |\n",
    "| **CI/CD** | Automate testing/deployment | GitHub Actions, Jenkins |\n",
    "| **Orchestration** | Schedule and manage pipelines | Airflow, Kubeflow |\n",
    "\n",
    "### Model Lifecycle:\n",
    "\n",
    "```\n",
    "Development → Staging → Production → Monitoring → Retraining\n",
    "     ↑                                              ↓\n",
    "     └──────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define enums and data classes for MLOps\n",
    "\n",
    "class ModelStage(Enum):\n",
    "    \"\"\"Model deployment stages.\"\"\"\n",
    "    DEVELOPMENT = \"development\"\n",
    "    STAGING = \"staging\"\n",
    "    PRODUCTION = \"production\"\n",
    "    ARCHIVED = \"archived\"\n",
    "\n",
    "class AlertSeverity(Enum):\n",
    "    \"\"\"Alert severity levels.\"\"\"\n",
    "    INFO = \"info\"\n",
    "    WARNING = \"warning\"\n",
    "    CRITICAL = \"critical\"\n",
    "\n",
    "class DriftType(Enum):\n",
    "    \"\"\"Types of drift.\"\"\"\n",
    "    DATA_DRIFT = \"data_drift\"\n",
    "    CONCEPT_DRIFT = \"concept_drift\"\n",
    "    PREDICTION_DRIFT = \"prediction_drift\"\n",
    "\n",
    "@dataclass\n",
    "class ModelMetadata:\n",
    "    \"\"\"Metadata for a trained model.\"\"\"\n",
    "    model_id: str\n",
    "    model_name: str\n",
    "    version: str\n",
    "    created_at: str\n",
    "    stage: str\n",
    "    metrics: Dict[str, float]\n",
    "    parameters: Dict[str, Any]\n",
    "    tags: Dict[str, str] = field(default_factory=dict)\n",
    "    description: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class Alert:\n",
    "    \"\"\"Monitoring alert.\"\"\"\n",
    "    alert_id: str\n",
    "    timestamp: str\n",
    "    severity: str\n",
    "    message: str\n",
    "    metric_name: str\n",
    "    metric_value: float\n",
    "    threshold: float\n",
    "\n",
    "print(\"MLOps data structures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment Tracker\n",
    "\n",
    "Track experiments with parameters, metrics, and artifacts - similar to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentTracker:\n",
    "    \"\"\"\n",
    "    Track ML experiments with parameters, metrics, and artifacts.\n",
    "    \n",
    "    Similar to MLflow tracking but file-based for portability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tracking_dir: str = './mlops_artifacts/experiments'):\n",
    "        self.tracking_dir = Path(tracking_dir)\n",
    "        self.tracking_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.current_run = None\n",
    "        self.runs = self._load_runs()\n",
    "    \n",
    "    def _load_runs(self) -> Dict:\n",
    "        \"\"\"Load existing runs from disk.\"\"\"\n",
    "        runs_file = self.tracking_dir / 'runs.json'\n",
    "        if runs_file.exists():\n",
    "            with open(runs_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_runs(self):\n",
    "        \"\"\"Save runs to disk.\"\"\"\n",
    "        with open(self.tracking_dir / 'runs.json', 'w') as f:\n",
    "            json.dump(self.runs, f, indent=2, default=str)\n",
    "    \n",
    "    def start_run(self, experiment_name: str, run_name: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Start a new experiment run.\n",
    "        \n",
    "        Returns:\n",
    "            run_id: Unique identifier for this run\n",
    "        \"\"\"\n",
    "        run_id = str(uuid.uuid4())[:8]\n",
    "        run_name = run_name or f\"run_{run_id}\"\n",
    "        \n",
    "        self.current_run = {\n",
    "            'run_id': run_id,\n",
    "            'run_name': run_name,\n",
    "            'experiment_name': experiment_name,\n",
    "            'start_time': datetime.now().isoformat(),\n",
    "            'end_time': None,\n",
    "            'status': 'running',\n",
    "            'parameters': {},\n",
    "            'metrics': {},\n",
    "            'artifacts': [],\n",
    "            'tags': {}\n",
    "        }\n",
    "        \n",
    "        print(f\"Started run: {run_name} (ID: {run_id})\")\n",
    "        return run_id\n",
    "    \n",
    "    def log_param(self, key: str, value: Any):\n",
    "        \"\"\"Log a parameter.\"\"\"\n",
    "        if self.current_run is None:\n",
    "            raise ValueError(\"No active run. Call start_run() first.\")\n",
    "        self.current_run['parameters'][key] = value\n",
    "    \n",
    "    def log_params(self, params: Dict[str, Any]):\n",
    "        \"\"\"Log multiple parameters.\"\"\"\n",
    "        for key, value in params.items():\n",
    "            self.log_param(key, value)\n",
    "    \n",
    "    def log_metric(self, key: str, value: float, step: int = None):\n",
    "        \"\"\"Log a metric.\"\"\"\n",
    "        if self.current_run is None:\n",
    "            raise ValueError(\"No active run. Call start_run() first.\")\n",
    "        \n",
    "        if key not in self.current_run['metrics']:\n",
    "            self.current_run['metrics'][key] = []\n",
    "        \n",
    "        self.current_run['metrics'][key].append({\n",
    "            'value': value,\n",
    "            'step': step,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    def log_metrics(self, metrics: Dict[str, float], step: int = None):\n",
    "        \"\"\"Log multiple metrics.\"\"\"\n",
    "        for key, value in metrics.items():\n",
    "            self.log_metric(key, value, step)\n",
    "    \n",
    "    def log_artifact(self, artifact_path: str, artifact_name: str = None):\n",
    "        \"\"\"Log an artifact (file).\"\"\"\n",
    "        if self.current_run is None:\n",
    "            raise ValueError(\"No active run. Call start_run() first.\")\n",
    "        \n",
    "        artifact_name = artifact_name or Path(artifact_path).name\n",
    "        self.current_run['artifacts'].append({\n",
    "            'name': artifact_name,\n",
    "            'path': str(artifact_path),\n",
    "            'logged_at': datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    def set_tag(self, key: str, value: str):\n",
    "        \"\"\"Set a tag on the current run.\"\"\"\n",
    "        if self.current_run is None:\n",
    "            raise ValueError(\"No active run. Call start_run() first.\")\n",
    "        self.current_run['tags'][key] = value\n",
    "    \n",
    "    def end_run(self, status: str = 'completed'):\n",
    "        \"\"\"End the current run.\"\"\"\n",
    "        if self.current_run is None:\n",
    "            raise ValueError(\"No active run.\")\n",
    "        \n",
    "        self.current_run['end_time'] = datetime.now().isoformat()\n",
    "        self.current_run['status'] = status\n",
    "        \n",
    "        # Store run\n",
    "        run_id = self.current_run['run_id']\n",
    "        self.runs[run_id] = self.current_run\n",
    "        self._save_runs()\n",
    "        \n",
    "        print(f\"Ended run: {self.current_run['run_name']} (Status: {status})\")\n",
    "        self.current_run = None\n",
    "    \n",
    "    def get_run(self, run_id: str) -> Dict:\n",
    "        \"\"\"Get a specific run by ID.\"\"\"\n",
    "        return self.runs.get(run_id)\n",
    "    \n",
    "    def search_runs(self, experiment_name: str = None, \n",
    "                    metric_name: str = None, \n",
    "                    min_value: float = None) -> List[Dict]:\n",
    "        \"\"\"Search runs with filters.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for run_id, run in self.runs.items():\n",
    "            if experiment_name and run['experiment_name'] != experiment_name:\n",
    "                continue\n",
    "            \n",
    "            if metric_name and metric_name in run['metrics']:\n",
    "                metric_values = [m['value'] for m in run['metrics'][metric_name]]\n",
    "                if min_value and max(metric_values) < min_value:\n",
    "                    continue\n",
    "            \n",
    "            results.append(run)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_best_run(self, experiment_name: str, metric_name: str, \n",
    "                     maximize: bool = True) -> Dict:\n",
    "        \"\"\"Get the best run for an experiment based on a metric.\"\"\"\n",
    "        runs = self.search_runs(experiment_name=experiment_name)\n",
    "        \n",
    "        best_run = None\n",
    "        best_value = -np.inf if maximize else np.inf\n",
    "        \n",
    "        for run in runs:\n",
    "            if metric_name in run['metrics']:\n",
    "                values = [m['value'] for m in run['metrics'][metric_name]]\n",
    "                value = max(values) if maximize else min(values)\n",
    "                \n",
    "                if (maximize and value > best_value) or (not maximize and value < best_value):\n",
    "                    best_value = value\n",
    "                    best_run = run\n",
    "        \n",
    "        return best_run\n",
    "    \n",
    "    def get_runs_df(self) -> pd.DataFrame:\n",
    "        \"\"\"Get all runs as a DataFrame.\"\"\"\n",
    "        rows = []\n",
    "        for run_id, run in self.runs.items():\n",
    "            row = {\n",
    "                'run_id': run_id,\n",
    "                'run_name': run['run_name'],\n",
    "                'experiment': run['experiment_name'],\n",
    "                'status': run['status'],\n",
    "                'start_time': run['start_time']\n",
    "            }\n",
    "            # Add final metric values\n",
    "            for metric_name, values in run['metrics'].items():\n",
    "                row[metric_name] = values[-1]['value'] if values else None\n",
    "            rows.append(row)\n",
    "        \n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "# Test experiment tracker\n",
    "tracker = ExperimentTracker()\n",
    "print(\"\\nExperimentTracker created!\")\n",
    "print(\"\\nCapabilities:\")\n",
    "print(\"  - log_param(), log_params()\")\n",
    "print(\"  - log_metric(), log_metrics()\")\n",
    "print(\"  - log_artifact()\")\n",
    "print(\"  - search_runs(), get_best_run()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Registry\n",
    "\n",
    "Version control for models with staging capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRegistry:\n",
    "    \"\"\"\n",
    "    Registry for versioning and managing ML models.\n",
    "    \n",
    "    Features:\n",
    "    - Model versioning\n",
    "    - Stage transitions (dev → staging → production)\n",
    "    - Model metadata storage\n",
    "    - Model loading/saving\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, registry_dir: str = './mlops_artifacts/model_registry'):\n",
    "        self.registry_dir = Path(registry_dir)\n",
    "        self.registry_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.registry = self._load_registry()\n",
    "    \n",
    "    def _load_registry(self) -> Dict:\n",
    "        \"\"\"Load registry from disk.\"\"\"\n",
    "        registry_file = self.registry_dir / 'registry.json'\n",
    "        if registry_file.exists():\n",
    "            with open(registry_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {'models': {}, 'versions': {}}\n",
    "    \n",
    "    def _save_registry(self):\n",
    "        \"\"\"Save registry to disk.\"\"\"\n",
    "        with open(self.registry_dir / 'registry.json', 'w') as f:\n",
    "            json.dump(self.registry, f, indent=2, default=str)\n",
    "    \n",
    "    def register_model(self, model, model_name: str, \n",
    "                       metrics: Dict[str, float],\n",
    "                       parameters: Dict[str, Any] = None,\n",
    "                       description: str = \"\",\n",
    "                       tags: Dict[str, str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Register a new model version.\n",
    "        \n",
    "        Returns:\n",
    "            version: Version string (e.g., 'v1', 'v2')\n",
    "        \"\"\"\n",
    "        # Initialize model entry if new\n",
    "        if model_name not in self.registry['models']:\n",
    "            self.registry['models'][model_name] = {\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'versions': [],\n",
    "                'latest_version': None,\n",
    "                'production_version': None,\n",
    "                'staging_version': None\n",
    "            }\n",
    "        \n",
    "        # Generate version\n",
    "        version_num = len(self.registry['models'][model_name]['versions']) + 1\n",
    "        version = f\"v{version_num}\"\n",
    "        \n",
    "        # Generate model ID\n",
    "        model_id = f\"{model_name}_{version}_{str(uuid.uuid4())[:8]}\"\n",
    "        \n",
    "        # Save model artifact\n",
    "        model_path = self.registry_dir / model_name / version\n",
    "        model_path.mkdir(parents=True, exist_ok=True)\n",
    "        joblib.dump(model, model_path / 'model.joblib')\n",
    "        \n",
    "        # Create metadata\n",
    "        metadata = ModelMetadata(\n",
    "            model_id=model_id,\n",
    "            model_name=model_name,\n",
    "            version=version,\n",
    "            created_at=datetime.now().isoformat(),\n",
    "            stage=ModelStage.DEVELOPMENT.value,\n",
    "            metrics=metrics,\n",
    "            parameters=parameters or {},\n",
    "            tags=tags or {},\n",
    "            description=description\n",
    "        )\n",
    "        \n",
    "        # Save metadata\n",
    "        with open(model_path / 'metadata.json', 'w') as f:\n",
    "            json.dump(asdict(metadata), f, indent=2)\n",
    "        \n",
    "        # Update registry\n",
    "        self.registry['models'][model_name]['versions'].append(version)\n",
    "        self.registry['models'][model_name]['latest_version'] = version\n",
    "        self.registry['versions'][model_id] = asdict(metadata)\n",
    "        \n",
    "        self._save_registry()\n",
    "        \n",
    "        print(f\"Registered model: {model_name} {version}\")\n",
    "        return version\n",
    "    \n",
    "    def transition_stage(self, model_name: str, version: str, stage: ModelStage):\n",
    "        \"\"\"\n",
    "        Transition model to a new stage.\n",
    "        \"\"\"\n",
    "        model_path = self.registry_dir / model_name / version / 'metadata.json'\n",
    "        \n",
    "        if not model_path.exists():\n",
    "            raise ValueError(f\"Model {model_name} {version} not found\")\n",
    "        \n",
    "        # Update metadata\n",
    "        with open(model_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        old_stage = metadata['stage']\n",
    "        metadata['stage'] = stage.value\n",
    "        \n",
    "        with open(model_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        # Update registry\n",
    "        if stage == ModelStage.PRODUCTION:\n",
    "            # Archive previous production model\n",
    "            old_prod = self.registry['models'][model_name].get('production_version')\n",
    "            if old_prod:\n",
    "                self.transition_stage(model_name, old_prod, ModelStage.ARCHIVED)\n",
    "            self.registry['models'][model_name]['production_version'] = version\n",
    "        elif stage == ModelStage.STAGING:\n",
    "            self.registry['models'][model_name]['staging_version'] = version\n",
    "        \n",
    "        # Update version metadata in registry\n",
    "        model_id = metadata['model_id']\n",
    "        self.registry['versions'][model_id]['stage'] = stage.value\n",
    "        \n",
    "        self._save_registry()\n",
    "        \n",
    "        print(f\"Transitioned {model_name} {version}: {old_stage} → {stage.value}\")\n",
    "    \n",
    "    def load_model(self, model_name: str, version: str = None, \n",
    "                   stage: ModelStage = None):\n",
    "        \"\"\"\n",
    "        Load a model from the registry.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the model\n",
    "            version: Specific version (e.g., 'v1')\n",
    "            stage: Load model at specific stage (e.g., PRODUCTION)\n",
    "        \"\"\"\n",
    "        if stage:\n",
    "            if stage == ModelStage.PRODUCTION:\n",
    "                version = self.registry['models'][model_name].get('production_version')\n",
    "            elif stage == ModelStage.STAGING:\n",
    "                version = self.registry['models'][model_name].get('staging_version')\n",
    "        \n",
    "        if version is None:\n",
    "            version = self.registry['models'][model_name]['latest_version']\n",
    "        \n",
    "        model_path = self.registry_dir / model_name / version / 'model.joblib'\n",
    "        \n",
    "        if not model_path.exists():\n",
    "            raise ValueError(f\"Model {model_name} {version} not found\")\n",
    "        \n",
    "        return joblib.load(model_path)\n",
    "    \n",
    "    def get_model_metadata(self, model_name: str, version: str) -> Dict:\n",
    "        \"\"\"Get metadata for a specific model version.\"\"\"\n",
    "        metadata_path = self.registry_dir / model_name / version / 'metadata.json'\n",
    "        \n",
    "        if metadata_path.exists():\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "    \n",
    "    def list_models(self) -> List[str]:\n",
    "        \"\"\"List all registered models.\"\"\"\n",
    "        return list(self.registry['models'].keys())\n",
    "    \n",
    "    def list_versions(self, model_name: str) -> List[str]:\n",
    "        \"\"\"List all versions of a model.\"\"\"\n",
    "        if model_name in self.registry['models']:\n",
    "            return self.registry['models'][model_name]['versions']\n",
    "        return []\n",
    "    \n",
    "    def get_production_model(self, model_name: str):\n",
    "        \"\"\"Get the production model.\"\"\"\n",
    "        return self.load_model(model_name, stage=ModelStage.PRODUCTION)\n",
    "    \n",
    "    def compare_versions(self, model_name: str, versions: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Compare metrics across model versions.\"\"\"\n",
    "        versions = versions or self.list_versions(model_name)\n",
    "        \n",
    "        rows = []\n",
    "        for version in versions:\n",
    "            metadata = self.get_model_metadata(model_name, version)\n",
    "            if metadata:\n",
    "                row = {\n",
    "                    'version': version,\n",
    "                    'stage': metadata['stage'],\n",
    "                    'created_at': metadata['created_at'],\n",
    "                    **metadata['metrics']\n",
    "                }\n",
    "                rows.append(row)\n",
    "        \n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "# Test model registry\n",
    "registry = ModelRegistry()\n",
    "print(\"\\nModelRegistry created!\")\n",
    "print(\"\\nCapabilities:\")\n",
    "print(\"  - register_model()\")\n",
    "print(\"  - transition_stage()\")\n",
    "print(\"  - load_model()\")\n",
    "print(\"  - compare_versions()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Store\n",
    "\n",
    "A simple feature store for managing and serving features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureStore:\n",
    "    \"\"\"\n",
    "    Simple feature store for managing ML features.\n",
    "    \n",
    "    Features:\n",
    "    - Store feature definitions\n",
    "    - Track feature statistics\n",
    "    - Serve features for training/inference\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, store_dir: str = './mlops_artifacts/feature_store'):\n",
    "        self.store_dir = Path(store_dir)\n",
    "        self.store_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.feature_groups = self._load_store()\n",
    "    \n",
    "    def _load_store(self) -> Dict:\n",
    "        \"\"\"Load feature store from disk.\"\"\"\n",
    "        store_file = self.store_dir / 'feature_store.json'\n",
    "        if store_file.exists():\n",
    "            with open(store_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_store(self):\n",
    "        \"\"\"Save feature store to disk.\"\"\"\n",
    "        with open(self.store_dir / 'feature_store.json', 'w') as f:\n",
    "            json.dump(self.feature_groups, f, indent=2, default=str)\n",
    "    \n",
    "    def create_feature_group(self, name: str, description: str = \"\"):\n",
    "        \"\"\"Create a new feature group.\"\"\"\n",
    "        if name not in self.feature_groups:\n",
    "            self.feature_groups[name] = {\n",
    "                'name': name,\n",
    "                'description': description,\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'features': {},\n",
    "                'statistics': {}\n",
    "            }\n",
    "            self._save_store()\n",
    "            print(f\"Created feature group: {name}\")\n",
    "    \n",
    "    def register_features(self, group_name: str, df: pd.DataFrame, \n",
    "                          feature_cols: List[str] = None):\n",
    "        \"\"\"\n",
    "        Register features from a DataFrame.\n",
    "        \"\"\"\n",
    "        if group_name not in self.feature_groups:\n",
    "            self.create_feature_group(group_name)\n",
    "        \n",
    "        feature_cols = feature_cols or df.columns.tolist()\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            if col in df.columns:\n",
    "                # Store feature definition\n",
    "                self.feature_groups[group_name]['features'][col] = {\n",
    "                    'dtype': str(df[col].dtype),\n",
    "                    'registered_at': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                # Calculate and store statistics\n",
    "                if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    self.feature_groups[group_name]['statistics'][col] = {\n",
    "                        'mean': float(df[col].mean()),\n",
    "                        'std': float(df[col].std()),\n",
    "                        'min': float(df[col].min()),\n",
    "                        'max': float(df[col].max()),\n",
    "                        'missing_pct': float(df[col].isnull().mean() * 100)\n",
    "                    }\n",
    "                else:\n",
    "                    self.feature_groups[group_name]['statistics'][col] = {\n",
    "                        'n_unique': int(df[col].nunique()),\n",
    "                        'top_value': str(df[col].mode().iloc[0]) if len(df[col].mode()) > 0 else None,\n",
    "                        'missing_pct': float(df[col].isnull().mean() * 100)\n",
    "                    }\n",
    "        \n",
    "        # Save feature data\n",
    "        data_path = self.store_dir / group_name\n",
    "        data_path.mkdir(exist_ok=True)\n",
    "        df[feature_cols].to_parquet(data_path / 'features.parquet', index=False)\n",
    "        \n",
    "        self._save_store()\n",
    "        print(f\"Registered {len(feature_cols)} features to group '{group_name}'\")\n",
    "    \n",
    "    def get_features(self, group_name: str, feature_cols: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Get features from the store.\"\"\"\n",
    "        data_path = self.store_dir / group_name / 'features.parquet'\n",
    "        \n",
    "        if not data_path.exists():\n",
    "            raise ValueError(f\"Feature group '{group_name}' has no data\")\n",
    "        \n",
    "        df = pd.read_parquet(data_path)\n",
    "        \n",
    "        if feature_cols:\n",
    "            df = df[feature_cols]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_statistics(self, group_name: str) -> Dict:\n",
    "        \"\"\"Get feature statistics.\"\"\"\n",
    "        if group_name in self.feature_groups:\n",
    "            return self.feature_groups[group_name]['statistics']\n",
    "        return {}\n",
    "    \n",
    "    def list_feature_groups(self) -> List[str]:\n",
    "        \"\"\"List all feature groups.\"\"\"\n",
    "        return list(self.feature_groups.keys())\n",
    "    \n",
    "    def list_features(self, group_name: str) -> List[str]:\n",
    "        \"\"\"List features in a group.\"\"\"\n",
    "        if group_name in self.feature_groups:\n",
    "            return list(self.feature_groups[group_name]['features'].keys())\n",
    "        return []\n",
    "\n",
    "# Test feature store\n",
    "feature_store = FeatureStore()\n",
    "print(\"\\nFeatureStore created!\")\n",
    "print(\"\\nCapabilities:\")\n",
    "print(\"  - create_feature_group()\")\n",
    "print(\"  - register_features()\")\n",
    "print(\"  - get_features()\")\n",
    "print(\"  - get_statistics()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Drift Detection\n",
    "\n",
    "Detect when data distribution changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriftDetector:\n",
    "    \"\"\"\n",
    "    Detect data drift using statistical tests.\n",
    "    \n",
    "    Methods:\n",
    "    - KS Test (Kolmogorov-Smirnov) for numeric features\n",
    "    - Chi-Square test for categorical features\n",
    "    - PSI (Population Stability Index)\n",
    "    - Wasserstein distance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, significance_level: float = 0.05, psi_threshold: float = 0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            significance_level: P-value threshold for statistical tests\n",
    "            psi_threshold: PSI threshold (>0.2 indicates significant drift)\n",
    "        \"\"\"\n",
    "        self.significance_level = significance_level\n",
    "        self.psi_threshold = psi_threshold\n",
    "        self.reference_data = None\n",
    "        self.reference_stats = {}\n",
    "    \n",
    "    def set_reference(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Set reference (baseline) data for comparison.\n",
    "        \"\"\"\n",
    "        self.reference_data = df.copy()\n",
    "        \n",
    "        # Calculate reference statistics\n",
    "        for col in df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                self.reference_stats[col] = {\n",
    "                    'type': 'numeric',\n",
    "                    'mean': df[col].mean(),\n",
    "                    'std': df[col].std(),\n",
    "                    'quantiles': df[col].quantile([0.25, 0.5, 0.75]).to_dict()\n",
    "                }\n",
    "            else:\n",
    "                self.reference_stats[col] = {\n",
    "                    'type': 'categorical',\n",
    "                    'value_counts': df[col].value_counts(normalize=True).to_dict()\n",
    "                }\n",
    "        \n",
    "        print(f\"Reference data set with {len(df)} samples, {len(df.columns)} features\")\n",
    "    \n",
    "    def detect_drift(self, current_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Detect drift between reference and current data.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with drift results per feature\n",
    "        \"\"\"\n",
    "        if self.reference_data is None:\n",
    "            raise ValueError(\"Reference data not set. Call set_reference() first.\")\n",
    "        \n",
    "        results = {\n",
    "            'overall_drift': False,\n",
    "            'n_drifted_features': 0,\n",
    "            'features': {},\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        common_cols = set(self.reference_data.columns) & set(current_df.columns)\n",
    "        \n",
    "        for col in common_cols:\n",
    "            if col not in self.reference_stats:\n",
    "                continue\n",
    "            \n",
    "            ref_data = self.reference_data[col].dropna()\n",
    "            cur_data = current_df[col].dropna()\n",
    "            \n",
    "            if self.reference_stats[col]['type'] == 'numeric':\n",
    "                drift_result = self._detect_numeric_drift(ref_data, cur_data, col)\n",
    "            else:\n",
    "                drift_result = self._detect_categorical_drift(ref_data, cur_data, col)\n",
    "            \n",
    "            results['features'][col] = drift_result\n",
    "            \n",
    "            if drift_result['is_drifted']:\n",
    "                results['n_drifted_features'] += 1\n",
    "        \n",
    "        # Overall drift if more than 20% of features drifted\n",
    "        drift_ratio = results['n_drifted_features'] / len(common_cols) if common_cols else 0\n",
    "        results['overall_drift'] = drift_ratio > 0.2\n",
    "        results['drift_ratio'] = drift_ratio\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _detect_numeric_drift(self, ref_data: pd.Series, cur_data: pd.Series, \n",
    "                               col_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Detect drift in numeric feature using KS test and PSI.\n",
    "        \"\"\"\n",
    "        # KS Test\n",
    "        ks_stat, ks_pvalue = ks_2samp(ref_data, cur_data)\n",
    "        \n",
    "        # PSI (Population Stability Index)\n",
    "        psi = self._calculate_psi(ref_data, cur_data)\n",
    "        \n",
    "        # Wasserstein distance\n",
    "        wasserstein = wasserstein_distance(ref_data, cur_data)\n",
    "        \n",
    "        # Mean shift\n",
    "        mean_shift = abs(ref_data.mean() - cur_data.mean()) / (ref_data.std() + 1e-10)\n",
    "        \n",
    "        is_drifted = (ks_pvalue < self.significance_level) or (psi > self.psi_threshold)\n",
    "        \n",
    "        return {\n",
    "            'type': 'numeric',\n",
    "            'is_drifted': is_drifted,\n",
    "            'ks_statistic': float(ks_stat),\n",
    "            'ks_pvalue': float(ks_pvalue),\n",
    "            'psi': float(psi),\n",
    "            'wasserstein_distance': float(wasserstein),\n",
    "            'mean_shift': float(mean_shift),\n",
    "            'ref_mean': float(ref_data.mean()),\n",
    "            'cur_mean': float(cur_data.mean())\n",
    "        }\n",
    "    \n",
    "    def _detect_categorical_drift(self, ref_data: pd.Series, cur_data: pd.Series,\n",
    "                                   col_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Detect drift in categorical feature using Chi-Square test.\n",
    "        \"\"\"\n",
    "        # Get all categories\n",
    "        all_cats = set(ref_data.unique()) | set(cur_data.unique())\n",
    "        \n",
    "        # Calculate frequencies\n",
    "        ref_counts = ref_data.value_counts()\n",
    "        cur_counts = cur_data.value_counts()\n",
    "        \n",
    "        # Align categories\n",
    "        ref_freq = [ref_counts.get(cat, 0) for cat in all_cats]\n",
    "        cur_freq = [cur_counts.get(cat, 0) for cat in all_cats]\n",
    "        \n",
    "        # Chi-Square test (need at least some counts)\n",
    "        if sum(ref_freq) > 0 and sum(cur_freq) > 0:\n",
    "            # Create contingency table\n",
    "            contingency = np.array([ref_freq, cur_freq])\n",
    "            # Remove zero columns\n",
    "            contingency = contingency[:, contingency.sum(axis=0) > 0]\n",
    "            \n",
    "            if contingency.shape[1] > 1:\n",
    "                chi2, pvalue, dof, expected = chi2_contingency(contingency)\n",
    "            else:\n",
    "                chi2, pvalue = 0, 1.0\n",
    "        else:\n",
    "            chi2, pvalue = 0, 1.0\n",
    "        \n",
    "        is_drifted = pvalue < self.significance_level\n",
    "        \n",
    "        return {\n",
    "            'type': 'categorical',\n",
    "            'is_drifted': is_drifted,\n",
    "            'chi2_statistic': float(chi2),\n",
    "            'chi2_pvalue': float(pvalue),\n",
    "            'n_categories_ref': len(ref_counts),\n",
    "            'n_categories_cur': len(cur_counts)\n",
    "        }\n",
    "    \n",
    "    def _calculate_psi(self, ref_data: pd.Series, cur_data: pd.Series, \n",
    "                       n_bins: int = 10) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Population Stability Index (PSI).\n",
    "        \n",
    "        PSI < 0.1: No significant change\n",
    "        0.1 <= PSI < 0.2: Moderate change\n",
    "        PSI >= 0.2: Significant change\n",
    "        \"\"\"\n",
    "        # Create bins based on reference data\n",
    "        bins = np.quantile(ref_data, np.linspace(0, 1, n_bins + 1))\n",
    "        bins = np.unique(bins)  # Remove duplicates\n",
    "        \n",
    "        if len(bins) < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate proportions\n",
    "        ref_counts, _ = np.histogram(ref_data, bins=bins)\n",
    "        cur_counts, _ = np.histogram(cur_data, bins=bins)\n",
    "        \n",
    "        ref_pct = ref_counts / len(ref_data)\n",
    "        cur_pct = cur_counts / len(cur_data)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        ref_pct = np.clip(ref_pct, 0.0001, None)\n",
    "        cur_pct = np.clip(cur_pct, 0.0001, None)\n",
    "        \n",
    "        # Calculate PSI\n",
    "        psi = np.sum((cur_pct - ref_pct) * np.log(cur_pct / ref_pct))\n",
    "        \n",
    "        return psi\n",
    "    \n",
    "    def get_drift_report(self, drift_results: Dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get drift results as a DataFrame.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        for feature, result in drift_results['features'].items():\n",
    "            row = {'feature': feature, **result}\n",
    "            rows.append(row)\n",
    "        \n",
    "        return pd.DataFrame(rows).sort_values('is_drifted', ascending=False)\n",
    "\n",
    "# Test drift detector\n",
    "drift_detector = DriftDetector()\n",
    "print(\"\\nDriftDetector created!\")\n",
    "print(\"\\nMethods:\")\n",
    "print(\"  - KS Test (numeric)\")\n",
    "print(\"  - Chi-Square Test (categorical)\")\n",
    "print(\"  - PSI (Population Stability Index)\")\n",
    "print(\"  - Wasserstein Distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Performance Monitoring\n",
    "\n",
    "Track model performance over time and alert on degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceMonitor:\n",
    "    \"\"\"\n",
    "    Monitor model performance over time with alerting.\n",
    "    \n",
    "    Features:\n",
    "    - Track metrics over time\n",
    "    - Detect performance degradation\n",
    "    - Generate alerts\n",
    "    - Visualize trends\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, monitor_dir: str = './mlops_artifacts/monitoring'):\n",
    "        self.monitor_dir = Path(monitor_dir)\n",
    "        self.monitor_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.metrics_history = defaultdict(list)\n",
    "        self.alerts = []\n",
    "        self.thresholds = {}\n",
    "        self.baseline_metrics = {}\n",
    "    \n",
    "    def set_baseline(self, metrics: Dict[str, float]):\n",
    "        \"\"\"\n",
    "        Set baseline metrics for comparison.\n",
    "        \"\"\"\n",
    "        self.baseline_metrics = metrics\n",
    "        print(f\"Baseline set: {metrics}\")\n",
    "    \n",
    "    def set_threshold(self, metric_name: str, min_value: float = None, \n",
    "                      max_value: float = None, max_degradation: float = None):\n",
    "        \"\"\"\n",
    "        Set alerting thresholds for a metric.\n",
    "        \n",
    "        Args:\n",
    "            metric_name: Name of the metric\n",
    "            min_value: Alert if metric falls below this\n",
    "            max_value: Alert if metric exceeds this\n",
    "            max_degradation: Max allowed % degradation from baseline\n",
    "        \"\"\"\n",
    "        self.thresholds[metric_name] = {\n",
    "            'min_value': min_value,\n",
    "            'max_value': max_value,\n",
    "            'max_degradation': max_degradation\n",
    "        }\n",
    "    \n",
    "    def log_metrics(self, metrics: Dict[str, float], \n",
    "                    timestamp: datetime = None,\n",
    "                    model_version: str = None) -> List[Alert]:\n",
    "        \"\"\"\n",
    "        Log metrics and check for alerts.\n",
    "        \n",
    "        Returns:\n",
    "            List of triggered alerts\n",
    "        \"\"\"\n",
    "        timestamp = timestamp or datetime.now()\n",
    "        new_alerts = []\n",
    "        \n",
    "        for metric_name, value in metrics.items():\n",
    "            # Store metric\n",
    "            self.metrics_history[metric_name].append({\n",
    "                'value': value,\n",
    "                'timestamp': timestamp.isoformat(),\n",
    "                'model_version': model_version\n",
    "            })\n",
    "            \n",
    "            # Check thresholds\n",
    "            if metric_name in self.thresholds:\n",
    "                alert = self._check_threshold(metric_name, value, timestamp)\n",
    "                if alert:\n",
    "                    new_alerts.append(alert)\n",
    "                    self.alerts.append(alert)\n",
    "        \n",
    "        return new_alerts\n",
    "    \n",
    "    def _check_threshold(self, metric_name: str, value: float, \n",
    "                         timestamp: datetime) -> Optional[Alert]:\n",
    "        \"\"\"\n",
    "        Check if metric violates thresholds.\n",
    "        \"\"\"\n",
    "        threshold = self.thresholds[metric_name]\n",
    "        \n",
    "        # Check minimum value\n",
    "        if threshold['min_value'] is not None and value < threshold['min_value']:\n",
    "            return Alert(\n",
    "                alert_id=str(uuid.uuid4())[:8],\n",
    "                timestamp=timestamp.isoformat(),\n",
    "                severity=AlertSeverity.CRITICAL.value,\n",
    "                message=f\"{metric_name} ({value:.4f}) below minimum threshold ({threshold['min_value']})\",\n",
    "                metric_name=metric_name,\n",
    "                metric_value=value,\n",
    "                threshold=threshold['min_value']\n",
    "            )\n",
    "        \n",
    "        # Check maximum value\n",
    "        if threshold['max_value'] is not None and value > threshold['max_value']:\n",
    "            return Alert(\n",
    "                alert_id=str(uuid.uuid4())[:8],\n",
    "                timestamp=timestamp.isoformat(),\n",
    "                severity=AlertSeverity.WARNING.value,\n",
    "                message=f\"{metric_name} ({value:.4f}) above maximum threshold ({threshold['max_value']})\",\n",
    "                metric_name=metric_name,\n",
    "                metric_value=value,\n",
    "                threshold=threshold['max_value']\n",
    "            )\n",
    "        \n",
    "        # Check degradation from baseline\n",
    "        if threshold['max_degradation'] is not None and metric_name in self.baseline_metrics:\n",
    "            baseline = self.baseline_metrics[metric_name]\n",
    "            degradation = (baseline - value) / baseline * 100\n",
    "            \n",
    "            if degradation > threshold['max_degradation']:\n",
    "                return Alert(\n",
    "                    alert_id=str(uuid.uuid4())[:8],\n",
    "                    timestamp=timestamp.isoformat(),\n",
    "                    severity=AlertSeverity.WARNING.value,\n",
    "                    message=f\"{metric_name} degraded by {degradation:.1f}% from baseline\",\n",
    "                    metric_name=metric_name,\n",
    "                    metric_value=value,\n",
    "                    threshold=baseline * (1 - threshold['max_degradation']/100)\n",
    "                )\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_metrics_df(self, metric_name: str = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get metrics history as DataFrame.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        metrics_to_get = [metric_name] if metric_name else self.metrics_history.keys()\n",
    "        \n",
    "        for name in metrics_to_get:\n",
    "            if name in self.metrics_history:\n",
    "                for entry in self.metrics_history[name]:\n",
    "                    rows.append({\n",
    "                        'metric': name,\n",
    "                        'value': entry['value'],\n",
    "                        'timestamp': entry['timestamp'],\n",
    "                        'model_version': entry.get('model_version')\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(rows)\n",
    "    \n",
    "    def get_alerts_df(self) -> pd.DataFrame:\n",
    "        \"\"\"Get alerts as DataFrame.\"\"\"\n",
    "        return pd.DataFrame([asdict(a) for a in self.alerts])\n",
    "    \n",
    "    def plot_metrics(self, metric_names: List[str] = None, figsize=(12, 6)):\n",
    "        \"\"\"\n",
    "        Plot metrics over time.\n",
    "        \"\"\"\n",
    "        metric_names = metric_names or list(self.metrics_history.keys())\n",
    "        \n",
    "        fig, axes = plt.subplots(len(metric_names), 1, figsize=(figsize[0], figsize[1] * len(metric_names)))\n",
    "        if len(metric_names) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for ax, metric_name in zip(axes, metric_names):\n",
    "            if metric_name in self.metrics_history:\n",
    "                values = [m['value'] for m in self.metrics_history[metric_name]]\n",
    "                timestamps = range(len(values))\n",
    "                \n",
    "                ax.plot(timestamps, values, 'b-o', label=metric_name)\n",
    "                \n",
    "                # Plot baseline\n",
    "                if metric_name in self.baseline_metrics:\n",
    "                    ax.axhline(y=self.baseline_metrics[metric_name], \n",
    "                              color='g', linestyle='--', label='Baseline')\n",
    "                \n",
    "                # Plot threshold\n",
    "                if metric_name in self.thresholds:\n",
    "                    if self.thresholds[metric_name]['min_value']:\n",
    "                        ax.axhline(y=self.thresholds[metric_name]['min_value'],\n",
    "                                  color='r', linestyle='--', label='Min Threshold')\n",
    "                \n",
    "                ax.set_xlabel('Time')\n",
    "                ax.set_ylabel(metric_name)\n",
    "                ax.set_title(f'{metric_name} Over Time')\n",
    "                ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def check_health(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Check overall model health.\n",
    "        \"\"\"\n",
    "        health = {\n",
    "            'status': 'healthy',\n",
    "            'issues': [],\n",
    "            'recent_alerts': len([a for a in self.alerts[-10:] \n",
    "                                  if a.severity == AlertSeverity.CRITICAL.value])\n",
    "        }\n",
    "        \n",
    "        # Check recent metrics\n",
    "        for metric_name, history in self.metrics_history.items():\n",
    "            if history:\n",
    "                recent_value = history[-1]['value']\n",
    "                \n",
    "                if metric_name in self.baseline_metrics:\n",
    "                    baseline = self.baseline_metrics[metric_name]\n",
    "                    degradation = (baseline - recent_value) / baseline * 100\n",
    "                    \n",
    "                    if degradation > 20:\n",
    "                        health['status'] = 'degraded'\n",
    "                        health['issues'].append(f\"{metric_name} degraded by {degradation:.1f}%\")\n",
    "                    elif degradation > 10:\n",
    "                        if health['status'] == 'healthy':\n",
    "                            health['status'] = 'warning'\n",
    "                        health['issues'].append(f\"{metric_name} degraded by {degradation:.1f}%\")\n",
    "        \n",
    "        if health['recent_alerts'] > 3:\n",
    "            health['status'] = 'critical'\n",
    "        \n",
    "        return health\n",
    "\n",
    "# Test performance monitor\n",
    "monitor = PerformanceMonitor()\n",
    "print(\"\\nPerformanceMonitor created!\")\n",
    "print(\"\\nCapabilities:\")\n",
    "print(\"  - log_metrics()\")\n",
    "print(\"  - set_threshold()\")\n",
    "print(\"  - check_health()\")\n",
    "print(\"  - plot_metrics()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Automated Retraining Pipeline\n",
    "\n",
    "Automatically retrain models when drift is detected or performance degrades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrainingPipeline:\n",
    "    \"\"\"\n",
    "    Automated model retraining pipeline.\n",
    "    \n",
    "    Triggers:\n",
    "    - Data drift detected\n",
    "    - Performance below threshold\n",
    "    - Scheduled retraining\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_registry: ModelRegistry,\n",
    "                 drift_detector: DriftDetector,\n",
    "                 performance_monitor: PerformanceMonitor,\n",
    "                 experiment_tracker: ExperimentTracker):\n",
    "        \n",
    "        self.registry = model_registry\n",
    "        self.drift_detector = drift_detector\n",
    "        self.monitor = performance_monitor\n",
    "        self.tracker = experiment_tracker\n",
    "        \n",
    "        self.retrain_history = []\n",
    "        self.model_factory = None  # Function to create new model\n",
    "    \n",
    "    def set_model_factory(self, factory_fn):\n",
    "        \"\"\"\n",
    "        Set the function used to create new models.\n",
    "        \n",
    "        Args:\n",
    "            factory_fn: Function that returns an untrained model\n",
    "        \"\"\"\n",
    "        self.model_factory = factory_fn\n",
    "    \n",
    "    def check_retrain_triggers(self, X_current: pd.DataFrame, \n",
    "                                y_current: pd.Series = None,\n",
    "                                model_name: str = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Check if retraining should be triggered.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with trigger status and reasons\n",
    "        \"\"\"\n",
    "        triggers = {\n",
    "            'should_retrain': False,\n",
    "            'reasons': [],\n",
    "            'drift_detected': False,\n",
    "            'performance_degraded': False\n",
    "        }\n",
    "        \n",
    "        # Check data drift\n",
    "        if self.drift_detector.reference_data is not None:\n",
    "            drift_results = self.drift_detector.detect_drift(X_current)\n",
    "            if drift_results['overall_drift']:\n",
    "                triggers['should_retrain'] = True\n",
    "                triggers['drift_detected'] = True\n",
    "                triggers['reasons'].append(\n",
    "                    f\"Data drift detected: {drift_results['n_drifted_features']} features drifted\"\n",
    "                )\n",
    "        \n",
    "        # Check performance\n",
    "        health = self.monitor.check_health()\n",
    "        if health['status'] in ['degraded', 'critical']:\n",
    "            triggers['should_retrain'] = True\n",
    "            triggers['performance_degraded'] = True\n",
    "            triggers['reasons'].extend(health['issues'])\n",
    "        \n",
    "        return triggers\n",
    "    \n",
    "    def retrain(self, X_train: pd.DataFrame, y_train: pd.Series,\n",
    "                X_val: pd.DataFrame, y_val: pd.Series,\n",
    "                model_name: str,\n",
    "                reason: str = \"Manual retrain\") -> str:\n",
    "        \"\"\"\n",
    "        Retrain the model with new data.\n",
    "        \n",
    "        Returns:\n",
    "            new_version: Version of the newly trained model\n",
    "        \"\"\"\n",
    "        if self.model_factory is None:\n",
    "            raise ValueError(\"Model factory not set. Call set_model_factory() first.\")\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"RETRAINING TRIGGERED\")\n",
    "        print(f\"Reason: {reason}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Start experiment tracking\n",
    "        run_id = self.tracker.start_run(\n",
    "            experiment_name=f\"{model_name}_retrain\",\n",
    "            run_name=f\"retrain_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Create and train new model\n",
    "            model = self.model_factory()\n",
    "            \n",
    "            # Log parameters\n",
    "            if hasattr(model, 'get_params'):\n",
    "                self.tracker.log_params(model.get_params())\n",
    "            \n",
    "            self.tracker.log_param('train_samples', len(X_train))\n",
    "            self.tracker.log_param('retrain_reason', reason)\n",
    "            \n",
    "            # Train\n",
    "            print(\"Training new model...\")\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_val)\n",
    "            \n",
    "            metrics = {\n",
    "                'accuracy': accuracy_score(y_val, y_pred),\n",
    "                'precision': precision_score(y_val, y_pred, average='weighted'),\n",
    "                'recall': recall_score(y_val, y_pred, average='weighted'),\n",
    "                'f1': f1_score(y_val, y_pred, average='weighted')\n",
    "            }\n",
    "            \n",
    "            # Log metrics\n",
    "            self.tracker.log_metrics(metrics)\n",
    "            self.tracker.set_tag('retrain_reason', reason)\n",
    "            \n",
    "            print(f\"New model metrics: {metrics}\")\n",
    "            \n",
    "            # Register new model\n",
    "            new_version = self.registry.register_model(\n",
    "                model=model,\n",
    "                model_name=model_name,\n",
    "                metrics=metrics,\n",
    "                parameters=model.get_params() if hasattr(model, 'get_params') else {},\n",
    "                description=f\"Retrained due to: {reason}\",\n",
    "                tags={'retrain_reason': reason}\n",
    "            )\n",
    "            \n",
    "            # Update reference data for drift detection\n",
    "            self.drift_detector.set_reference(X_train)\n",
    "            \n",
    "            # Update baseline metrics\n",
    "            self.monitor.set_baseline(metrics)\n",
    "            \n",
    "            # Log retrain event\n",
    "            self.retrain_history.append({\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'model_name': model_name,\n",
    "                'new_version': new_version,\n",
    "                'reason': reason,\n",
    "                'metrics': metrics\n",
    "            })\n",
    "            \n",
    "            self.tracker.end_run('completed')\n",
    "            \n",
    "            print(f\"\\nNew model registered: {model_name} {new_version}\")\n",
    "            return new_version\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.tracker.end_run('failed')\n",
    "            raise e\n",
    "    \n",
    "    def auto_retrain_if_needed(self, X_current: pd.DataFrame,\n",
    "                                y_current: pd.Series,\n",
    "                                X_train: pd.DataFrame,\n",
    "                                y_train: pd.Series,\n",
    "                                model_name: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Automatically retrain if triggers are met.\n",
    "        \n",
    "        Returns:\n",
    "            new_version if retrained, None otherwise\n",
    "        \"\"\"\n",
    "        triggers = self.check_retrain_triggers(X_current, y_current, model_name)\n",
    "        \n",
    "        if triggers['should_retrain']:\n",
    "            reason = \"; \".join(triggers['reasons'])\n",
    "            \n",
    "            # Split training data for validation\n",
    "            X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "                X_train, y_train, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            return self.retrain(X_tr, y_tr, X_val, y_val, model_name, reason)\n",
    "        \n",
    "        print(\"No retraining needed.\")\n",
    "        return None\n",
    "    \n",
    "    def get_retrain_history(self) -> pd.DataFrame:\n",
    "        \"\"\"Get retraining history as DataFrame.\"\"\"\n",
    "        return pd.DataFrame(self.retrain_history)\n",
    "\n",
    "print(\"\\nRetrainingPipeline created!\")\n",
    "print(\"\\nTriggers:\")\n",
    "print(\"  - Data drift detected\")\n",
    "print(\"  - Performance degradation\")\n",
    "print(\"  - Manual trigger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. A/B Testing Framework\n",
    "\n",
    "Compare model versions statistically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABTester:\n",
    "    \"\"\"\n",
    "    A/B Testing framework for comparing model versions.\n",
    "    \n",
    "    Features:\n",
    "    - Split traffic between models\n",
    "    - Statistical significance testing\n",
    "    - Performance comparison\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, significance_level: float = 0.05):\n",
    "        self.significance_level = significance_level\n",
    "        self.experiments = {}\n",
    "        self.results_history = []\n",
    "    \n",
    "    def create_experiment(self, experiment_name: str,\n",
    "                          model_a, model_b,\n",
    "                          traffic_split: float = 0.5):\n",
    "        \"\"\"\n",
    "        Create an A/B experiment.\n",
    "        \n",
    "        Args:\n",
    "            experiment_name: Name of the experiment\n",
    "            model_a: Control model (current production)\n",
    "            model_b: Treatment model (challenger)\n",
    "            traffic_split: Fraction of traffic to model B\n",
    "        \"\"\"\n",
    "        self.experiments[experiment_name] = {\n",
    "            'model_a': model_a,\n",
    "            'model_b': model_b,\n",
    "            'traffic_split': traffic_split,\n",
    "            'results_a': [],\n",
    "            'results_b': [],\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'status': 'running'\n",
    "        }\n",
    "        print(f\"Created A/B experiment: {experiment_name}\")\n",
    "        print(f\"  Traffic split: {(1-traffic_split)*100:.0f}% A / {traffic_split*100:.0f}% B\")\n",
    "    \n",
    "    def route_prediction(self, experiment_name: str, X: np.ndarray) -> Tuple[np.ndarray, str]:\n",
    "        \"\"\"\n",
    "        Route prediction to appropriate model based on traffic split.\n",
    "        \n",
    "        Returns:\n",
    "            predictions, model_used ('A' or 'B')\n",
    "        \"\"\"\n",
    "        exp = self.experiments[experiment_name]\n",
    "        \n",
    "        if np.random.random() < exp['traffic_split']:\n",
    "            predictions = exp['model_b'].predict(X)\n",
    "            return predictions, 'B'\n",
    "        else:\n",
    "            predictions = exp['model_a'].predict(X)\n",
    "            return predictions, 'A'\n",
    "    \n",
    "    def record_result(self, experiment_name: str, model_used: str,\n",
    "                      y_true: np.ndarray, y_pred: np.ndarray):\n",
    "        \"\"\"\n",
    "        Record results for an experiment.\n",
    "        \"\"\"\n",
    "        exp = self.experiments[experiment_name]\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        result = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'accuracy': accuracy,\n",
    "            'n_samples': len(y_true)\n",
    "        }\n",
    "        \n",
    "        if model_used == 'A':\n",
    "            exp['results_a'].append(result)\n",
    "        else:\n",
    "            exp['results_b'].append(result)\n",
    "    \n",
    "    def run_comparison(self, experiment_name: str, X: pd.DataFrame, \n",
    "                       y: pd.Series, n_iterations: int = 100) -> Dict:\n",
    "        \"\"\"\n",
    "        Run full A/B comparison.\n",
    "        \"\"\"\n",
    "        exp = self.experiments[experiment_name]\n",
    "        \n",
    "        # Get predictions from both models\n",
    "        pred_a = exp['model_a'].predict(X)\n",
    "        pred_b = exp['model_b'].predict(X)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics_a = {\n",
    "            'accuracy': accuracy_score(y, pred_a),\n",
    "            'precision': precision_score(y, pred_a, average='weighted'),\n",
    "            'recall': recall_score(y, pred_a, average='weighted'),\n",
    "            'f1': f1_score(y, pred_a, average='weighted')\n",
    "        }\n",
    "        \n",
    "        metrics_b = {\n",
    "            'accuracy': accuracy_score(y, pred_b),\n",
    "            'precision': precision_score(y, pred_b, average='weighted'),\n",
    "            'recall': recall_score(y, pred_b, average='weighted'),\n",
    "            'f1': f1_score(y, pred_b, average='weighted')\n",
    "        }\n",
    "        \n",
    "        # Bootstrap for statistical testing\n",
    "        bootstrap_diffs = []\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        for _ in range(n_iterations):\n",
    "            idx = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            acc_a = accuracy_score(y.iloc[idx], pred_a[idx])\n",
    "            acc_b = accuracy_score(y.iloc[idx], pred_b[idx])\n",
    "            bootstrap_diffs.append(acc_b - acc_a)\n",
    "        \n",
    "        # Calculate p-value (proportion of times A is better)\n",
    "        p_value = np.mean(np.array(bootstrap_diffs) <= 0)\n",
    "        \n",
    "        # Determine winner\n",
    "        diff = metrics_b['accuracy'] - metrics_a['accuracy']\n",
    "        \n",
    "        if p_value < self.significance_level and diff > 0:\n",
    "            winner = 'B'\n",
    "            conclusion = 'Model B is significantly better'\n",
    "        elif p_value < self.significance_level and diff < 0:\n",
    "            winner = 'A'\n",
    "            conclusion = 'Model A is significantly better'\n",
    "        else:\n",
    "            winner = 'None'\n",
    "            conclusion = 'No significant difference'\n",
    "        \n",
    "        result = {\n",
    "            'experiment_name': experiment_name,\n",
    "            'metrics_a': metrics_a,\n",
    "            'metrics_b': metrics_b,\n",
    "            'accuracy_diff': diff,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < self.significance_level,\n",
    "            'winner': winner,\n",
    "            'conclusion': conclusion,\n",
    "            'n_samples': n_samples\n",
    "        }\n",
    "        \n",
    "        self.results_history.append(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def print_results(self, result: Dict):\n",
    "        \"\"\"\n",
    "        Print A/B test results.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"A/B TEST RESULTS: {result['experiment_name']}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        print(f\"\\nModel A (Control):\")\n",
    "        for metric, value in result['metrics_a'].items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        \n",
    "        print(f\"\\nModel B (Treatment):\")\n",
    "        for metric, value in result['metrics_b'].items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        \n",
    "        print(f\"\\nStatistical Analysis:\")\n",
    "        print(f\"  Accuracy Difference: {result['accuracy_diff']:+.4f}\")\n",
    "        print(f\"  P-value: {result['p_value']:.4f}\")\n",
    "        print(f\"  Significant: {result['significant']}\")\n",
    "        \n",
    "        print(f\"\\nConclusion: {result['conclusion']}\")\n",
    "        print(f\"Winner: Model {result['winner']}\")\n",
    "\n",
    "# Test A/B tester\n",
    "ab_tester = ABTester()\n",
    "print(\"\\nABTester created!\")\n",
    "print(\"\\nCapabilities:\")\n",
    "print(\"  - create_experiment()\")\n",
    "print(\"  - run_comparison()\")\n",
    "print(\"  - Statistical significance testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Complete MLOps Pipeline\n",
    "\n",
    "Combine all components into a unified MLOps system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLOpsPipeline:\n",
    "    \"\"\"\n",
    "    Complete MLOps Pipeline combining all components.\n",
    "    \n",
    "    Features:\n",
    "    - End-to-end ML lifecycle management\n",
    "    - Automated monitoring and retraining\n",
    "    - Model versioning and registry\n",
    "    - A/B testing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, project_name: str = \"mlops_project\"):\n",
    "        self.project_name = project_name\n",
    "        self.base_dir = Path(f'./mlops_artifacts/{project_name}')\n",
    "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.tracker = ExperimentTracker(str(self.base_dir / 'experiments'))\n",
    "        self.registry = ModelRegistry(str(self.base_dir / 'model_registry'))\n",
    "        self.feature_store = FeatureStore(str(self.base_dir / 'feature_store'))\n",
    "        self.drift_detector = DriftDetector()\n",
    "        self.monitor = PerformanceMonitor(str(self.base_dir / 'monitoring'))\n",
    "        self.ab_tester = ABTester()\n",
    "        \n",
    "        # Retraining pipeline\n",
    "        self.retrain_pipeline = RetrainingPipeline(\n",
    "            self.registry, self.drift_detector, self.monitor, self.tracker\n",
    "        )\n",
    "        \n",
    "        # State\n",
    "        self.current_model_name = None\n",
    "        self.current_model = None\n",
    "        \n",
    "        print(f\"MLOps Pipeline initialized: {project_name}\")\n",
    "    \n",
    "    def train_initial_model(self, model, X_train: pd.DataFrame, y_train: pd.Series,\n",
    "                            X_val: pd.DataFrame, y_val: pd.Series,\n",
    "                            model_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Train and register initial model.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"TRAINING INITIAL MODEL: {model_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Start experiment\n",
    "        self.tracker.start_run(f\"{model_name}_initial\", \"initial_training\")\n",
    "        \n",
    "        # Log parameters\n",
    "        if hasattr(model, 'get_params'):\n",
    "            self.tracker.log_params(model.get_params())\n",
    "        self.tracker.log_param('train_samples', len(X_train))\n",
    "        \n",
    "        # Train\n",
    "        print(\"Training model...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_val)\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_val, y_pred),\n",
    "            'precision': precision_score(y_val, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y_val, y_pred, average='weighted'),\n",
    "            'f1': f1_score(y_val, y_pred, average='weighted')\n",
    "        }\n",
    "        \n",
    "        self.tracker.log_metrics(metrics)\n",
    "        print(f\"Metrics: {metrics}\")\n",
    "        \n",
    "        # Register model\n",
    "        version = self.registry.register_model(\n",
    "            model=model,\n",
    "            model_name=model_name,\n",
    "            metrics=metrics,\n",
    "            parameters=model.get_params() if hasattr(model, 'get_params') else {},\n",
    "            description=\"Initial model\"\n",
    "        )\n",
    "        \n",
    "        # Set up monitoring\n",
    "        self.monitor.set_baseline(metrics)\n",
    "        self.monitor.set_threshold('accuracy', min_value=metrics['accuracy'] * 0.9, max_degradation=10)\n",
    "        self.monitor.set_threshold('f1', min_value=metrics['f1'] * 0.9, max_degradation=10)\n",
    "        \n",
    "        # Set reference data for drift detection\n",
    "        self.drift_detector.set_reference(X_train)\n",
    "        \n",
    "        # Store features\n",
    "        self.feature_store.register_features(f\"{model_name}_features\", X_train)\n",
    "        \n",
    "        # Update state\n",
    "        self.current_model_name = model_name\n",
    "        self.current_model = model\n",
    "        \n",
    "        # Promote to production\n",
    "        self.registry.transition_stage(model_name, version, ModelStage.PRODUCTION)\n",
    "        \n",
    "        self.tracker.end_run('completed')\n",
    "        \n",
    "        print(f\"\\nModel {model_name} {version} deployed to production!\")\n",
    "        return version\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions using production model.\n",
    "        \"\"\"\n",
    "        if self.current_model is None:\n",
    "            self.current_model = self.registry.get_production_model(self.current_model_name)\n",
    "        \n",
    "        return self.current_model.predict(X)\n",
    "    \n",
    "    def monitor_and_evaluate(self, X: pd.DataFrame, y: pd.Series,\n",
    "                              model_version: str = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Monitor model performance on new data.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"MONITORING MODEL PERFORMANCE\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Check drift\n",
    "        drift_results = self.drift_detector.detect_drift(X)\n",
    "        print(f\"\\nDrift Detection:\")\n",
    "        print(f\"  Overall drift: {drift_results['overall_drift']}\")\n",
    "        print(f\"  Drifted features: {drift_results['n_drifted_features']}\")\n",
    "        \n",
    "        # Evaluate performance\n",
    "        y_pred = self.predict(X)\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y, y_pred),\n",
    "            'precision': precision_score(y, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y, y_pred, average='weighted'),\n",
    "            'f1': f1_score(y, y_pred, average='weighted')\n",
    "        }\n",
    "        \n",
    "        # Log metrics and get alerts\n",
    "        alerts = self.monitor.log_metrics(metrics, model_version=model_version)\n",
    "        \n",
    "        print(f\"\\nPerformance Metrics:\")\n",
    "        for name, value in metrics.items():\n",
    "            baseline = self.monitor.baseline_metrics.get(name, 0)\n",
    "            diff = ((value - baseline) / baseline * 100) if baseline else 0\n",
    "            print(f\"  {name}: {value:.4f} ({diff:+.1f}% from baseline)\")\n",
    "        \n",
    "        if alerts:\n",
    "            print(f\"\\nAlerts ({len(alerts)}):\")\n",
    "            for alert in alerts:\n",
    "                print(f\"  [{alert.severity}] {alert.message}\")\n",
    "        \n",
    "        # Check health\n",
    "        health = self.monitor.check_health()\n",
    "        print(f\"\\nModel Health: {health['status'].upper()}\")\n",
    "        \n",
    "        return {\n",
    "            'drift_results': drift_results,\n",
    "            'metrics': metrics,\n",
    "            'alerts': alerts,\n",
    "            'health': health\n",
    "        }\n",
    "    \n",
    "    def auto_retrain_check(self, X_current: pd.DataFrame, y_current: pd.Series,\n",
    "                           X_train: pd.DataFrame, y_train: pd.Series) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Check if retraining is needed and trigger if so.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"CHECKING RETRAINING TRIGGERS\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        triggers = self.retrain_pipeline.check_retrain_triggers(\n",
    "            X_current, y_current, self.current_model_name\n",
    "        )\n",
    "        \n",
    "        print(f\"Should retrain: {triggers['should_retrain']}\")\n",
    "        if triggers['reasons']:\n",
    "            print(\"Reasons:\")\n",
    "            for reason in triggers['reasons']:\n",
    "                print(f\"  - {reason}\")\n",
    "        \n",
    "        if triggers['should_retrain']:\n",
    "            # Set model factory if not set\n",
    "            if self.retrain_pipeline.model_factory is None:\n",
    "                model_class = type(self.current_model)\n",
    "                model_params = self.current_model.get_params() if hasattr(self.current_model, 'get_params') else {}\n",
    "                self.retrain_pipeline.set_model_factory(lambda: model_class(**model_params))\n",
    "            \n",
    "            return self.retrain_pipeline.auto_retrain_if_needed(\n",
    "                X_current, y_current, X_train, y_train, self.current_model_name\n",
    "            )\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def run_ab_test(self, challenger_model, X_test: pd.DataFrame, \n",
    "                    y_test: pd.Series, experiment_name: str = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Run A/B test between current production model and challenger.\n",
    "        \"\"\"\n",
    "        experiment_name = experiment_name or f\"ab_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        self.ab_tester.create_experiment(\n",
    "            experiment_name,\n",
    "            model_a=self.current_model,\n",
    "            model_b=challenger_model\n",
    "        )\n",
    "        \n",
    "        result = self.ab_tester.run_comparison(experiment_name, X_test, y_test)\n",
    "        self.ab_tester.print_results(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_dashboard_data(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get data for monitoring dashboard.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'model_name': self.current_model_name,\n",
    "            'production_version': self.registry.registry['models'].get(\n",
    "                self.current_model_name, {}).get('production_version'),\n",
    "            'health': self.monitor.check_health(),\n",
    "            'recent_alerts': self.monitor.get_alerts_df().tail(10).to_dict('records'),\n",
    "            'metrics_history': self.monitor.get_metrics_df().to_dict('records'),\n",
    "            'retrain_history': self.retrain_pipeline.get_retrain_history().to_dict('records')\n",
    "        }\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MLOPS PIPELINE READY!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Demo: Full MLOps Simulation\n",
    "\n",
    "Let's simulate a complete MLOps workflow with drift and retraining!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset\n",
    "print(\"Creating synthetic dataset for MLOps demo...\")\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=5000, n_features=20, n_informative=15,\n",
    "    n_redundant=5, random_state=42\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(20)])\n",
    "y = pd.Series(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLOps Pipeline\n",
    "mlops = MLOpsPipeline(project_name=\"fraud_detection_demo\")\n",
    "\n",
    "# Train initial model\n",
    "initial_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "version = mlops.train_initial_model(\n",
    "    model=initial_model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    model_name=\"fraud_classifier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor on test data (no drift expected)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WEEK 1: Monitoring on similar data (no drift)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_week1 = mlops.monitor_and_evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data drift\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WEEK 4: Simulating data drift...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create drifted data\n",
    "X_drifted = X_test.copy()\n",
    "# Shift some features\n",
    "X_drifted['feature_0'] = X_drifted['feature_0'] + 2  # Mean shift\n",
    "X_drifted['feature_1'] = X_drifted['feature_1'] * 1.5  # Variance change\n",
    "X_drifted['feature_2'] = X_drifted['feature_2'] + np.random.normal(0, 1, len(X_drifted))  # Add noise\n",
    "\n",
    "# Monitor on drifted data\n",
    "results_week4 = mlops.monitor_and_evaluate(X_drifted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if retraining is needed\n",
    "new_version = mlops.auto_retrain_check(\n",
    "    X_current=X_drifted,\n",
    "    y_current=y_test,\n",
    "    X_train=pd.concat([X_train, X_drifted]),  # Include new data\n",
    "    y_train=pd.concat([y_train, y_test])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run A/B test with a challenger model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"A/B TEST: RandomForest vs GradientBoosting\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train challenger\n",
    "challenger = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "challenger.fit(X_train, y_train)\n",
    "\n",
    "# Run A/B test\n",
    "ab_result = mlops.run_ab_test(challenger, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize monitoring results\n",
    "fig = mlops.monitor.plot_metrics(['accuracy', 'f1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View model registry\n",
    "print(\"\\nModel Registry:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Registered models: {mlops.registry.list_models()}\")\n",
    "print(f\"\\nVersions for fraud_classifier:\")\n",
    "print(mlops.registry.compare_versions('fraud_classifier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View experiment tracking\n",
    "print(\"\\nExperiment Tracking:\")\n",
    "print(\"=\"*50)\n",
    "print(mlops.tracker.get_runs_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View drift report\n",
    "if results_week4['drift_results']['features']:\n",
    "    print(\"\\nDrift Report:\")\n",
    "    print(\"=\"*50)\n",
    "    drift_df = mlops.drift_detector.get_drift_report(results_week4['drift_results'])\n",
    "    print(drift_df[['feature', 'is_drifted', 'type']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "### What We Built\n",
    "\n",
    "A complete **MLOps Pipeline** that handles the entire ML lifecycle:\n",
    "\n",
    "| Component | Functionality |\n",
    "|-----------|---------------|\n",
    "| `ExperimentTracker` | Log params, metrics, artifacts (like MLflow) |\n",
    "| `ModelRegistry` | Version control, stage transitions |\n",
    "| `FeatureStore` | Feature storage and statistics |\n",
    "| `DriftDetector` | KS test, Chi-square, PSI for drift |\n",
    "| `PerformanceMonitor` | Track metrics, generate alerts |\n",
    "| `RetrainingPipeline` | Auto-retrain on triggers |\n",
    "| `ABTester` | Statistical model comparison |\n",
    "| `MLOpsPipeline` | Unified orchestration |\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "1. **Model Versioning**: Track and manage model versions\n",
    "2. **Experiment Tracking**: Log all experiments reproducibly\n",
    "3. **Drift Detection**: Detect data distribution changes\n",
    "4. **Performance Monitoring**: Track metrics and alert on degradation\n",
    "5. **Automated Retraining**: Trigger retraining based on rules\n",
    "6. **A/B Testing**: Statistically compare model versions\n",
    "\n",
    "### Production Tools Mapping\n",
    "\n",
    "| Our Component | Production Equivalent |\n",
    "|---------------|----------------------|\n",
    "| ExperimentTracker | MLflow, Weights & Biases |\n",
    "| ModelRegistry | MLflow Model Registry, SageMaker |\n",
    "| FeatureStore | Feast, Tecton |\n",
    "| DriftDetector | Evidently, WhyLabs |\n",
    "| PerformanceMonitor | Prometheus + Grafana |\n",
    "| RetrainingPipeline | Airflow, Kubeflow Pipelines |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"MLOPS PIPELINE - COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "Components Built (100% Kaggle Compatible):\n",
    "──────────────────────────────────────────\n",
    "1. ExperimentTracker  - Log experiments (like MLflow)\n",
    "2. ModelRegistry      - Version control for models\n",
    "3. FeatureStore       - Feature management\n",
    "4. DriftDetector      - Statistical drift detection\n",
    "5. PerformanceMonitor - Metrics tracking & alerting\n",
    "6. RetrainingPipeline - Automated retraining\n",
    "7. ABTester           - A/B testing framework\n",
    "8. MLOpsPipeline      - Complete orchestration\n",
    "\n",
    "Drift Detection Methods:\n",
    "────────────────────────\n",
    "• KS Test (Kolmogorov-Smirnov) - Numeric features\n",
    "• Chi-Square Test - Categorical features\n",
    "• PSI (Population Stability Index)\n",
    "• Wasserstein Distance\n",
    "\n",
    "Retraining Triggers:\n",
    "────────────────────\n",
    "• Data drift detected (>20% features drifted)\n",
    "• Performance degradation (>10% from baseline)\n",
    "• Manual trigger\n",
    "\n",
    "This MLOps framework can be used as a foundation for\n",
    "production ML systems or as a learning tool!\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
