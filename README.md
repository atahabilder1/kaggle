# Machine Learning Mastery: A Complete Practical Guide

## From Fundamentals to Production Systems in 22 Projects

---

<p align="center">
  <strong>Author:</strong> Anik Tahabilder<br>
  <strong>PhD Student, Department of Computer Science</strong><br>
  <strong>Wayne State University</strong><br>
  <em>Detroit, Michigan</em>
</p>

---

## About This Book

Welcome to **Machine Learning Mastery**, a comprehensive hands-on guide to machine learning that I developed during my PhD studies at Wayne State University. This repository contains 22 complete projects that take you from basic data exploration to building enterprise-grade distributed ML systems.

Each project is a self-contained chapter that teaches specific concepts through practical implementation. Unlike traditional textbooks that focus on theory, this guide emphasizes learning by doing. Every concept is immediately applied to real datasets and real problems.

**What makes this different:**
- **Practical First:** Every concept is taught through implementation
- **Progressive Difficulty:** Projects build on each other systematically
- **Production Ready:** Later projects cover deployment, monitoring, and scaling
- **Complete Solutions:** Each notebook runs end-to-end with explanations
- **Real Challenges:** Includes debugging, optimization, and trade-off discussions

---

## Table of Contents

### Part I: Foundations (Chapters 1-5)
| Chapter | Title | Difficulty |
|---------|-------|------------|
| [Chapter 1](#chapter-1-data-exploration-and-visualization) | Data Exploration and Visualization | Beginner |
| [Chapter 2](#chapter-2-iris-species-classifier) | Iris Species Classifier | Beginner |
| [Chapter 3](#chapter-3-linear-regression-from-scratch) | Linear Regression from Scratch | Beginner |
| [Chapter 4](#chapter-4-titanic-survival-prediction) | Titanic Survival Prediction | Beginner |
| [Chapter 5](#chapter-5-housing-price-prediction) | Housing Price Prediction | Intermediate |

### Part II: Deep Learning & Computer Vision (Chapters 6, 10-11, 16)
| Chapter | Title | Difficulty |
|---------|-------|------------|
| [Chapter 6](#chapter-6-cats-vs-dogs-image-classifier) | Cats vs Dogs Image Classifier | Intermediate |
| [Chapter 10](#chapter-10-neural-network-from-scratch) | Neural Network from Scratch | Intermediate |
| [Chapter 11](#chapter-11-real-time-face-recognition) | Real-Time Face Recognition | Advanced |
| [Chapter 16](#chapter-16-gan-image-generator) | GAN Image Generator | Advanced |

### Part III: Natural Language Processing (Chapters 7, 14, 17)
| Chapter | Title | Difficulty |
|---------|-------|------------|
| [Chapter 7](#chapter-7-sentiment-analysis-system) | Sentiment Analysis System | Intermediate |
| [Chapter 14](#chapter-14-language-model-from-scratch) | Language Model from Scratch | Advanced |
| [Chapter 17](#chapter-17-multilingual-nlp-pipeline) | Multilingual NLP Pipeline | Advanced |

### Part IV: Business & Time Series (Chapters 8-9, 15, 19)
| Chapter | Title | Difficulty |
|---------|-------|------------|
| [Chapter 8](#chapter-8-customer-churn-model) | Customer Churn Model | Intermediate |
| [Chapter 9](#chapter-9-stock-price-prediction) | Stock Price Prediction | Intermediate |
| [Chapter 15](#chapter-15-ab-testing-framework) | A/B Testing Framework | Advanced |
| [Chapter 19](#chapter-19-real-time-fraud-detection) | Real-Time Fraud Detection | Advanced |

### Part V: Recommendations & Reinforcement Learning (Chapters 12, 18)
| Chapter | Title | Difficulty |
|---------|-------|------------|
| [Chapter 12](#chapter-12-recommendation-system) | Recommendation System | Advanced |
| [Chapter 18](#chapter-18-reinforcement-learning-agent) | Reinforcement Learning Agent | Advanced |

### Part VI: Production & Systems (Chapters 13, 20-22)
| Chapter | Title | Difficulty |
|---------|-------|------------|
| [Chapter 13](#chapter-13-automated-ml-pipeline) | Automated ML Pipeline | Advanced |
| [Chapter 20](#chapter-20-automl-system) | AutoML System | Expert |
| [Chapter 21](#chapter-21-mlops-pipeline) | MLOps Pipeline | Expert |
| [Chapter 22](#chapter-22-distributed-ml-training) | Distributed ML Training | Expert |

---

## 10-Week Learning Curriculum

I've organized these 22 projects into a structured 10-week curriculum. Following this schedule with 10-15 hours per week will take you from beginner to advanced practitioner.

<details>
<summary><strong>Week 1: Data Science Fundamentals</strong></summary>

**Focus:** Master data exploration before modeling

| Chapter | Topics |
|---------|--------|
| Chapter 1 | Pandas, Matplotlib, Seaborn, EDA workflow |
| Chapter 2 | Scikit-learn basics, classification metrics |
| Review | Practice EDA on a new dataset |

**Milestone:** You can load any dataset and create a comprehensive analysis report.

</details>

<details>
<summary><strong>Week 2: Core Algorithms</strong></summary>

**Focus:** Understand ML math by implementing from scratch

| Chapter | Topics |
|---------|--------|
| Chapter 3 | Gradient descent, cost functions, vectorization |
| Chapter 4 | Feature engineering, model selection |
| Review | Compare scratch vs scikit-learn implementations |

**Milestone:** You understand what happens inside ML algorithms.

</details>

<details>
<summary><strong>Week 3: Advanced Regression & Business ML</strong></summary>

**Focus:** Handle real-world data challenges

| Chapter | Topics |
|---------|--------|
| Chapter 5 | Feature scaling, regularization, XGBoost |
| Chapter 8 | Class imbalance, business metrics, SMOTE |
| Review | Build a complete regression pipeline |

**Milestone:** You can handle messy data and imbalanced classes.

</details>

<details>
<summary><strong>Week 4: Deep Learning Fundamentals</strong></summary>

**Focus:** Enter the neural network world

| Chapter | Topics |
|---------|--------|
| Chapter 10 | Backpropagation, activations, from scratch |
| Chapter 6 | CNNs, transfer learning, data augmentation |

**Milestone:** You understand neural networks at the mathematical level.

</details>

<details>
<summary><strong>Week 5: Natural Language Processing</strong></summary>

**Focus:** Process and understand text

| Chapter | Topics |
|---------|--------|
| Chapter 7 | Text preprocessing, TF-IDF, sentiment |
| Chapter 14 | Attention, transformers, language models |

**Milestone:** You can build text classification and generation systems.

</details>

<details>
<summary><strong>Week 6: Time Series & Sequences</strong></summary>

**Focus:** Handle temporal data

| Chapter | Topics |
|---------|--------|
| Chapter 9 | LSTM, technical indicators, forecasting |
| Chapter 17 | Multilingual models, NER, cross-lingual |

**Milestone:** You can forecast sequences and handle multiple languages.

</details>

<details>
<summary><strong>Week 7: Advanced Computer Vision</strong></summary>

**Focus:** Build vision applications

| Chapter | Topics |
|---------|--------|
| Chapter 11 | Face detection, embeddings, real-time |
| Chapter 16 | GANs, generative models, image synthesis |

**Milestone:** You can build real-time vision and generative systems.

</details>

<details>
<summary><strong>Week 8: Recommendations & Experimentation</strong></summary>

**Focus:** Systems that learn from users

| Chapter | Topics |
|---------|--------|
| Chapter 12 | Collaborative filtering, matrix factorization |
| Chapter 15 | A/B testing, statistical significance, bandits |

**Milestone:** You can build recommenders and run proper experiments.

</details>

<details>
<summary><strong>Week 9: Advanced Applications</strong></summary>

**Focus:** Specialized ML applications

| Chapter | Topics |
|---------|--------|
| Chapter 19 | Anomaly detection, streaming, fraud |
| Chapter 18 | Q-learning, policy gradients, RL |
| Chapter 13 | Pipelines, automation, reproducibility |

**Milestone:** You can handle specialized ML problems.

</details>

<details>
<summary><strong>Week 10: Production Systems</strong></summary>

**Focus:** Deploy and scale ML

| Chapter | Topics |
|---------|--------|
| Chapter 20 | AutoML, hyperparameter optimization, NAS |
| Chapter 21 | MLOps, CI/CD, monitoring, deployment |
| Chapter 22 | Distributed training, federated learning, privacy |

**Milestone:** You can build production-ready ML systems at scale.

</details>

---

## Chapter Details

<details>
<summary><strong>Chapter 1: Data Exploration and Visualization</strong></summary>

<a id="chapter-1-data-exploration-and-visualization"></a>

**File:** `01_Tahabilder_data_exploration_visuals.ipynb`

**Difficulty:** Beginner (1/10) | **Time:** 4 hours

### Overview

Every successful ML project begins with understanding your data. This chapter teaches you the systematic approach to Exploratory Data Analysis (EDA) that I use in all my research projects. You'll learn to identify patterns, detect anomalies, and create visualizations that tell a story.

### What You'll Learn

1. **Loading Data from Various Sources**
   - CSV, Excel, JSON files
   - Database connections
   - API data retrieval

2. **Initial Data Inspection**
   - Shape, dtypes, memory usage
   - First/last rows examination
   - Statistical summaries

3. **Missing Value Analysis**
   - Detection patterns
   - Visualization of missingness
   - Imputation strategies

4. **Outlier Detection**
   - IQR method
   - Z-score approach
   - Visual detection with box plots

5. **Univariate Analysis**
   - Distribution plots (histograms, KDE)
   - Categorical frequency counts
   - Summary statistics interpretation

6. **Bivariate Analysis**
   - Scatter plots for relationships
   - Correlation matrices
   - Cross-tabulations

7. **Multivariate Analysis**
   - Pair plots
   - Heatmaps
   - Dimensionality reduction visualization

### Code Concepts

```python
# Example: Comprehensive EDA function
def comprehensive_eda(df):
    print("Shape:", df.shape)
    print("\nData Types:\n", df.dtypes)
    print("\nMissing Values:\n", df.isnull().sum())
    print("\nStatistics:\n", df.describe())

    # Visualize distributions
    numerical_cols = df.select_dtypes(include=[np.number]).columns
    df[numerical_cols].hist(figsize=(15, 10))
    plt.tight_layout()
    plt.show()
```

### Key Takeaways

- Never skip EDA - it prevents costly mistakes later
- Visualizations reveal patterns that statistics miss
- Document your findings for reproducibility
- EDA is iterative - keep exploring as you learn more

</details>

<details>
<summary><strong>Chapter 2: Iris Species Classifier</strong></summary>

<a id="chapter-2-iris-species-classifier"></a>

**File:** `02_Tahabilder_iris_species_classifier.ipynb`

**Difficulty:** Beginner (2/10) | **Time:** 4 hours

### Overview

The Iris dataset is the "Hello World" of machine learning. While simple, it teaches fundamental classification concepts that apply to complex problems. This chapter introduces the scikit-learn workflow that you'll use throughout your ML career.

### What You'll Learn

1. **The ML Workflow**
   - Data loading → EDA → Preprocessing → Training → Evaluation

2. **Classification Algorithms**
   - K-Nearest Neighbors (KNN)
   - Support Vector Machines (SVM)
   - Decision Trees
   - Random Forests

3. **Model Evaluation**
   - Train/test splitting
   - Accuracy, precision, recall, F1
   - Confusion matrix interpretation
   - Cross-validation

4. **Hyperparameter Tuning**
   - Grid search basics
   - Parameter impact understanding

### The Iris Dataset

| Feature | Description |
|---------|-------------|
| sepal_length | Length of sepal in cm |
| sepal_width | Width of sepal in cm |
| petal_length | Length of petal in cm |
| petal_width | Width of petal in cm |
| species | Setosa, Versicolor, or Virginica |

### Code Concepts

```python
# Standard ML workflow
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train model
model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train_scaled, y_train)

# Evaluate
predictions = model.predict(X_test_scaled)
print(classification_report(y_test, predictions))
```

### Key Takeaways

- Always split data before any preprocessing
- Scale features for distance-based algorithms
- Use cross-validation for robust evaluation
- Compare multiple algorithms before deciding

</details>

<details>
<summary><strong>Chapter 3: Linear Regression from Scratch</strong></summary>

<a id="chapter-3-linear-regression-from-scratch"></a>

**File:** `03_Tahabilder_linear_regression_scratch.ipynb`

**Difficulty:** Beginner (3/10) | **Time:** 5 hours

### Overview

Understanding the math behind ML algorithms makes you a better practitioner. This chapter implements linear regression using only NumPy, teaching you gradient descent and optimization fundamentals that underlie all of deep learning.

### What You'll Learn

1. **Mathematical Foundations**
   - Linear algebra review
   - Hypothesis function: h(x) = wx + b
   - Cost function (MSE)

2. **Gradient Descent**
   - Intuition: walking downhill
   - Derivative computation
   - Update rule implementation

3. **Optimization Variants**
   - Batch gradient descent
   - Stochastic gradient descent
   - Mini-batch gradient descent

4. **Regularization**
   - L1 (Lasso) - sparsity
   - L2 (Ridge) - smaller weights
   - ElasticNet - combination

### Mathematical Formulas

**Hypothesis:**
```
h(x) = w₁x₁ + w₂x₂ + ... + wₙxₙ + b
```

**Mean Squared Error:**
```
J(w) = (1/2m) × Σᵢ(h(xᵢ) - yᵢ)²
```

**Gradient Descent Update:**
```
w = w - α × ∂J/∂w
b = b - α × ∂J/∂b
```

Where α is the learning rate.

### Code Concepts

```python
class LinearRegressionScratch:
    def __init__(self, learning_rate=0.01, iterations=1000):
        self.lr = learning_rate
        self.iterations = iterations

    def fit(self, X, y):
        m, n = X.shape
        self.weights = np.zeros(n)
        self.bias = 0

        for _ in range(self.iterations):
            # Forward pass
            y_pred = np.dot(X, self.weights) + self.bias

            # Compute gradients
            dw = (1/m) * np.dot(X.T, (y_pred - y))
            db = (1/m) * np.sum(y_pred - y)

            # Update parameters
            self.weights -= self.lr * dw
            self.bias -= self.lr * db

    def predict(self, X):
        return np.dot(X, self.weights) + self.bias
```

### Key Takeaways

- Learning rate is crucial: too high = diverge, too low = slow
- Feature scaling dramatically improves convergence
- Regularization prevents overfitting
- Vectorization makes code 100x faster than loops

</details>

<details>
<summary><strong>Chapter 4: Titanic Survival Prediction</strong></summary>

<a id="chapter-4-titanic-survival-prediction"></a>

**File:** `04_Tahabilder_titanic_survival_prediction.ipynb`

**Difficulty:** Beginner (3/10) | **Time:** 5 hours

### Overview

The Titanic dataset is a rite of passage for data scientists. This chapter focuses on feature engineering - creating informative features from raw data that dramatically improve model performance.

### What You'll Learn

1. **Feature Engineering Techniques**
   - Title extraction from names
   - Family size computation
   - Fare binning strategies
   - Cabin deck extraction

2. **Handling Missing Data**
   - Age imputation strategies
   - Cabin information utilization
   - Embarked filling

3. **Categorical Encoding**
   - One-hot encoding
   - Label encoding
   - Target encoding concepts

4. **Model Selection**
   - Logistic regression
   - Ensemble methods
   - Model comparison

### Feature Engineering Examples

```python
# Extract title from name
df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\.')

# Create family size
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1

# Extract deck from cabin
df['Deck'] = df['Cabin'].str[0]

# Create age groups
df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 12, 18, 60, 100],
                        labels=['Child', 'Teen', 'Adult', 'Senior'])

# Is traveling alone
df['IsAlone'] = (df['FamilySize'] == 1).astype(int)
```

### Key Takeaways

- Good features often matter more than fancy algorithms
- Domain knowledge guides feature creation
- Iterate: create features → evaluate → improve
- Document your feature engineering decisions

</details>

<details>
<summary><strong>Chapter 5: Housing Price Prediction</strong></summary>

<a id="chapter-5-housing-price-prediction"></a>

**File:** `05_Tahabilder_housing_price_prediction.ipynb`

**Difficulty:** Intermediate (4/10) | **Time:** 6 hours

### Overview

Predicting housing prices involves handling many features with different scales and distributions. This chapter teaches advanced regression techniques including gradient boosting and proper feature preprocessing.

### What You'll Learn

1. **Data Preprocessing**
   - Log transformations for skewed data
   - Feature scaling methods
   - Handling categorical variables

2. **Feature Engineering**
   - Geographic features
   - Derived ratios
   - Interaction features

3. **Advanced Regression**
   - Ridge and Lasso regression
   - ElasticNet
   - Gradient Boosting (XGBoost, LightGBM)

4. **Model Interpretation**
   - Feature importance
   - SHAP values
   - Partial dependence plots

### Code Concepts

```python
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from xgboost import XGBRegressor

# Create preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_columns),
        ('cat', OneHotEncoder(drop='first'), categorical_columns)
    ])

# Create full pipeline
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', XGBRegressor(n_estimators=100, learning_rate=0.1))
])

# Fit and evaluate
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

</details>

<details>
<summary><strong>Chapter 6: Cats vs Dogs Image Classifier</strong></summary>

<a id="chapter-6-cats-vs-dogs-image-classifier"></a>

**File:** `06_Tahabilder_cats_dogs_image_classifier.ipynb`

**Difficulty:** Intermediate (5/10) | **Time:** 7 hours

### Overview

Enter the world of computer vision with Convolutional Neural Networks. This chapter teaches you CNN architecture design and the powerful technique of transfer learning.

### What You'll Learn

1. **Image Preprocessing**
   - Loading and resizing images
   - Normalization
   - Data augmentation

2. **CNN Architecture**
   - Convolutional layers
   - Pooling layers
   - Fully connected layers

3. **Transfer Learning**
   - Using pre-trained models (VGG16, ResNet)
   - Fine-tuning strategies
   - Feature extraction

4. **Training Deep Networks**
   - Callbacks (EarlyStopping, ModelCheckpoint)
   - Learning rate scheduling
   - Batch normalization

### CNN Architecture

```
Input (224×224×3)
    ↓
Conv2D (32 filters, 3×3) → ReLU → MaxPool (2×2)
    ↓
Conv2D (64 filters, 3×3) → ReLU → MaxPool (2×2)
    ↓
Conv2D (128 filters, 3×3) → ReLU → MaxPool (2×2)
    ↓
Flatten
    ↓
Dense (512) → ReLU → Dropout (0.5)
    ↓
Dense (1) → Sigmoid
    ↓
Output (Cat/Dog probability)
```

### Data Augmentation

```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)
```

</details>

<details>
<summary><strong>Chapter 7: Sentiment Analysis System</strong></summary>

<a id="chapter-7-sentiment-analysis-system"></a>

**File:** `07_Tahabilder_sentiment_analysis_system.ipynb`

**Difficulty:** Intermediate (5/10) | **Time:** 6 hours

### Overview

Teach machines to understand human emotions in text. This chapter builds a complete sentiment analysis pipeline from raw text to predictions.

### What You'll Learn

1. **Text Preprocessing**
   - Tokenization
   - Stop word removal
   - Stemming vs lemmatization
   - Regular expressions

2. **Feature Extraction**
   - Bag of Words
   - TF-IDF
   - Word embeddings

3. **Classification Models**
   - Naive Bayes
   - SVM for text
   - LSTM for sequences

4. **Evaluation**
   - Text-specific metrics
   - Error analysis
   - Confusion analysis

### Text Preprocessing Pipeline

```python
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

def preprocess_text(text):
    # Lowercase
    text = text.lower()

    # Remove special characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Tokenize
    tokens = word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [t for t in tokens if t not in stop_words]

    # Lemmatize
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(t) for t in tokens]

    return ' '.join(tokens)
```

</details>

<details>
<summary><strong>Chapter 8: Customer Churn Model</strong></summary>

<a id="chapter-8-customer-churn-model"></a>

**File:** `08_Tahabilder_customer_churn_model.ipynb`

**Difficulty:** Intermediate (5/10) | **Time:** 6 hours

### Overview

Predict which customers will leave - a critical business problem worth millions. Learn to handle imbalanced datasets and translate ML metrics into business value.

### What You'll Learn

1. **Business Understanding**
   - What drives churn?
   - Cost of churn vs retention
   - Actionable predictions

2. **Imbalanced Data Handling**
   - SMOTE oversampling
   - Class weights
   - Threshold optimization

3. **Feature Engineering**
   - Behavioral features
   - Tenure patterns
   - Service interactions

4. **Business Metrics**
   - Customer Lifetime Value
   - Cost-benefit analysis
   - ROI of interventions

### Handling Imbalance

```python
from imblearn.over_sampling import SMOTE
from sklearn.utils.class_weight import compute_class_weight

# Option 1: SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Option 2: Class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
model = RandomForestClassifier(class_weight='balanced')
```

</details>

<details>
<summary><strong>Chapter 9: Stock Price Prediction</strong></summary>

<a id="chapter-9-stock-price-prediction"></a>

**File:** `09_Tahabilder_stock_price_prediction.ipynb`

**Difficulty:** Intermediate (6/10) | **Time:** 7 hours

### Overview

Tackle time series forecasting with financial data. Learn LSTM networks and technical analysis indicators.

### What You'll Learn

1. **Time Series Fundamentals**
   - Stationarity
   - Trend and seasonality
   - Autocorrelation

2. **Technical Indicators**
   - Moving averages (SMA, EMA)
   - RSI
   - MACD
   - Bollinger Bands

3. **LSTM Networks**
   - Sequence preparation
   - Architecture design
   - Multi-step forecasting

4. **Proper Validation**
   - Walk-forward validation
   - Time-based splitting
   - Avoiding look-ahead bias

### LSTM Sequence Preparation

```python
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length])
    return np.array(X), np.array(y)

# Create sequences
seq_length = 60  # 60 days of history
X, y = create_sequences(scaled_data, seq_length)

# Reshape for LSTM: (samples, timesteps, features)
X = X.reshape((X.shape[0], X.shape[1], 1))
```

</details>

<details>
<summary><strong>Chapter 10: Neural Network from Scratch</strong></summary>

<a id="chapter-10-neural-network-from-scratch"></a>

**File:** `10_Tahabilder_neural_network_scratch.ipynb`

**Difficulty:** Intermediate (6/10) | **Time:** 8 hours

### Overview

Build a complete neural network using only NumPy. This chapter demystifies deep learning by implementing backpropagation yourself.

### What You'll Learn

1. **Forward Propagation**
   - Layer computations
   - Activation functions
   - Output generation

2. **Backward Propagation**
   - Chain rule application
   - Gradient computation
   - Weight updates

3. **Activation Functions**
   - Sigmoid
   - ReLU
   - Softmax

4. **Training Dynamics**
   - Loss functions
   - Gradient descent variants
   - Initialization strategies

### Backpropagation Implementation

```python
class NeuralNetwork:
    def backward(self, X, y, output):
        m = X.shape[0]

        # Output layer gradients
        dZ2 = output - y
        dW2 = (1/m) * np.dot(self.A1.T, dZ2)
        db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)

        # Hidden layer gradients
        dA1 = np.dot(dZ2, self.W2.T)
        dZ1 = dA1 * self.relu_derivative(self.Z1)
        dW1 = (1/m) * np.dot(X.T, dZ1)
        db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)

        # Update weights
        self.W2 -= self.lr * dW2
        self.b2 -= self.lr * db2
        self.W1 -= self.lr * dW1
        self.b1 -= self.lr * db1
```

</details>

<details>
<summary><strong>Chapter 11: Real-Time Face Recognition</strong></summary>

<a id="chapter-11-real-time-face-recognition"></a>

**File:** `11_Tahabilder_real_time_face_recognition.ipynb`

**Difficulty:** Advanced (7/10) | **Time:** 8 hours

### Overview

Build a complete face recognition system that identifies people in real-time video. Learn about face embeddings and similarity matching.

### What You'll Learn

1. **Face Detection**
   - Haar cascades
   - DNN-based detection
   - MTCNN

2. **Face Embeddings**
   - What are embeddings?
   - FaceNet/ArcFace concepts
   - Embedding extraction

3. **Recognition System**
   - Database building
   - Similarity matching
   - Threshold tuning

4. **Real-Time Processing**
   - Video capture
   - Frame-by-frame processing
   - Performance optimization

</details>

<details>
<summary><strong>Chapter 12: Recommendation System</strong></summary>

<a id="chapter-12-recommendation-system"></a>

**File:** `12_Tahabilder_recommendation_system.ipynb`

**Difficulty:** Advanced (7/10) | **Time:** 7 hours

### Overview

Build the algorithms behind Netflix and Amazon recommendations. Implement collaborative filtering and content-based approaches.

### What You'll Learn

1. **Recommendation Types**
   - Content-based filtering
   - Collaborative filtering
   - Hybrid systems

2. **Collaborative Filtering**
   - User-user similarity
   - Item-item similarity
   - Matrix factorization (SVD)

3. **Cold Start Problem**
   - New user handling
   - New item handling
   - Hybrid solutions

4. **Evaluation**
   - RMSE, MAE
   - Precision@K, Recall@K
   - Diversity metrics

### Collaborative Filtering

```python
# User-based collaborative filtering
def predict_rating(user, item, ratings_matrix, similarity_matrix, k=10):
    # Get similar users who rated this item
    similar_users = similarity_matrix[user].argsort()[::-1][1:k+1]

    # Weighted average of their ratings
    weighted_sum = 0
    similarity_sum = 0

    for similar_user in similar_users:
        if ratings_matrix[similar_user, item] > 0:
            weighted_sum += similarity_matrix[user, similar_user] * ratings_matrix[similar_user, item]
            similarity_sum += abs(similarity_matrix[user, similar_user])

    if similarity_sum == 0:
        return 0
    return weighted_sum / similarity_sum
```

</details>

<details>
<summary><strong>Chapter 13: Automated ML Pipeline</strong></summary>

<a id="chapter-13-automated-ml-pipeline"></a>

**File:** `13_Tahabilder_automated_ml_pipeline.ipynb`

**Difficulty:** Advanced (7/10) | **Time:** 6 hours

### Overview

Move beyond notebooks to production-ready pipelines. Learn to automate preprocessing, training, and evaluation.

### What You'll Learn

1. **Scikit-learn Pipelines**
   - Pipeline construction
   - Custom transformers
   - ColumnTransformer

2. **Automation**
   - Preprocessing automation
   - Feature selection
   - Model selection

3. **Production Practices**
   - Pipeline serialization
   - Reproducibility
   - Testing pipelines

</details>

<details>
<summary><strong>Chapter 14: Language Model from Scratch</strong></summary>

<a id="chapter-14-language-model-from-scratch"></a>

**File:** `14_Tahabilder_language_model_scratch.ipynb`

**Difficulty:** Advanced (8/10) | **Time:** 10 hours

### Overview

Build a transformer-based language model from scratch. Understand the attention mechanism that powers GPT and BERT.

### What You'll Learn

1. **Tokenization**
   - Character-level
   - Word-level
   - Subword (BPE)

2. **Attention Mechanism**
   - Self-attention intuition
   - Query, Key, Value
   - Multi-head attention

3. **Transformer Architecture**
   - Positional encoding
   - Layer normalization
   - Feed-forward networks

4. **Text Generation**
   - Greedy decoding
   - Beam search
   - Temperature sampling

### Self-Attention Formula

```
Attention(Q, K, V) = softmax(QK^T / √d_k) × V
```

Where:
- Q = Query matrix (what am I looking for?)
- K = Key matrix (what do I contain?)
- V = Value matrix (what do I return?)
- d_k = dimension of keys (scaling factor)

</details>

<details>
<summary><strong>Chapter 15: A/B Testing Framework</strong></summary>

<a id="chapter-15-ab-testing-framework"></a>

**File:** `15_Tahabilder_ab_testing_framework.ipynb`

**Difficulty:** Advanced (7/10) | **Time:** 7 hours

### Overview

Build a complete A/B testing framework. Learn statistical testing and experiment design used at tech companies.

### What You'll Learn

1. **Experiment Design**
   - Sample size calculation
   - Power analysis
   - Randomization

2. **Statistical Tests**
   - t-test
   - Chi-square
   - Mann-Whitney U

3. **Bayesian A/B Testing**
   - Prior specification
   - Posterior computation
   - Decision rules

4. **Multi-Armed Bandits**
   - Epsilon-greedy
   - UCB
   - Thompson Sampling

</details>

<details>
<summary><strong>Chapter 16: GAN Image Generator</strong></summary>

<a id="chapter-16-gan-image-generator"></a>

**File:** `16_Tahabilder_gan_image_generator.ipynb`

**Difficulty:** Advanced (8/10) | **Time:** 8 hours

### Overview

Generate new images using Generative Adversarial Networks. Enter the fascinating world of generative models.

### What You'll Learn

1. **GAN Architecture**
   - Generator network
   - Discriminator network
   - Adversarial training

2. **Training Dynamics**
   - Min-max game
   - Training stability
   - Mode collapse

3. **Advanced GANs**
   - DCGAN
   - Conditional GAN
   - Progressive GAN concepts

### GAN Training

```python
# Discriminator training
real_loss = bce_loss(discriminator(real_images), torch.ones(...))
fake_images = generator(noise)
fake_loss = bce_loss(discriminator(fake_images.detach()), torch.zeros(...))
d_loss = real_loss + fake_loss

# Generator training
fake_images = generator(noise)
g_loss = bce_loss(discriminator(fake_images), torch.ones(...))
```

</details>

<details>
<summary><strong>Chapter 17: Multilingual NLP Pipeline</strong></summary>

<a id="chapter-17-multilingual-nlp-pipeline"></a>

**File:** `17_Tahabilder_multilingual_nlp_pipeline.ipynb`

**Difficulty:** Advanced (7/10) | **Time:** 7 hours

### Overview

Handle text in multiple languages. Build models that work across linguistic boundaries.

### What You'll Learn

1. **Language Detection**
2. **Cross-lingual Embeddings**
3. **Named Entity Recognition**
4. **Multilingual Classification**

</details>

<details>
<summary><strong>Chapter 18: Reinforcement Learning Agent</strong></summary>

<a id="chapter-18-reinforcement-learning-agent"></a>

**File:** `18_Tahabilder_reinforcement_learning_agent.ipynb`

**Difficulty:** Advanced (8/10) | **Time:** 8 hours

### Overview

Train an agent to play games using reinforcement learning. Learn Q-learning and policy gradients.

### What You'll Learn

1. **RL Fundamentals**
   - States, Actions, Rewards
   - Policy vs Value
   - Exploration vs Exploitation

2. **Q-Learning**
   - Q-table
   - Bellman equation
   - DQN

3. **Policy Gradients**
   - REINFORCE
   - Actor-Critic

### Q-Learning Update

```
Q(s,a) = Q(s,a) + α[r + γ × max(Q(s',a')) - Q(s,a)]
```

</details>

<details>
<summary><strong>Chapter 19: Real-Time Fraud Detection</strong></summary>

<a id="chapter-19-real-time-fraud-detection"></a>

**File:** `19_Tahabilder_real_time_fraud_detection.ipynb`

**Difficulty:** Advanced (8/10) | **Time:** 7 hours

### Overview

Build a system that detects fraudulent transactions in real-time. Learn streaming ML and anomaly detection.

### What You'll Learn

1. **Anomaly Detection**
   - Isolation Forest
   - Autoencoders
   - One-class SVM

2. **Feature Engineering**
   - Transaction velocity
   - Behavioral patterns
   - Geographic features

3. **Real-Time Systems**
   - Streaming concepts
   - Model updating

</details>

<details>
<summary><strong>Chapter 20: AutoML System</strong></summary>

<a id="chapter-20-automl-system"></a>

**File:** `20_Tahabilder_automl_system.ipynb`

**Difficulty:** Expert (9/10) | **Time:** 10 hours

### Overview

Build a system that automatically selects and tunes ML models. Learn hyperparameter optimization and neural architecture search.

### What You'll Learn

1. **Hyperparameter Optimization**
   - Grid Search
   - Random Search
   - Bayesian Optimization

2. **Neural Architecture Search**
   - Search space design
   - Search algorithms
   - Evaluation strategies

3. **Meta-Learning**
   - Learning to learn
   - Warm starting

</details>

<details>
<summary><strong>Chapter 21: MLOps Pipeline</strong></summary>

<a id="chapter-21-mlops-pipeline"></a>

**File:** `21_Tahabilder_mlops_pipeline.ipynb`

**Difficulty:** Expert (9/10) | **Time:** 10 hours

### Overview

Build a complete MLOps pipeline with CI/CD, monitoring, and deployment. Bridge model development and production.

### What You'll Learn

1. **Version Control for ML**
   - Code versioning
   - Data versioning
   - Model versioning

2. **CI/CD for ML**
   - Automated testing
   - Model validation
   - Deployment automation

3. **Monitoring**
   - Data drift detection
   - Model performance tracking
   - Alert systems

</details>

<details>
<summary><strong>Chapter 22: Distributed ML Training</strong></summary>

<a id="chapter-22-distributed-ml-training"></a>

**File:** `22_Tahabilder_distributed_ml_training.ipynb`

**Difficulty:** Expert (10/10) | **Time:** 12 hours

### Overview

The capstone project! Build a complete distributed ML system from scratch supporting federated learning, data parallelism, secure aggregation, and fault tolerance.

### What You'll Learn

1. **Distributed Training Strategies**
   - Data Parallelism
   - Model Parallelism
   - Federated Learning

2. **Aggregation Algorithms**
   - FedAvg
   - Ring AllReduce
   - Parameter Server

3. **Privacy & Security**
   - Differential Privacy
   - Secure Aggregation
   - Gradient Protection

4. **System Design**
   - Fault Tolerance
   - Checkpointing
   - Health Monitoring

5. **Communication Optimization**
   - Gradient Compression
   - Top-K Sparsification
   - Quantization

### System Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                 DISTRIBUTED ML SYSTEM                        │
├─────────────────────────────────────────────────────────────┤
│  STRATEGIES: Federated Learning | Data Parallelism          │
├─────────────────────────────────────────────────────────────┤
│  FRAMEWORKS: PyTorch | TensorFlow (Switchable)              │
├─────────────────────────────────────────────────────────────┤
│  ADVANCED:   Secure Aggregation | Fault Tolerance |         │
│              Gradient Compression                            │
└─────────────────────────────────────────────────────────────┘
```

### Classes Implemented

- `DistributedMLConfig` - Configuration management
- `ModelWrapper` - Framework-agnostic interface
- `Communicator` - Message passing abstraction
- `FederatedLearning` - FL orchestrator
- `DataParallelTrainer` - DP orchestrator
- `SecureAggregator` - Privacy-preserving aggregation
- `CheckpointManager` - Fault tolerance
- `GradientCompressor` - Communication optimization

</details>

---

## Technologies Used

<details>
<summary><strong>Core Stack</strong></summary>

| Technology | Purpose |
|------------|---------|
| Python 3.8+ | Primary language |
| NumPy | Numerical computing |
| Pandas | Data manipulation |
| Matplotlib/Seaborn | Visualization |
| Scikit-learn | ML algorithms |
| TensorFlow 2.x | Deep learning |
| PyTorch | Deep learning |

</details>

<details>
<summary><strong>Specialized Libraries</strong></summary>

| Library | Chapters | Purpose |
|---------|----------|---------|
| NLTK, spaCy | 7, 14, 17 | NLP |
| OpenCV | 6, 11 | Computer vision |
| XGBoost, LightGBM | 5, 8, 19 | Gradient boosting |
| Transformers | 14, 17 | Pre-trained models |
| OpenAI Gym | 18 | RL environments |

</details>

---

## Setup Instructions

<details>
<summary><strong>Quick Start</strong></summary>

```bash
# Clone the repository
git clone https://github.com/aniktahabilder/ml-portfolio.git
cd ml-portfolio

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Start Jupyter
jupyter notebook
```

</details>

<details>
<summary><strong>Google Colab</strong></summary>

Each notebook can be opened directly in Google Colab. Just click the "Open in Colab" badge at the top of each notebook.

</details>

<details>
<summary><strong>Kaggle</strong></summary>

All notebooks are designed to run on Kaggle with default dependencies. Simply upload the notebook and run!

</details>

---

## Requirements

```
numpy>=1.21.0
pandas>=1.3.0
matplotlib>=3.4.0
seaborn>=0.11.0
scikit-learn>=1.0.0
tensorflow>=2.8.0
torch>=1.10.0
nltk>=3.6.0
opencv-python>=4.5.0
xgboost>=1.5.0
```

---

## About the Author

**Anik Tahabilder** is a PhD student in the Department of Computer Science at Wayne State University, Detroit, Michigan. His research focuses on machine learning systems, distributed computing, and privacy-preserving machine learning.

This repository represents his practical journey through machine learning, documenting the concepts, challenges, and solutions encountered while building real ML systems.

---

## License

This project is open source and available under the MIT License.

---

## Acknowledgments

- Wayne State University Department of Computer Science
- The open-source ML community
- Authors of scikit-learn, TensorFlow, and PyTorch

---

*Last Updated: January 2025*
