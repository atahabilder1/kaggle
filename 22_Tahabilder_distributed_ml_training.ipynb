{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Machine Learning System\n",
    "\n",
    "**Author:** Anik Tahabilder  \n",
    "**Project:** 22 of 22 - Kaggle ML Portfolio  \n",
    "**Topic:** Distributed & Federated Learning  \n",
    "**Difficulty:** 10/10 | **Learning Value:** 10/10 | **Resume Value:** 10/10\n",
    "\n",
    "---\n",
    "\n",
    "## What is Distributed Machine Learning?\n",
    "\n",
    "**Distributed Machine Learning** is a paradigm where model training is spread across multiple machines or processes to handle:\n",
    "- **Large datasets** that don't fit in single machine memory\n",
    "- **Complex models** requiring massive compute resources\n",
    "- **Privacy-sensitive data** that cannot be centralized\n",
    "\n",
    "### Types of Distributed ML:\n",
    "\n",
    "| Approach | Description | Use Case |\n",
    "|----------|-------------|----------|\n",
    "| **Data Parallelism** | Split data across workers, each trains same model | Large datasets, faster training |\n",
    "| **Model Parallelism** | Split model across workers | Very large models (GPT, etc.) |\n",
    "| **Federated Learning** | Train on decentralized data, share only updates | Privacy-preserving ML |\n",
    "\n",
    "### What We'll Build:\n",
    "\n",
    "A **complete distributed ML system** featuring:\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Federated Learning** | FedAvg, FedProx algorithms |\n",
    "| **Data Parallelism** | Synchronous SGD, Ring AllReduce |\n",
    "| **Framework-Agnostic** | Works with PyTorch AND TensorFlow |\n",
    "| **Secure Aggregation** | Encrypted gradients, differential privacy |\n",
    "| **Fault Tolerance** | Checkpointing, failure recovery |\n",
    "| **Communication Optimization** | Gradient compression |\n",
    "\n",
    "---\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         DISTRIBUTED ML SYSTEM                               │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│    ┌─────────────────────────────────────────────────────────────────┐     │\n",
    "│    │                     PARAMETER SERVER                             │     │\n",
    "│    │  • Maintains global model    • Aggregates updates               │     │\n",
    "│    │  • Coordinates training      • Broadcasts weights               │     │\n",
    "│    └──────────────────────┬───────────────────────────────────────────┘     │\n",
    "│                           │                                                 │\n",
    "│           ┌───────────────┼───────────────┬───────────────┐                │\n",
    "│           │               │               │               │                │\n",
    "│           ▼               ▼               ▼               ▼                │\n",
    "│    ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐          │\n",
    "│    │ Worker 1 │    │ Worker 2 │    │ Worker 3 │    │ Worker N │          │\n",
    "│    │ ┌──────┐ │    │ ┌──────┐ │    │ ┌──────┐ │    │ ┌──────┐ │          │\n",
    "│    │ │Model │ │    │ │Model │ │    │ │Model │ │    │ │Model │ │          │\n",
    "│    │ └──────┘ │    │ └──────┘ │    │ └──────┘ │    │ └──────┘ │          │\n",
    "│    │ ┌──────┐ │    │ ┌──────┐ │    │ ┌──────┐ │    │ ┌──────┐ │          │\n",
    "│    │ │ Data │ │    │ │ Data │ │    │ │ Data │ │    │ │ Data │ │          │\n",
    "│    │ └──────┘ │    │ └──────┘ │    │ └──────┘ │    │ └──────┘ │          │\n",
    "│    └──────────┘    └──────────┘    └──────────┘    └──────────┘          │\n",
    "│                                                                             │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│  TRAINING FLOW:                                                             │\n",
    "│  1. Server broadcasts global model to workers                               │\n",
    "│  2. Workers train locally on their data                                     │\n",
    "│  3. Workers send gradients/weights to server                                │\n",
    "│  4. Server aggregates updates (FedAvg, etc.)                               │\n",
    "│  5. Repeat until convergence                                                │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: Setup and Configuration](#part1)\n",
    "2. [Part 2: Distributed ML Fundamentals](#part2)\n",
    "3. [Part 3: Framework-Agnostic Model Wrapper](#part3)\n",
    "4. [Part 4: Communication Layer](#part4)\n",
    "5. [Part 5: Federated Learning Implementation](#part5)\n",
    "6. [Part 6: Data Parallelism Implementation](#part6)\n",
    "7. [Part 7: Secure Aggregation](#part7)\n",
    "8. [Part 8: Fault Tolerance](#part8)\n",
    "9. [Part 9: Communication Optimization](#part9)\n",
    "10. [Part 10: Complete Distributed ML System](#part10)\n",
    "11. [Part 11: Comprehensive Demos](#part11)\n",
    "12. [Part 12: Summary and Conclusions](#part12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='part1'></a>\n",
    "# Part 1: Setup and Configuration\n",
    "---\n",
    "\n",
    "## 1.1 Importing Libraries\n",
    "\n",
    "| Library | Purpose |\n",
    "|---------|--------|\n",
    "| **numpy/pandas** | Data manipulation |\n",
    "| **torch** | PyTorch deep learning framework |\n",
    "| **tensorflow** | TensorFlow deep learning framework |\n",
    "| **multiprocessing/threading** | Parallel execution |\n",
    "| **queue** | Thread-safe communication |\n",
    "| **dataclasses** | Configuration management |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CORE LIBRARIES\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================================\n",
    "# DEEP LEARNING FRAMEWORKS\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ============================================================\n",
    "# DISTRIBUTED/PARALLEL COMPUTING\n",
    "# ============================================================\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Process, Queue, Manager\n",
    "import threading\n",
    "from threading import Thread, Lock\n",
    "import queue\n",
    "import socket\n",
    "import pickle\n",
    "import struct\n",
    "\n",
    "# ============================================================\n",
    "# UTILITIES\n",
    "# ============================================================\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Dict, List, Optional, Any, Tuple, Union, Callable\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "import hashlib\n",
    "import time\n",
    "import uuid\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# SKLEARN FOR DATA\n",
    "# ============================================================\n",
    "from sklearn.datasets import fetch_openml, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# ============================================================\n",
    "# DISPLAY SETTINGS\n",
    "# ============================================================\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.precision', 4)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DISTRIBUTED ML SYSTEM - LIBRARIES LOADED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"CPU cores available: {mp.cpu_count()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configuration Classes\n",
    "\n",
    "We use **dataclasses** to manage configuration - a clean, type-safe approach used in production ML systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ENUMS FOR TYPE-SAFE CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "class ExecutionMode(Enum):\n",
    "    \"\"\"How the distributed system runs.\"\"\"\n",
    "    SIMULATION = \"simulation\"    # Single machine, multiple threads/processes\n",
    "    NETWORK = \"network\"          # Multiple machines over network\n",
    "\n",
    "class TrainingStrategy(Enum):\n",
    "    \"\"\"Distributed training approach.\"\"\"\n",
    "    FEDERATED = \"federated\"      # Federated Learning (decentralized data)\n",
    "    DATA_PARALLEL = \"data_parallel\"  # Data Parallelism (centralized data split)\n",
    "\n",
    "class AggregationMethod(Enum):\n",
    "    \"\"\"How to aggregate worker updates.\"\"\"\n",
    "    FEDAVG = \"fedavg\"            # Federated Averaging\n",
    "    FEDPROX = \"fedprox\"          # FedAvg with proximal term\n",
    "    SYNC_SGD = \"sync_sgd\"        # Synchronous SGD\n",
    "    ASYNC_SGD = \"async_sgd\"      # Asynchronous SGD\n",
    "    RING_ALLREDUCE = \"ring_allreduce\"  # Ring AllReduce\n",
    "\n",
    "class Framework(Enum):\n",
    "    \"\"\"ML framework to use.\"\"\"\n",
    "    PYTORCH = \"pytorch\"\n",
    "    TENSORFLOW = \"tensorflow\"\n",
    "\n",
    "print(\"Enums defined:\")\n",
    "print(f\"  ExecutionMode: {[e.value for e in ExecutionMode]}\")\n",
    "print(f\"  TrainingStrategy: {[e.value for e in TrainingStrategy]}\")\n",
    "print(f\"  AggregationMethod: {[e.value for e in AggregationMethod]}\")\n",
    "print(f\"  Framework: {[e.value for e in Framework]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN CONFIGURATION DATACLASS\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class DistributedMLConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the Distributed ML System.\n",
    "    \n",
    "    This dataclass holds all settings for distributed training,\n",
    "    making it easy to experiment with different configurations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # === Execution Settings ===\n",
    "    mode: ExecutionMode = ExecutionMode.SIMULATION\n",
    "    strategy: TrainingStrategy = TrainingStrategy.FEDERATED\n",
    "    aggregation: AggregationMethod = AggregationMethod.FEDAVG\n",
    "    framework: Framework = Framework.PYTORCH\n",
    "    \n",
    "    # === Worker Settings ===\n",
    "    n_workers: int = 5\n",
    "    worker_addresses: List[str] = field(default_factory=list)  # For network mode\n",
    "    \n",
    "    # === Training Hyperparameters ===\n",
    "    local_epochs: int = 5        # Epochs per worker per round\n",
    "    global_rounds: int = 10      # Total communication rounds\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 0.01\n",
    "    \n",
    "    # === Federated Learning Specific ===\n",
    "    client_fraction: float = 1.0  # Fraction of clients per round\n",
    "    fedprox_mu: float = 0.01      # Proximal term coefficient\n",
    "    iid_data: bool = True         # IID vs Non-IID data distribution\n",
    "    \n",
    "    # === Advanced Features ===\n",
    "    secure_aggregation: bool = False\n",
    "    differential_privacy: bool = False\n",
    "    dp_epsilon: float = 1.0       # Privacy budget\n",
    "    dp_delta: float = 1e-5        # Privacy parameter\n",
    "    \n",
    "    gradient_compression: bool = False\n",
    "    compression_ratio: float = 0.1  # Keep top 10% gradients\n",
    "    \n",
    "    fault_tolerance: bool = True\n",
    "    checkpoint_frequency: int = 5   # Checkpoint every N rounds\n",
    "    max_worker_failures: int = 2    # Max failures before abort\n",
    "    \n",
    "    # === Logging ===\n",
    "    verbose: bool = True\n",
    "    log_frequency: int = 1\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration after initialization.\"\"\"\n",
    "        assert self.n_workers > 0, \"Must have at least 1 worker\"\n",
    "        assert 0 < self.client_fraction <= 1.0, \"Client fraction must be in (0, 1]\"\n",
    "        assert self.compression_ratio > 0, \"Compression ratio must be positive\"\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert config to dictionary (for logging).\"\"\"\n",
    "        result = {}\n",
    "        for key, value in asdict(self).items():\n",
    "            if isinstance(value, Enum):\n",
    "                result[key] = value.value\n",
    "            else:\n",
    "                result[key] = value\n",
    "        return result\n",
    "\n",
    "# Create default configuration\n",
    "config = DistributedMLConfig()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DEFAULT CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in config.to_dict().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Load Dataset\n",
    "\n",
    "We'll use **MNIST** for demonstrations - it's small enough to run quickly but complex enough to show distributed training benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD AND PREPARE MNIST DATASET\n",
    "# ============================================================\n",
    "\n",
    "def load_mnist_data(n_samples: int = 10000) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load MNIST dataset and preprocess for distributed training.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples to use (for faster demos)\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test as numpy arrays\n",
    "    \"\"\"\n",
    "    print(\"Loading MNIST dataset...\")\n",
    "    \n",
    "    # Load using sklearn (smaller subset for demos)\n",
    "    digits = load_digits()\n",
    "    X, y = digits.data, digits.target\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    X = X / 16.0  # Digits dataset is 0-16\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset loaded:\")\n",
    "    print(f\"  Training samples: {len(X_train)}\")\n",
    "    print(f\"  Test samples: {len(X_test)}\")\n",
    "    print(f\"  Features: {X_train.shape[1]}\")\n",
    "    print(f\"  Classes: {len(np.unique(y_train))}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Load data\n",
    "X_train, X_test, y_train, y_test = load_mnist_data()\n",
    "\n",
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(X_train[i].reshape(8, 8), cmap='gray')\n",
    "    ax.set_title(f'Label: {y_train[i]}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample MNIST Digits', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='part2'></a>\n",
    "# Part 2: Distributed ML Fundamentals\n",
    "---\n",
    "\n",
    "Before implementing, let's understand the **key concepts** in distributed machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Parallelism vs Model Parallelism\n",
    "\n",
    "| Aspect | Data Parallelism | Model Parallelism |\n",
    "|--------|-----------------|------------------|\n",
    "| **What's split** | Data across workers | Model across workers |\n",
    "| **Each worker has** | Full model, subset of data | Part of model, all data |\n",
    "| **Communication** | Gradients/weights | Activations |\n",
    "| **Best for** | Large datasets | Very large models |\n",
    "| **Example** | Training ImageNet | Training GPT-4 |\n",
    "\n",
    "```\n",
    "DATA PARALLELISM:                    MODEL PARALLELISM:\n",
    "┌─────────────────────┐              ┌─────────────────────┐\n",
    "│      Full Model     │              │   Layer 1 │ Layer 2 │\n",
    "├─────────────────────┤              ├───────────┼─────────┤\n",
    "│ Data1 │ Data2 │ Data3│              │  Worker1  │ Worker2 │\n",
    "│Worker1│Worker2│Worker3│              │           │         │\n",
    "└─────────────────────┘              │  All Data │ All Data│\n",
    "     │      │      │                 └───────────┴─────────┘\n",
    "     └──────┼──────┘                        │         │\n",
    "            ▼                               └────┬────┘\n",
    "    Aggregate Gradients                   Pass Activations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Federated Learning vs Traditional Distributed Training\n",
    "\n",
    "| Aspect | Traditional Distributed | Federated Learning |\n",
    "|--------|------------------------|-------------------|\n",
    "| **Data location** | Centralized, then split | Decentralized (stays local) |\n",
    "| **Data access** | Full access | Never see raw data |\n",
    "| **Privacy** | Low | High |\n",
    "| **Data distribution** | IID (controlled) | Non-IID (natural) |\n",
    "| **Network** | Fast datacenter | Slow, unreliable |\n",
    "| **Workers** | Homogeneous | Heterogeneous |\n",
    "\n",
    "### Federated Learning Use Cases:\n",
    "- **Healthcare**: Hospitals train shared model without sharing patient data\n",
    "- **Mobile**: Keyboard prediction trained on user devices\n",
    "- **Finance**: Banks collaborate without exposing transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Communication Patterns\n",
    "\n",
    "### Parameter Server Architecture:\n",
    "```\n",
    "           ┌─────────────────┐\n",
    "           │ Parameter Server│\n",
    "           │   (Aggregator)  │\n",
    "           └────────┬────────┘\n",
    "                    │\n",
    "        ┌───────────┼───────────┐\n",
    "        │           │           │\n",
    "        ▼           ▼           ▼\n",
    "   ┌────────┐  ┌────────┐  ┌────────┐\n",
    "   │Worker 1│  │Worker 2│  │Worker 3│\n",
    "   └────────┘  └────────┘  └────────┘\n",
    "\n",
    "Pro: Simple, centralized coordination\n",
    "Con: Server bottleneck, single point of failure\n",
    "```\n",
    "\n",
    "### Ring AllReduce:\n",
    "```\n",
    "   ┌────────┐     ┌────────┐\n",
    "   │Worker 1│────▶│Worker 2│\n",
    "   └────┬───┘     └────┬───┘\n",
    "        │              │\n",
    "        │              │\n",
    "   ┌────┴───┐     ┌────┴───┐\n",
    "   │Worker 4│◀────│Worker 3│\n",
    "   └────────┘     └────────┘\n",
    "\n",
    "Pro: No bottleneck, bandwidth optimal\n",
    "Con: More complex, all workers must participate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION: COMMUNICATION PATTERNS\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Parameter Server visualization\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "\n",
    "# Server\n",
    "server = plt.Circle((5, 8), 0.8, color='#FF6B6B', ec='black', lw=2)\n",
    "ax1.add_patch(server)\n",
    "ax1.text(5, 8, 'Server', ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Workers\n",
    "worker_positions = [(2, 3), (5, 3), (8, 3)]\n",
    "colors = ['#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "for i, (pos, color) in enumerate(zip(worker_positions, colors)):\n",
    "    worker = plt.Circle(pos, 0.6, color=color, ec='black', lw=2)\n",
    "    ax1.add_patch(worker)\n",
    "    ax1.text(pos[0], pos[1], f'W{i+1}', ha='center', va='center', fontweight='bold')\n",
    "    # Arrows\n",
    "    ax1.annotate('', xy=(5, 7.2), xytext=(pos[0], pos[1]+0.6),\n",
    "                arrowprops=dict(arrowstyle='->', color='blue', lw=1.5))\n",
    "    ax1.annotate('', xy=(pos[0], pos[1]+0.6), xytext=(5, 7.2),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=1.5, ls='--'))\n",
    "\n",
    "ax1.text(1, 9.5, 'Blue: Send gradients', color='blue', fontsize=10)\n",
    "ax1.text(1, 9, 'Red: Receive model', color='red', fontsize=10)\n",
    "ax1.set_title('Parameter Server Architecture', fontweight='bold', fontsize=14)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Ring AllReduce visualization\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "\n",
    "ring_positions = [(3, 7), (7, 7), (7, 3), (3, 3)]\n",
    "for i, (pos, color) in enumerate(zip(ring_positions, ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])):\n",
    "    worker = plt.Circle(pos, 0.6, color=color, ec='black', lw=2)\n",
    "    ax2.add_patch(worker)\n",
    "    ax2.text(pos[0], pos[1], f'W{i+1}', ha='center', va='center', fontweight='bold')\n",
    "\n",
    "# Ring connections\n",
    "for i in range(4):\n",
    "    start = ring_positions[i]\n",
    "    end = ring_positions[(i+1) % 4]\n",
    "    ax2.annotate('', xy=end, xytext=start,\n",
    "                arrowprops=dict(arrowstyle='->', color='purple', lw=2))\n",
    "\n",
    "ax2.text(5, 5, 'Ring\\nTopology', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Ring AllReduce Architecture', fontweight='bold', fontsize=14)\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"- Parameter Server: Simple but can become bottleneck\")\n",
    "print(\"- Ring AllReduce: Bandwidth-optimal but requires all workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Aggregation Algorithms\n",
    "\n",
    "### FedAvg (Federated Averaging):\n",
    "\n",
    "The most popular federated learning algorithm:\n",
    "\n",
    "$$w_{t+1} = \\sum_{k=1}^{K} \\frac{n_k}{n} w_k^{t+1}$$\n",
    "\n",
    "Where:\n",
    "- $w_{t+1}$ = global model weights after round $t$\n",
    "- $w_k^{t+1}$ = local model weights from worker $k$\n",
    "- $n_k$ = number of samples on worker $k$\n",
    "- $n$ = total samples across all workers\n",
    "\n",
    "### FedProx:\n",
    "\n",
    "Adds a **proximal term** to handle heterogeneous data:\n",
    "\n",
    "$$\\min_w F_k(w) + \\frac{\\mu}{2} ||w - w_t||^2$$\n",
    "\n",
    "This keeps local models close to the global model, preventing drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ALGORITHM COMPARISON TABLE\n",
    "# ============================================================\n",
    "\n",
    "algorithms = pd.DataFrame({\n",
    "    'Algorithm': ['FedAvg', 'FedProx', 'Sync SGD', 'Async SGD', 'Ring AllReduce'],\n",
    "    'Type': ['Federated', 'Federated', 'Data Parallel', 'Data Parallel', 'Data Parallel'],\n",
    "    'Aggregation': ['Weighted Avg', 'Weighted Avg + Proximal', 'Gradient Avg', 'Gradient Update', 'Ring Reduction'],\n",
    "    'Handles Non-IID': ['Moderate', 'Good', 'Poor', 'Poor', 'Poor'],\n",
    "    'Communication': ['Low', 'Low', 'High', 'Medium', 'Optimal'],\n",
    "    'Fault Tolerance': ['Good', 'Good', 'Poor', 'Medium', 'Poor']\n",
    "})\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DISTRIBUTED ML ALGORITHM COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(algorithms.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='part3'></a>\n",
    "# Part 3: Framework-Agnostic Model Wrapper\n",
    "---\n",
    "\n",
    "To support both **PyTorch** and **TensorFlow**, we create an abstraction layer that provides a unified interface.\n",
    "\n",
    "## 3.1 Why Framework-Agnostic?\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **Flexibility** | Users can choose their preferred framework |\n",
    "| **Portability** | Same distributed code works with both |\n",
    "| **Comparison** | Easy to benchmark PyTorch vs TensorFlow |\n",
    "| **Production** | Deploy with whatever framework fits infrastructure |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ABSTRACT MODEL WRAPPER INTERFACE\n",
    "# ============================================================\n",
    "\n",
    "class ModelWrapper(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for framework-agnostic model handling.\n",
    "    \n",
    "    This interface allows the distributed training system to work\n",
    "    with any ML framework (PyTorch, TensorFlow, etc.) transparently.\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_weights(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Get model weights as numpy arrays.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping layer names to weight arrays\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def set_weights(self, weights: Dict[str, np.ndarray]) -> None:\n",
    "        \"\"\"\n",
    "        Set model weights from numpy arrays.\n",
    "        \n",
    "        Args:\n",
    "            weights: Dictionary mapping layer names to weight arrays\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train_step(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Perform one training step (forward + backward + update).\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            y: Target labels\n",
    "            \n",
    "        Returns:\n",
    "            Loss value for this step\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train_epoch(self, X: np.ndarray, y: np.ndarray, batch_size: int) -> float:\n",
    "        \"\"\"\n",
    "        Train for one full epoch.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            y: Target labels\n",
    "            batch_size: Batch size for training\n",
    "            \n",
    "        Returns:\n",
    "            Average loss for the epoch\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate model on data.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            y: Target labels\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with 'loss' and 'accuracy'\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_gradients(self, X: np.ndarray, y: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute gradients without updating weights.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            y: Target labels\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping layer names to gradient arrays\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def apply_gradients(self, gradients: Dict[str, np.ndarray]) -> None:\n",
    "        \"\"\"\n",
    "        Apply pre-computed gradients to update weights.\n",
    "        \n",
    "        Args:\n",
    "            gradients: Dictionary mapping layer names to gradient arrays\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def clone(self) -> 'ModelWrapper':\n",
    "        \"\"\"\n",
    "        Create a deep copy of this model wrapper.\n",
    "        \n",
    "        Returns:\n",
    "            New ModelWrapper with same architecture and weights\n",
    "        \"\"\"\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "print(\"ModelWrapper abstract interface defined!\")\n",
    "print(\"\\nMethods:\")\n",
    "for method in ['get_weights', 'set_weights', 'train_step', 'train_epoch', \n",
    "               'evaluate', 'get_gradients', 'apply_gradients', 'clone']:\n",
    "    print(f\"  - {method}()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PYTORCH MODEL WRAPPER\n",
    "# ============================================================\n",
    "\n",
    "class PyTorchModelWrapper(ModelWrapper):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of ModelWrapper.\n",
    "    \n",
    "    Wraps a PyTorch nn.Module to provide the standard interface\n",
    "    for distributed training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, learning_rate: float = 0.01):\n",
    "        \"\"\"\n",
    "        Initialize PyTorch model wrapper.\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch nn.Module\n",
    "            learning_rate: Learning rate for optimizer\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def get_weights(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Extract weights as numpy arrays.\"\"\"\n",
    "        weights = {}\n",
    "        for name, param in self.model.named_parameters():\n",
    "            weights[name] = param.data.cpu().numpy().copy()\n",
    "        return weights\n",
    "    \n",
    "    def set_weights(self, weights: Dict[str, np.ndarray]) -> None:\n",
    "        \"\"\"Set weights from numpy arrays.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if name in weights:\n",
    "                    param.data = torch.tensor(weights[name], \n",
    "                                             dtype=param.dtype,\n",
    "                                             device=self.device)\n",
    "    \n",
    "    def train_step(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"Perform single training step.\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long, device=self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(X_tensor)\n",
    "        loss = self.criterion(outputs, y_tensor)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def train_epoch(self, X: np.ndarray, y: np.ndarray, batch_size: int) -> float:\n",
    "        \"\"\"Train for one full epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        n_samples = len(X)\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        total_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for start_idx in range(0, n_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "            \n",
    "            loss = self.train_step(X_batch, y_batch)\n",
    "            total_loss += loss\n",
    "            n_batches += 1\n",
    "        \n",
    "        return total_loss / n_batches\n",
    "    \n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate model.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "            y_tensor = torch.tensor(y, dtype=torch.long, device=self.device)\n",
    "            \n",
    "            outputs = self.model(X_tensor)\n",
    "            loss = self.criterion(outputs, y_tensor).item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            accuracy = (predicted == y_tensor).float().mean().item()\n",
    "        \n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "    \n",
    "    def get_gradients(self, X: np.ndarray, y: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Compute gradients without updating.\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long, device=self.device)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(X_tensor)\n",
    "        loss = self.criterion(outputs, y_tensor)\n",
    "        loss.backward()\n",
    "        \n",
    "        gradients = {}\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                gradients[name] = param.grad.cpu().numpy().copy()\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def apply_gradients(self, gradients: Dict[str, np.ndarray]) -> None:\n",
    "        \"\"\"Apply pre-computed gradients.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if name in gradients:\n",
    "                    grad_tensor = torch.tensor(gradients[name], \n",
    "                                              dtype=param.dtype,\n",
    "                                              device=self.device)\n",
    "                    param.data -= self.learning_rate * grad_tensor\n",
    "\n",
    "print(\"PyTorchModelWrapper implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TENSORFLOW MODEL WRAPPER\n",
    "# ============================================================\n",
    "\n",
    "class TensorFlowModelWrapper(ModelWrapper):\n",
    "    \"\"\"\n",
    "    TensorFlow/Keras implementation of ModelWrapper.\n",
    "    \n",
    "    Wraps a Keras Model to provide the standard interface\n",
    "    for distributed training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: keras.Model, learning_rate: float = 0.01):\n",
    "        \"\"\"\n",
    "        Initialize TensorFlow model wrapper.\n",
    "        \n",
    "        Args:\n",
    "            model: Keras Model\n",
    "            learning_rate: Learning rate for optimizer\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "        self.loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        \n",
    "        # Compile model\n",
    "        self.model.compile(\n",
    "            optimizer=self.optimizer,\n",
    "            loss=self.loss_fn,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    def get_weights(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Extract weights as numpy arrays.\"\"\"\n",
    "        weights = {}\n",
    "        for layer in self.model.layers:\n",
    "            for i, w in enumerate(layer.get_weights()):\n",
    "                weights[f\"{layer.name}_{i}\"] = w.copy()\n",
    "        return weights\n",
    "    \n",
    "    def set_weights(self, weights: Dict[str, np.ndarray]) -> None:\n",
    "        \"\"\"Set weights from numpy arrays.\"\"\"\n",
    "        for layer in self.model.layers:\n",
    "            layer_weights = []\n",
    "            for i in range(len(layer.get_weights())):\n",
    "                key = f\"{layer.name}_{i}\"\n",
    "                if key in weights:\n",
    "                    layer_weights.append(weights[key])\n",
    "            if layer_weights:\n",
    "                layer.set_weights(layer_weights)\n",
    "    \n",
    "    def train_step(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"Perform single training step.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(X, training=True)\n",
    "            loss = self.loss_fn(y, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        \n",
    "        return float(loss)\n",
    "    \n",
    "    def train_epoch(self, X: np.ndarray, y: np.ndarray, batch_size: int) -> float:\n",
    "        \"\"\"Train for one full epoch.\"\"\"\n",
    "        n_samples = len(X)\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        total_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for start_idx in range(0, n_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            X_batch = X[batch_indices].astype(np.float32)\n",
    "            y_batch = y[batch_indices].astype(np.int32)\n",
    "            \n",
    "            loss = self.train_step(X_batch, y_batch)\n",
    "            total_loss += loss\n",
    "            n_batches += 1\n",
    "        \n",
    "        return total_loss / n_batches\n",
    "    \n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate model.\"\"\"\n",
    "        X = X.astype(np.float32)\n",
    "        y = y.astype(np.int32)\n",
    "        \n",
    "        results = self.model.evaluate(X, y, verbose=0)\n",
    "        return {'loss': results[0], 'accuracy': results[1]}\n",
    "    \n",
    "    def get_gradients(self, X: np.ndarray, y: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Compute gradients without updating.\"\"\"\n",
    "        X = X.astype(np.float32)\n",
    "        y = y.astype(np.int32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(X, training=True)\n",
    "            loss = self.loss_fn(y, predictions)\n",
    "        \n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        \n",
    "        gradients = {}\n",
    "        for var, grad in zip(self.model.trainable_variables, grads):\n",
    "            if grad is not None:\n",
    "                gradients[var.name] = grad.numpy()\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def apply_gradients(self, gradients: Dict[str, np.ndarray]) -> None:\n",
    "        \"\"\"Apply pre-computed gradients.\"\"\"\n",
    "        grads_and_vars = []\n",
    "        for var in self.model.trainable_variables:\n",
    "            if var.name in gradients:\n",
    "                grad = tf.constant(gradients[var.name], dtype=var.dtype)\n",
    "                grads_and_vars.append((grad, var))\n",
    "        \n",
    "        if grads_and_vars:\n",
    "            self.optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "print(\"TensorFlowModelWrapper implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL FACTORY - CREATE MODELS FOR EITHER FRAMEWORK\n",
    "# ============================================================\n",
    "\n",
    "def create_pytorch_model(input_dim: int, n_classes: int) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Create a simple neural network in PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Number of input features\n",
    "        n_classes: Number of output classes\n",
    "        \n",
    "    Returns:\n",
    "        PyTorch nn.Module\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(64, n_classes)\n",
    "    )\n",
    "\n",
    "def create_tensorflow_model(input_dim: int, n_classes: int) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Create a simple neural network in TensorFlow/Keras.\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Number of input features\n",
    "        n_classes: Number of output classes\n",
    "        \n",
    "    Returns:\n",
    "        Keras Model\n",
    "    \"\"\"\n",
    "    return keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(n_classes)\n",
    "    ])\n",
    "\n",
    "def create_model_wrapper(framework: Framework, input_dim: int, n_classes: int, \n",
    "                         learning_rate: float = 0.01) -> ModelWrapper:\n",
    "    \"\"\"\n",
    "    Factory function to create appropriate model wrapper.\n",
    "    \n",
    "    Args:\n",
    "        framework: Framework enum (PYTORCH or TENSORFLOW)\n",
    "        input_dim: Number of input features\n",
    "        n_classes: Number of output classes\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        \n",
    "    Returns:\n",
    "        ModelWrapper instance\n",
    "    \"\"\"\n",
    "    if framework == Framework.PYTORCH:\n",
    "        model = create_pytorch_model(input_dim, n_classes)\n",
    "        return PyTorchModelWrapper(model, learning_rate)\n",
    "    else:\n",
    "        model = create_tensorflow_model(input_dim, n_classes)\n",
    "        return TensorFlowModelWrapper(model, learning_rate)\n",
    "\n",
    "print(\"Model factory functions created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEMO: FRAMEWORK-AGNOSTIC TRAINING\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMO: FRAMEWORK-AGNOSTIC MODEL WRAPPER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "# Test PyTorch wrapper\n",
    "print(\"\\n--- PyTorch Model ---\")\n",
    "pytorch_wrapper = create_model_wrapper(Framework.PYTORCH, input_dim, n_classes)\n",
    "\n",
    "# Get initial weights\n",
    "initial_weights = pytorch_wrapper.get_weights()\n",
    "print(f\"Number of weight tensors: {len(initial_weights)}\")\n",
    "print(f\"Weight names: {list(initial_weights.keys())[:3]}...\")\n",
    "\n",
    "# Train for one epoch\n",
    "loss = pytorch_wrapper.train_epoch(X_train, y_train, batch_size=32)\n",
    "print(f\"Training loss: {loss:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "metrics = pytorch_wrapper.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {metrics['accuracy']*100:.2f}%\")\n",
    "\n",
    "# Test TensorFlow wrapper\n",
    "print(\"\\n--- TensorFlow Model ---\")\n",
    "tf_wrapper = create_model_wrapper(Framework.TENSORFLOW, input_dim, n_classes)\n",
    "\n",
    "# Train for one epoch\n",
    "loss = tf_wrapper.train_epoch(X_train, y_train, batch_size=32)\n",
    "print(f\"Training loss: {loss:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "metrics = tf_wrapper.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {metrics['accuracy']*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Both frameworks work with the same interface!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight:** The `ModelWrapper` abstraction allows our distributed training code to work with **any framework**. We can switch between PyTorch and TensorFlow by just changing one enum value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='part4'></a>\n",
    "# Part 4: Communication Layer\n",
    "---\n",
    "\n",
    "The communication layer abstracts **how workers and servers exchange messages**. This allows us to:\n",
    "- Run in **simulation mode** (single machine, queues)\n",
    "- Run in **network mode** (multiple machines, sockets)\n",
    "\n",
    "## 4.1 Communication Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MESSAGE TYPES\n",
    "# ============================================================\n",
    "\n",
    "class MessageType(Enum):\n",
    "    \"\"\"Types of messages in distributed training.\"\"\"\n",
    "    WEIGHTS = \"weights\"              # Model weights from server\n",
    "    GRADIENTS = \"gradients\"          # Gradients from worker\n",
    "    UPDATE = \"update\"                # Weight update from worker (FedAvg)\n",
    "    HEARTBEAT = \"heartbeat\"          # Worker health check\n",
    "    TRAIN_START = \"train_start\"      # Signal to start training\n",
    "    TRAIN_COMPLETE = \"train_complete\"  # Training round complete\n",
    "    SHUTDOWN = \"shutdown\"            # Shutdown signal\n",
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    \"\"\"\n",
    "    Message container for distributed communication.\n",
    "    \n",
    "    Attributes:\n",
    "        msg_type: Type of message\n",
    "        sender_id: ID of the sender\n",
    "        payload: Message content (weights, gradients, etc.)\n",
    "        timestamp: When message was created\n",
    "        round_num: Training round number (for synchronization)\n",
    "    \"\"\"\n",
    "    msg_type: MessageType\n",
    "    sender_id: str\n",
    "    payload: Any = None\n",
    "    timestamp: float = field(default_factory=time.time)\n",
    "    round_num: int = 0\n",
    "    \n",
    "    def serialize(self) -> bytes:\n",
    "        \"\"\"Serialize message to bytes for network transmission.\"\"\"\n",
    "        return pickle.dumps(self)\n",
    "    \n",
    "    @staticmethod\n",
    "    def deserialize(data: bytes) -> 'Message':\n",
    "        \"\"\"Deserialize message from bytes.\"\"\"\n",
    "        return pickle.loads(data)\n",
    "\n",
    "print(\"Message types defined:\")\n",
    "for msg_type in MessageType:\n",
    "    print(f\"  - {msg_type.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ABSTRACT COMMUNICATOR INTERFACE\n",
    "# ============================================================\n",
    "\n",
    "class Communicator(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for communication in distributed training.\n",
    "    \n",
    "    This interface abstracts away the details of how messages are\n",
    "    passed between workers and servers, allowing for both simulation\n",
    "    (in-memory queues) and real network (sockets) implementations.\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def send(self, destination: str, message: Message) -> bool:\n",
    "        \"\"\"\n",
    "        Send a message to a specific destination.\n",
    "        \n",
    "        Args:\n",
    "            destination: ID of the recipient\n",
    "            message: Message to send\n",
    "            \n",
    "        Returns:\n",
    "            True if sent successfully\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def receive(self, timeout: Optional[float] = None) -> Optional[Message]:\n",
    "        \"\"\"\n",
    "        Receive a message.\n",
    "        \n",
    "        Args:\n",
    "            timeout: Max time to wait (None = blocking)\n",
    "            \n",
    "        Returns:\n",
    "            Received message or None if timeout\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def broadcast(self, message: Message, destinations: List[str]) -> int:\n",
    "        \"\"\"\n",
    "        Send message to multiple destinations.\n",
    "        \n",
    "        Args:\n",
    "            message: Message to broadcast\n",
    "            destinations: List of recipient IDs\n",
    "            \n",
    "        Returns:\n",
    "            Number of successful sends\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        pass\n",
    "\n",
    "print(\"Communicator abstract interface defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SIMULATED COMMUNICATOR (QUEUE-BASED)\n",
    "# ============================================================\n",
    "\n",
    "class SimulatedCommunicator(Communicator):\n",
    "    \"\"\"\n",
    "    Queue-based communicator for single-machine simulation.\n",
    "    \n",
    "    Uses thread-safe queues to simulate network communication\n",
    "    between workers and server. Perfect for development and testing.\n",
    "    \n",
    "    Architecture:\n",
    "    - Each participant (worker/server) has an inbox queue\n",
    "    - Messages are passed through shared queue registry\n",
    "    - Thread-safe for multiprocessing simulation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class-level queue registry (shared across all instances)\n",
    "    _queues: Dict[str, queue.Queue] = {}\n",
    "    _lock = threading.Lock()\n",
    "    \n",
    "    def __init__(self, participant_id: str):\n",
    "        \"\"\"\n",
    "        Initialize communicator for a participant.\n",
    "        \n",
    "        Args:\n",
    "            participant_id: Unique ID for this participant\n",
    "        \"\"\"\n",
    "        self.participant_id = participant_id\n",
    "        \n",
    "        # Register this participant's inbox\n",
    "        with SimulatedCommunicator._lock:\n",
    "            if participant_id not in SimulatedCommunicator._queues:\n",
    "                SimulatedCommunicator._queues[participant_id] = queue.Queue()\n",
    "    \n",
    "    def send(self, destination: str, message: Message) -> bool:\n",
    "        \"\"\"\n",
    "        Send message to destination's inbox queue.\n",
    "        \"\"\"\n",
    "        with SimulatedCommunicator._lock:\n",
    "            if destination not in SimulatedCommunicator._queues:\n",
    "                SimulatedCommunicator._queues[destination] = queue.Queue()\n",
    "        \n",
    "        try:\n",
    "            SimulatedCommunicator._queues[destination].put(message)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Send error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def receive(self, timeout: Optional[float] = None) -> Optional[Message]:\n",
    "        \"\"\"\n",
    "        Receive message from own inbox queue.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            my_queue = SimulatedCommunicator._queues.get(self.participant_id)\n",
    "            if my_queue is None:\n",
    "                return None\n",
    "            \n",
    "            if timeout is not None:\n",
    "                return my_queue.get(timeout=timeout)\n",
    "            else:\n",
    "                return my_queue.get_nowait()\n",
    "        except queue.Empty:\n",
    "            return None\n",
    "    \n",
    "    def broadcast(self, message: Message, destinations: List[str]) -> int:\n",
    "        \"\"\"\n",
    "        Send message to all destinations.\n",
    "        \"\"\"\n",
    "        success_count = 0\n",
    "        for dest in destinations:\n",
    "            if self.send(dest, message):\n",
    "                success_count += 1\n",
    "        return success_count\n",
    "    \n",
    "    def close(self) -> None:\n",
    "        \"\"\"\n",
    "        Clean up this participant's queue.\n",
    "        \"\"\"\n",
    "        with SimulatedCommunicator._lock:\n",
    "            if self.participant_id in SimulatedCommunicator._queues:\n",
    "                del SimulatedCommunicator._queues[self.participant_id]\n",
    "    \n",
    "    @classmethod\n",
    "    def reset(cls):\n",
    "        \"\"\"Reset all queues (for testing).\"\"\"\n",
    "        with cls._lock:\n",
    "            cls._queues.clear()\n",
    "\n",
    "print(\"SimulatedCommunicator implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NETWORK COMMUNICATOR (SOCKET-BASED) - PRODUCTION REFERENCE\n",
    "# ============================================================\n",
    "\n",
    "class NetworkCommunicator(Communicator):\n",
    "    \"\"\"\n",
    "    Socket-based communicator for real multi-machine deployment.\n",
    "    \n",
    "    NOTE: This is provided as a production reference. In Kaggle,\n",
    "    we use SimulatedCommunicator instead since we can't do\n",
    "    cross-machine networking.\n",
    "    \n",
    "    For production deployment, you would:\n",
    "    1. Run server on one machine\n",
    "    2. Run workers on other machines\n",
    "    3. Configure firewall rules for the port\n",
    "    4. Use TLS for security (not shown here)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, participant_id: str, \n",
    "                 address_book: Dict[str, Tuple[str, int]],\n",
    "                 listen_port: int):\n",
    "        \"\"\"\n",
    "        Initialize network communicator.\n",
    "        \n",
    "        Args:\n",
    "            participant_id: Unique ID for this participant\n",
    "            address_book: Map of participant_id -> (host, port)\n",
    "            listen_port: Port to listen on for incoming messages\n",
    "        \"\"\"\n",
    "        self.participant_id = participant_id\n",
    "        self.address_book = address_book\n",
    "        self.listen_port = listen_port\n",
    "        self._inbox = queue.Queue()\n",
    "        self._running = False\n",
    "        self._listener_thread = None\n",
    "    \n",
    "    def start_listener(self):\n",
    "        \"\"\"Start background thread to listen for messages.\"\"\"\n",
    "        self._running = True\n",
    "        self._listener_thread = threading.Thread(target=self._listen_loop)\n",
    "        self._listener_thread.daemon = True\n",
    "        self._listener_thread.start()\n",
    "    \n",
    "    def _listen_loop(self):\n",
    "        \"\"\"Background loop to receive messages.\"\"\"\n",
    "        server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "        server_socket.bind(('0.0.0.0', self.listen_port))\n",
    "        server_socket.listen(10)\n",
    "        server_socket.settimeout(1.0)\n",
    "        \n",
    "        while self._running:\n",
    "            try:\n",
    "                client_socket, addr = server_socket.accept()\n",
    "                data = self._recv_all(client_socket)\n",
    "                if data:\n",
    "                    message = Message.deserialize(data)\n",
    "                    self._inbox.put(message)\n",
    "                client_socket.close()\n",
    "            except socket.timeout:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                if self._running:\n",
    "                    print(f\"Listener error: {e}\")\n",
    "        \n",
    "        server_socket.close()\n",
    "    \n",
    "    def _recv_all(self, sock: socket.socket) -> bytes:\n",
    "        \"\"\"Receive all data from socket.\"\"\"\n",
    "        # First receive the length (4 bytes)\n",
    "        length_data = sock.recv(4)\n",
    "        if not length_data:\n",
    "            return b''\n",
    "        length = struct.unpack('>I', length_data)[0]\n",
    "        \n",
    "        # Then receive the data\n",
    "        data = b''\n",
    "        while len(data) < length:\n",
    "            packet = sock.recv(length - len(data))\n",
    "            if not packet:\n",
    "                break\n",
    "            data += packet\n",
    "        return data\n",
    "    \n",
    "    def send(self, destination: str, message: Message) -> bool:\n",
    "        \"\"\"Send message over network.\"\"\"\n",
    "        if destination not in self.address_book:\n",
    "            print(f\"Unknown destination: {destination}\")\n",
    "            return False\n",
    "        \n",
    "        host, port = self.address_book[destination]\n",
    "        \n",
    "        try:\n",
    "            client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "            client_socket.connect((host, port))\n",
    "            \n",
    "            data = message.serialize()\n",
    "            # Send length first, then data\n",
    "            client_socket.sendall(struct.pack('>I', len(data)) + data)\n",
    "            client_socket.close()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Send error to {destination}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def receive(self, timeout: Optional[float] = None) -> Optional[Message]:\n",
    "        \"\"\"Receive message from inbox.\"\"\"\n",
    "        try:\n",
    "            if timeout is not None:\n",
    "                return self._inbox.get(timeout=timeout)\n",
    "            else:\n",
    "                return self._inbox.get_nowait()\n",
    "        except queue.Empty:\n",
    "            return None\n",
    "    \n",
    "    def broadcast(self, message: Message, destinations: List[str]) -> int:\n",
    "        \"\"\"Broadcast message to all destinations.\"\"\"\n",
    "        success_count = 0\n",
    "        for dest in destinations:\n",
    "            if self.send(dest, message):\n",
    "                success_count += 1\n",
    "        return success_count\n",
    "    \n",
    "    def close(self) -> None:\n",
    "        \"\"\"Stop listener and clean up.\"\"\"\n",
    "        self._running = False\n",
    "        if self._listener_thread:\n",
    "            self._listener_thread.join(timeout=2.0)\n",
    "\n",
    "print(\"NetworkCommunicator implemented (production reference)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEMO: SIMULATED COMMUNICATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMO: SIMULATED COMMUNICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reset queues\n",
    "SimulatedCommunicator.reset()\n",
    "\n",
    "# Create communicators\n",
    "server_comm = SimulatedCommunicator(\"server\")\n",
    "worker1_comm = SimulatedCommunicator(\"worker_1\")\n",
    "worker2_comm = SimulatedCommunicator(\"worker_2\")\n",
    "\n",
    "# Server broadcasts weights to workers\n",
    "print(\"\\n1. Server broadcasting weights to workers...\")\n",
    "weights_msg = Message(\n",
    "    msg_type=MessageType.WEIGHTS,\n",
    "    sender_id=\"server\",\n",
    "    payload={\"layer1\": np.random.randn(10, 5)},\n",
    "    round_num=1\n",
    ")\n",
    "\n",
    "server_comm.broadcast(weights_msg, [\"worker_1\", \"worker_2\"])\n",
    "print(\"   Broadcast complete!\")\n",
    "\n",
    "# Workers receive weights\n",
    "print(\"\\n2. Workers receiving weights...\")\n",
    "msg1 = worker1_comm.receive(timeout=1.0)\n",
    "msg2 = worker2_comm.receive(timeout=1.0)\n",
    "\n",
    "print(f\"   Worker 1 received: {msg1.msg_type.value if msg1 else 'None'}\")\n",
    "print(f\"   Worker 2 received: {msg2.msg_type.value if msg2 else 'None'}\")\n",
    "\n",
    "# Workers send gradients back\n",
    "print(\"\\n3. Workers sending gradients to server...\")\n",
    "for i, (worker_id, comm) in enumerate([(\"worker_1\", worker1_comm), (\"worker_2\", worker2_comm)]):\n",
    "    grad_msg = Message(\n",
    "        msg_type=MessageType.GRADIENTS,\n",
    "        sender_id=worker_id,\n",
    "        payload={\"layer1\": np.random.randn(10, 5) * 0.01},\n",
    "        round_num=1\n",
    "    )\n",
    "    comm.send(\"server\", grad_msg)\n",
    "print(\"   Workers sent gradients!\")\n",
    "\n",
    "# Server receives gradients\n",
    "print(\"\\n4. Server receiving gradients...\")\n",
    "received_grads = []\n",
    "while True:\n",
    "    msg = server_comm.receive(timeout=0.1)\n",
    "    if msg is None:\n",
    "        break\n",
    "    received_grads.append(msg)\n",
    "    print(f\"   Received from {msg.sender_id}: {msg.msg_type.value}\")\n",
    "\n",
    "print(f\"\\nTotal messages received by server: {len(received_grads)}\")\n",
    "\n",
    "# Cleanup\n",
    "server_comm.close()\n",
    "worker1_comm.close()\n",
    "worker2_comm.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Simulated communication working correctly!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight:** The `Communicator` abstraction lets us develop and test with `SimulatedCommunicator`, then deploy with `NetworkCommunicator` without changing any training code!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n<a id='part5'></a>\n# Part 5: Federated Learning Implementation\n---\n\nNow we implement the core **Federated Learning** system with FedAvg algorithm.\n\n## 5.1 FedAvg Algorithm\n\n```\nAlgorithm: Federated Averaging (FedAvg)\n─────────────────────────────────────────\n1. Server initializes global model w₀\n2. For each round t = 1, 2, ..., T:\n   a. Server selects subset of clients C_t\n   b. Server broadcasts w_t to selected clients\n   c. Each client k ∈ C_t:\n      - Trains locally for E epochs on local data\n      - Sends updated weights w_k to server\n   d. Server aggregates: w_{t+1} = Σ(n_k/n) * w_k\n3. Return final model w_T\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# DATA PARTITIONER - SPLIT DATA ACROSS WORKERS\n# ============================================================\n\nclass DataPartitioner:\n    \"\"\"\n    Partition data across workers for distributed training.\n    \n    Supports:\n    - IID (Independent & Identically Distributed): Random uniform split\n    - Non-IID: Heterogeneous distribution (e.g., by label)\n    \"\"\"\n    \n    @staticmethod\n    def partition_iid(X: np.ndarray, y: np.ndarray, \n                     n_partitions: int) -> List[Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"\n        Create IID partitions (random split).\n        \n        Each partition has approximately equal size and\n        similar class distribution.\n        \n        Args:\n            X: Features array\n            y: Labels array\n            n_partitions: Number of partitions\n            \n        Returns:\n            List of (X_partition, y_partition) tuples\n        \"\"\"\n        n_samples = len(X)\n        indices = np.random.permutation(n_samples)\n        \n        # Split indices evenly\n        partition_size = n_samples // n_partitions\n        partitions = []\n        \n        for i in range(n_partitions):\n            start_idx = i * partition_size\n            if i == n_partitions - 1:  # Last partition gets remainder\n                end_idx = n_samples\n            else:\n                end_idx = start_idx + partition_size\n            \n            partition_indices = indices[start_idx:end_idx]\n            partitions.append((X[partition_indices], y[partition_indices]))\n        \n        return partitions\n    \n    @staticmethod\n    def partition_non_iid(X: np.ndarray, y: np.ndarray, \n                          n_partitions: int, \n                          classes_per_partition: int = 2) -> List[Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"\n        Create Non-IID partitions (heterogeneous distribution).\n        \n        Each partition primarily contains data from a subset of classes,\n        simulating real-world federated scenarios.\n        \n        Args:\n            X: Features array\n            y: Labels array\n            n_partitions: Number of partitions\n            classes_per_partition: How many classes each partition gets\n            \n        Returns:\n            List of (X_partition, y_partition) tuples\n        \"\"\"\n        unique_classes = np.unique(y)\n        n_classes = len(unique_classes)\n        \n        # Group indices by class\n        class_indices = {c: np.where(y == c)[0] for c in unique_classes}\n        \n        partitions = []\n        \n        for i in range(n_partitions):\n            # Assign classes to this partition (circular assignment)\n            assigned_classes = []\n            for j in range(classes_per_partition):\n                class_idx = (i * classes_per_partition + j) % n_classes\n                assigned_classes.append(unique_classes[class_idx])\n            \n            # Collect samples from assigned classes\n            partition_indices = []\n            for c in assigned_classes:\n                # Get portion of this class's data\n                c_indices = class_indices[c]\n                n_take = len(c_indices) // n_partitions + 1\n                start = (i * n_take) % len(c_indices)\n                end = min(start + n_take, len(c_indices))\n                partition_indices.extend(c_indices[start:end])\n            \n            # Add some samples from other classes (to make it less extreme)\n            other_classes = [c for c in unique_classes if c not in assigned_classes]\n            for c in other_classes:\n                c_indices = class_indices[c]\n                n_take = len(c_indices) // (n_partitions * 5)  # Much fewer\n                if n_take > 0:\n                    selected = np.random.choice(c_indices, size=n_take, replace=False)\n                    partition_indices.extend(selected)\n            \n            partition_indices = np.array(partition_indices)\n            np.random.shuffle(partition_indices)\n            \n            partitions.append((X[partition_indices], y[partition_indices]))\n        \n        return partitions\n    \n    @staticmethod\n    def visualize_partitions(partitions: List[Tuple[np.ndarray, np.ndarray]], \n                            title: str = \"Data Distribution\"):\n        \"\"\"Visualize class distribution across partitions.\"\"\"\n        n_partitions = len(partitions)\n        \n        # Get all unique classes\n        all_classes = set()\n        for _, y in partitions:\n            all_classes.update(np.unique(y))\n        all_classes = sorted(all_classes)\n        \n        # Count class frequencies per partition\n        distributions = np.zeros((n_partitions, len(all_classes)))\n        for i, (_, y) in enumerate(partitions):\n            for j, c in enumerate(all_classes):\n                distributions[i, j] = np.sum(y == c)\n        \n        # Plot\n        fig, ax = plt.subplots(figsize=(12, 5))\n        x = np.arange(n_partitions)\n        width = 0.8 / len(all_classes)\n        \n        colors = plt.cm.tab10(np.linspace(0, 1, len(all_classes)))\n        \n        for j, (c, color) in enumerate(zip(all_classes, colors)):\n            offset = (j - len(all_classes)/2 + 0.5) * width\n            ax.bar(x + offset, distributions[:, j], width, label=f'Class {c}', color=color)\n        \n        ax.set_xlabel('Worker/Partition')\n        ax.set_ylabel('Number of Samples')\n        ax.set_title(title)\n        ax.set_xticks(x)\n        ax.set_xticklabels([f'W{i+1}' for i in range(n_partitions)])\n        ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', ncol=1)\n        plt.tight_layout()\n        plt.show()\n        \n        return distributions\n\nprint(\"DataPartitioner implemented!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# DEMO: IID vs NON-IID DATA PARTITIONING\n# ============================================================\n\nprint(\"=\" * 60)\nprint(\"DEMO: IID vs NON-IID DATA DISTRIBUTION\")\nprint(\"=\" * 60)\n\nn_workers = 5\n\n# IID partitioning\nprint(\"\\n--- IID Partitioning ---\")\niid_partitions = DataPartitioner.partition_iid(X_train, y_train, n_workers)\nfor i, (X_part, y_part) in enumerate(iid_partitions):\n    print(f\"Worker {i+1}: {len(X_part)} samples, classes: {sorted(np.unique(y_part))}\")\n\nDataPartitioner.visualize_partitions(iid_partitions, \"IID Data Distribution Across Workers\")\n\n# Non-IID partitioning\nprint(\"\\n--- Non-IID Partitioning ---\")\nnon_iid_partitions = DataPartitioner.partition_non_iid(X_train, y_train, n_workers, classes_per_partition=2)\nfor i, (X_part, y_part) in enumerate(non_iid_partitions):\n    unique, counts = np.unique(y_part, return_counts=True)\n    dominant = unique[np.argmax(counts)]\n    print(f\"Worker {i+1}: {len(X_part)} samples, dominant class: {dominant}\")\n\nDataPartitioner.visualize_partitions(non_iid_partitions, \"Non-IID Data Distribution Across Workers\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5.2 Gradient Aggregation Strategies\n\nDifferent aggregation methods for combining worker updates:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# GRADIENT/WEIGHT AGGREGATOR\n# ============================================================\n\nclass GradientAggregator:\n    \"\"\"\n    Aggregates gradients or model weights from multiple workers.\n    \n    Supports multiple aggregation strategies:\n    - Simple Average: Equal weight to all workers\n    - FedAvg: Weighted by number of samples\n    - FedProx: FedAvg with proximal regularization\n    \"\"\"\n    \n    @staticmethod\n    def simple_average(weight_dicts: List[Dict[str, np.ndarray]]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Simple unweighted average of weights.\n        \n        Args:\n            weight_dicts: List of weight dictionaries from workers\n            \n        Returns:\n            Averaged weights dictionary\n        \"\"\"\n        if not weight_dicts:\n            return {}\n        \n        # Get all layer names\n        layer_names = weight_dicts[0].keys()\n        n_workers = len(weight_dicts)\n        \n        # Average each layer\n        aggregated = {}\n        for name in layer_names:\n            stacked = np.stack([w[name] for w in weight_dicts])\n            aggregated[name] = np.mean(stacked, axis=0)\n        \n        return aggregated\n    \n    @staticmethod\n    def fedavg(weight_dicts: List[Dict[str, np.ndarray]], \n               sample_counts: List[int]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Federated Averaging - weighted by sample count.\n        \n        w_global = Σ (n_k / n_total) * w_k\n        \n        Args:\n            weight_dicts: List of weight dictionaries from workers\n            sample_counts: Number of samples each worker trained on\n            \n        Returns:\n            Weighted average of weights\n        \"\"\"\n        if not weight_dicts:\n            return {}\n        \n        layer_names = weight_dicts[0].keys()\n        total_samples = sum(sample_counts)\n        \n        # Weighted average\n        aggregated = {}\n        for name in layer_names:\n            weighted_sum = np.zeros_like(weight_dicts[0][name])\n            for weights, n_samples in zip(weight_dicts, sample_counts):\n                weight_factor = n_samples / total_samples\n                weighted_sum += weight_factor * weights[name]\n            aggregated[name] = weighted_sum\n        \n        return aggregated\n    \n    @staticmethod\n    def fedprox_loss_term(local_weights: Dict[str, np.ndarray],\n                          global_weights: Dict[str, np.ndarray],\n                          mu: float = 0.01) -> float:\n        \"\"\"\n        Compute FedProx proximal term.\n        \n        Loss_prox = (μ/2) * ||w_local - w_global||²\n        \n        Args:\n            local_weights: Current local model weights\n            global_weights: Global model weights from server\n            mu: Proximal coefficient\n            \n        Returns:\n            Proximal loss term\n        \"\"\"\n        prox_term = 0.0\n        for name in local_weights.keys():\n            if name in global_weights:\n                diff = local_weights[name] - global_weights[name]\n                prox_term += np.sum(diff ** 2)\n        \n        return (mu / 2) * prox_term\n    \n    @staticmethod\n    def compute_update(old_weights: Dict[str, np.ndarray],\n                       new_weights: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Compute the weight update (delta) between old and new weights.\n        \n        Args:\n            old_weights: Weights before training\n            new_weights: Weights after training\n            \n        Returns:\n            Weight delta dictionary\n        \"\"\"\n        delta = {}\n        for name in new_weights.keys():\n            if name in old_weights:\n                delta[name] = new_weights[name] - old_weights[name]\n        return delta\n    \n    @staticmethod\n    def apply_update(weights: Dict[str, np.ndarray],\n                     delta: Dict[str, np.ndarray],\n                     learning_rate: float = 1.0) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Apply aggregated update to weights.\n        \n        Args:\n            weights: Current weights\n            delta: Weight update to apply\n            learning_rate: Scale factor for update\n            \n        Returns:\n            Updated weights\n        \"\"\"\n        updated = {}\n        for name in weights.keys():\n            if name in delta:\n                updated[name] = weights[name] + learning_rate * delta[name]\n            else:\n                updated[name] = weights[name]\n        return updated\n\nprint(\"GradientAggregator implemented!\")\nprint(\"Available methods: simple_average, fedavg, fedprox_loss_term, compute_update, apply_update\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5.3 Federated Worker\n\nEach worker trains locally on its own data and sends updates to the server.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# FEDERATED WORKER\n# ============================================================\n\nclass FederatedWorker:\n    \"\"\"\n    A worker node in federated learning.\n    \n    Responsibilities:\n    - Hold local data (never shared)\n    - Train local model for specified epochs\n    - Send model updates to server\n    - Receive global model from server\n    \"\"\"\n    \n    def __init__(self, \n                 worker_id: str,\n                 model_wrapper: ModelWrapper,\n                 X_local: np.ndarray,\n                 y_local: np.ndarray,\n                 communicator: Optional[Communicator] = None,\n                 config: Optional[DistributedMLConfig] = None):\n        \"\"\"\n        Initialize federated worker.\n        \n        Args:\n            worker_id: Unique identifier for this worker\n            model_wrapper: Wrapped model for training\n            X_local: Local training features\n            y_local: Local training labels\n            communicator: Communication interface\n            config: Training configuration\n        \"\"\"\n        self.worker_id = worker_id\n        self.model = model_wrapper\n        self.X_local = X_local\n        self.y_local = y_local\n        self.communicator = communicator\n        self.config = config or DistributedMLConfig()\n        \n        # Training state\n        self.n_samples = len(X_local)\n        self.training_history: List[Dict[str, float]] = []\n        self.current_round = 0\n        self.global_weights: Optional[Dict[str, np.ndarray]] = None\n    \n    def receive_global_model(self, weights: Dict[str, np.ndarray]) -> None:\n        \"\"\"\n        Receive and apply global model weights from server.\n        \n        Args:\n            weights: Global model weights\n        \"\"\"\n        self.model.set_weights(weights)\n        self.global_weights = copy.deepcopy(weights)\n    \n    def train_local(self, n_epochs: Optional[int] = None) -> Dict[str, Any]:\n        \"\"\"\n        Train on local data for specified epochs.\n        \n        Args:\n            n_epochs: Number of epochs (uses config if not specified)\n            \n        Returns:\n            Training results including loss, weights, and sample count\n        \"\"\"\n        n_epochs = n_epochs or self.config.local_epochs\n        \n        epoch_losses = []\n        for epoch in range(n_epochs):\n            loss = self.model.train_epoch(\n                self.X_local, \n                self.y_local, \n                self.config.batch_size\n            )\n            epoch_losses.append(loss)\n        \n        # Get updated weights\n        new_weights = self.model.get_weights()\n        \n        # Compute weight update if using delta mode\n        weight_delta = None\n        if self.global_weights is not None:\n            weight_delta = GradientAggregator.compute_update(\n                self.global_weights, new_weights\n            )\n        \n        # Record history\n        result = {\n            'worker_id': self.worker_id,\n            'round': self.current_round,\n            'n_epochs': n_epochs,\n            'n_samples': self.n_samples,\n            'avg_loss': np.mean(epoch_losses),\n            'final_loss': epoch_losses[-1],\n            'weights': new_weights,\n            'weight_delta': weight_delta\n        }\n        \n        self.training_history.append({\n            'round': self.current_round,\n            'loss': result['avg_loss']\n        })\n        \n        return result\n    \n    def evaluate_local(self) -> Dict[str, float]:\n        \"\"\"Evaluate model on local data.\"\"\"\n        return self.model.evaluate(self.X_local, self.y_local)\n    \n    def send_update(self, server_id: str = \"server\") -> bool:\n        \"\"\"\n        Send weight update to server.\n        \n        Args:\n            server_id: ID of the server to send to\n            \n        Returns:\n            True if sent successfully\n        \"\"\"\n        if self.communicator is None:\n            return False\n        \n        result = self.train_local()\n        \n        message = Message(\n            msg_type=MessageType.UPDATE,\n            sender_id=self.worker_id,\n            payload={\n                'weights': result['weights'],\n                'n_samples': result['n_samples'],\n                'loss': result['avg_loss']\n            },\n            round_num=self.current_round\n        )\n        \n        return self.communicator.send(server_id, message)\n    \n    def __repr__(self) -> str:\n        return f\"FederatedWorker(id={self.worker_id}, samples={self.n_samples})\"\n\nprint(\"FederatedWorker implemented!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5.4 Federated Server\n\nThe server coordinates training, aggregates updates, and maintains the global model.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# FEDERATED SERVER\n# ============================================================\n\nclass FederatedServer:\n    \"\"\"\n    Central server for federated learning coordination.\n    \n    Responsibilities:\n    - Maintain global model\n    - Aggregate worker updates using FedAvg/FedProx\n    - Broadcast global model to workers\n    - Track training progress\n    \"\"\"\n    \n    def __init__(self,\n                 global_model: ModelWrapper,\n                 communicator: Optional[Communicator] = None,\n                 config: Optional[DistributedMLConfig] = None):\n        \"\"\"\n        Initialize federated server.\n        \n        Args:\n            global_model: Initial global model\n            communicator: Communication interface\n            config: Training configuration\n        \"\"\"\n        self.global_model = global_model\n        self.communicator = communicator\n        self.config = config or DistributedMLConfig()\n        \n        # Get initial weights\n        self.global_weights = global_model.get_weights()\n        \n        # Tracking\n        self.current_round = 0\n        self.round_history: List[Dict[str, Any]] = []\n        self.worker_contributions: Dict[str, int] = defaultdict(int)\n    \n    def select_workers(self, worker_ids: List[str]) -> List[str]:\n        \"\"\"\n        Select subset of workers for this round.\n        \n        Args:\n            worker_ids: All available worker IDs\n            \n        Returns:\n            Selected worker IDs for this round\n        \"\"\"\n        n_select = max(1, int(len(worker_ids) * self.config.client_fraction))\n        selected = np.random.choice(worker_ids, size=n_select, replace=False)\n        return list(selected)\n    \n    def aggregate_updates(self, \n                          worker_updates: List[Dict[str, Any]],\n                          method: AggregationMethod = None) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Aggregate worker updates into new global model.\n        \n        Args:\n            worker_updates: List of updates from workers\n            method: Aggregation method (default from config)\n            \n        Returns:\n            New global weights\n        \"\"\"\n        method = method or self.config.aggregation\n        \n        if not worker_updates:\n            return self.global_weights\n        \n        # Extract weights and sample counts\n        weight_dicts = [u['weights'] for u in worker_updates]\n        sample_counts = [u['n_samples'] for u in worker_updates]\n        \n        # Record contributions\n        for update in worker_updates:\n            self.worker_contributions[update['worker_id']] += update['n_samples']\n        \n        # Aggregate based on method\n        if method == AggregationMethod.FEDAVG or method == AggregationMethod.FEDPROX:\n            new_weights = GradientAggregator.fedavg(weight_dicts, sample_counts)\n        else:\n            new_weights = GradientAggregator.simple_average(weight_dicts)\n        \n        return new_weights\n    \n    def update_global_model(self, new_weights: Dict[str, np.ndarray]) -> None:\n        \"\"\"\n        Update global model with new weights.\n        \n        Args:\n            new_weights: New global weights\n        \"\"\"\n        self.global_weights = new_weights\n        self.global_model.set_weights(new_weights)\n    \n    def broadcast_model(self, worker_ids: List[str]) -> int:\n        \"\"\"\n        Broadcast current global model to workers.\n        \n        Args:\n            worker_ids: Workers to broadcast to\n            \n        Returns:\n            Number of successful broadcasts\n        \"\"\"\n        if self.communicator is None:\n            return 0\n        \n        message = Message(\n            msg_type=MessageType.WEIGHTS,\n            sender_id=\"server\",\n            payload=self.global_weights,\n            round_num=self.current_round\n        )\n        \n        return self.communicator.broadcast(message, worker_ids)\n    \n    def record_round(self, \n                     worker_updates: List[Dict[str, Any]],\n                     test_metrics: Optional[Dict[str, float]] = None) -> None:\n        \"\"\"\n        Record round statistics.\n        \n        Args:\n            worker_updates: Updates from this round\n            test_metrics: Optional test set evaluation\n        \"\"\"\n        avg_loss = np.mean([u['loss'] for u in worker_updates]) if worker_updates else 0\n        \n        record = {\n            'round': self.current_round,\n            'n_workers': len(worker_updates),\n            'total_samples': sum(u['n_samples'] for u in worker_updates),\n            'avg_worker_loss': avg_loss,\n            'test_metrics': test_metrics\n        }\n        \n        self.round_history.append(record)\n    \n    def get_training_summary(self) -> pd.DataFrame:\n        \"\"\"Get training history as DataFrame.\"\"\"\n        return pd.DataFrame(self.round_history)\n    \n    def evaluate_global_model(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:\n        \"\"\"Evaluate global model on test set.\"\"\"\n        return self.global_model.evaluate(X_test, y_test)\n\nprint(\"FederatedServer implemented!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5.5 Federated Learning Orchestrator\n\nThe main orchestrator that ties everything together and runs the federated learning loop.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# FEDERATED LEARNING ORCHESTRATOR\n# ============================================================\n\nclass FederatedLearning:\n    \"\"\"\n    Main orchestrator for federated learning training.\n    \n    Coordinates the complete FL workflow:\n    1. Initialize server and workers\n    2. Partition data\n    3. Run training rounds\n    4. Aggregate and evaluate\n    \"\"\"\n    \n    def __init__(self, config: DistributedMLConfig):\n        \"\"\"\n        Initialize federated learning system.\n        \n        Args:\n            config: System configuration\n        \"\"\"\n        self.config = config\n        self.server: Optional[FederatedServer] = None\n        self.workers: List[FederatedWorker] = []\n        self.training_complete = False\n        self.history: Dict[str, List] = {\n            'round': [],\n            'train_loss': [],\n            'test_loss': [],\n            'test_accuracy': []\n        }\n    \n    def setup(self,\n              X_train: np.ndarray,\n              y_train: np.ndarray,\n              X_test: np.ndarray,\n              y_test: np.ndarray,\n              model_fn: Callable[[], ModelWrapper]) -> None:\n        \"\"\"\n        Setup federated learning system.\n        \n        Args:\n            X_train: Training features\n            y_train: Training labels\n            X_test: Test features\n            y_test: Test labels\n            model_fn: Function that creates a ModelWrapper\n        \"\"\"\n        print(\"=\" * 60)\n        print(\"SETTING UP FEDERATED LEARNING\")\n        print(\"=\" * 60)\n        \n        self.X_test = X_test\n        self.y_test = y_test\n        \n        # Partition data\n        print(f\"\\nPartitioning data for {self.config.n_workers} workers...\")\n        if self.config.iid_data:\n            partitions = DataPartitioner.partition_iid(X_train, y_train, self.config.n_workers)\n        else:\n            partitions = DataPartitioner.partition_non_iid(X_train, y_train, self.config.n_workers)\n        \n        # Create server with global model\n        print(\"Creating server with global model...\")\n        global_model = model_fn()\n        self.server = FederatedServer(global_model, config=self.config)\n        \n        # Create workers\n        print(\"Creating workers...\")\n        self.workers = []\n        for i, (X_part, y_part) in enumerate(partitions):\n            worker_id = f\"worker_{i+1}\"\n            worker_model = model_fn()\n            worker = FederatedWorker(\n                worker_id=worker_id,\n                model_wrapper=worker_model,\n                X_local=X_part,\n                y_local=y_part,\n                config=self.config\n            )\n            self.workers.append(worker)\n            print(f\"  {worker_id}: {len(X_part)} samples\")\n        \n        print(\"\\nSetup complete!\")\n    \n    def run_round(self) -> Dict[str, Any]:\n        \"\"\"\n        Run one round of federated learning.\n        \n        Returns:\n            Round statistics\n        \"\"\"\n        round_num = self.server.current_round\n        \n        # Select workers for this round\n        worker_ids = [w.worker_id for w in self.workers]\n        selected_ids = self.server.select_workers(worker_ids)\n        selected_workers = [w for w in self.workers if w.worker_id in selected_ids]\n        \n        # Broadcast global model to workers\n        for worker in selected_workers:\n            worker.receive_global_model(self.server.global_weights)\n            worker.current_round = round_num\n        \n        # Workers train locally\n        worker_updates = []\n        for worker in selected_workers:\n            result = worker.train_local()\n            worker_updates.append({\n                'worker_id': worker.worker_id,\n                'weights': result['weights'],\n                'n_samples': result['n_samples'],\n                'loss': result['avg_loss']\n            })\n        \n        # Server aggregates updates\n        new_weights = self.server.aggregate_updates(worker_updates)\n        self.server.update_global_model(new_weights)\n        \n        # Evaluate on test set\n        test_metrics = self.server.evaluate_global_model(self.X_test, self.y_test)\n        \n        # Record round\n        self.server.record_round(worker_updates, test_metrics)\n        self.server.current_round += 1\n        \n        # Update history\n        avg_train_loss = np.mean([u['loss'] for u in worker_updates])\n        self.history['round'].append(round_num)\n        self.history['train_loss'].append(avg_train_loss)\n        self.history['test_loss'].append(test_metrics['loss'])\n        self.history['test_accuracy'].append(test_metrics['accuracy'])\n        \n        return {\n            'round': round_num,\n            'n_workers': len(selected_workers),\n            'train_loss': avg_train_loss,\n            'test_loss': test_metrics['loss'],\n            'test_accuracy': test_metrics['accuracy']\n        }\n    \n    def train(self, n_rounds: Optional[int] = None, verbose: bool = True) -> pd.DataFrame:\n        \"\"\"\n        Run complete federated learning training.\n        \n        Args:\n            n_rounds: Number of rounds (default from config)\n            verbose: Print progress\n            \n        Returns:\n            Training history DataFrame\n        \"\"\"\n        n_rounds = n_rounds or self.config.global_rounds\n        \n        if verbose:\n            print(\"\\n\" + \"=\" * 70)\n            print(\"FEDERATED LEARNING TRAINING\")\n            print(\"=\" * 70)\n            print(f\"{'Round':<8}{'Workers':<10}{'Train Loss':<15}{'Test Loss':<15}{'Test Acc':<12}\")\n            print(\"-\" * 70)\n        \n        for round_num in range(n_rounds):\n            stats = self.run_round()\n            \n            if verbose:\n                print(f\"{stats['round']+1:<8}{stats['n_workers']:<10}\"\n                      f\"{stats['train_loss']:<15.4f}{stats['test_loss']:<15.4f}\"\n                      f\"{stats['test_accuracy']*100:<12.2f}%\")\n        \n        self.training_complete = True\n        \n        if verbose:\n            print(\"-\" * 70)\n            print(f\"Training complete! Final accuracy: {stats['test_accuracy']*100:.2f}%\")\n        \n        return pd.DataFrame(self.history)\n    \n    def plot_training_history(self) -> None:\n        \"\"\"Plot training curves.\"\"\"\n        if not self.history['round']:\n            print(\"No training history to plot!\")\n            return\n        \n        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n        \n        # Loss plot\n        ax1 = axes[0]\n        ax1.plot(self.history['round'], self.history['train_loss'], \n                'b-o', label='Train Loss', markersize=4)\n        ax1.plot(self.history['round'], self.history['test_loss'],\n                'r-s', label='Test Loss', markersize=4)\n        ax1.set_xlabel('Round')\n        ax1.set_ylabel('Loss')\n        ax1.set_title('Federated Learning: Loss vs Rounds')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # Accuracy plot\n        ax2 = axes[1]\n        ax2.plot(self.history['round'], \n                [acc * 100 for acc in self.history['test_accuracy']],\n                'g-^', markersize=4)\n        ax2.set_xlabel('Round')\n        ax2.set_ylabel('Accuracy (%)')\n        ax2.set_title('Federated Learning: Test Accuracy vs Rounds')\n        ax2.grid(True, alpha=0.3)\n        ax2.set_ylim([0, 100])\n        \n        plt.tight_layout()\n        plt.show()\n\nprint(\"FederatedLearning orchestrator implemented!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5.6 Demo: Federated Learning with FedAvg\n\nNow let's run a complete federated learning training session!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# DEMO: COMPLETE FEDERATED LEARNING WITH FEDAVG\n# ============================================================\n\n# Create configuration for federated learning\nfl_config = DistributedMLConfig(\n    mode=ExecutionMode.SIMULATION,\n    strategy=TrainingStrategy.FEDERATED,\n    aggregation=AggregationMethod.FEDAVG,\n    framework=Framework.PYTORCH,\n    n_workers=5,\n    local_epochs=3,\n    global_rounds=10,\n    batch_size=32,\n    learning_rate=0.01,\n    client_fraction=1.0,  # Use all clients each round\n    iid_data=True  # IID data distribution\n)\n\n# Model factory function\ndef create_fl_model():\n    return create_model_wrapper(\n        Framework.PYTORCH, \n        input_dim=X_train.shape[1], \n        n_classes=10,\n        learning_rate=fl_config.learning_rate\n    )\n\n# Initialize federated learning system\nfl_system = FederatedLearning(fl_config)\n\n# Setup with data\nfl_system.setup(X_train, y_train, X_test, y_test, create_fl_model)\n\n# Train!\nhistory_df = fl_system.train(n_rounds=10)\n\n# Plot results\nfl_system.plot_training_history()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# COMPARISON: IID vs NON-IID FEDERATED LEARNING\n# ============================================================\n\nprint(\"=\" * 60)\nprint(\"COMPARISON: IID vs NON-IID DATA DISTRIBUTION\")\nprint(\"=\" * 60)\n\n# Non-IID configuration\nnon_iid_config = DistributedMLConfig(\n    mode=ExecutionMode.SIMULATION,\n    strategy=TrainingStrategy.FEDERATED,\n    aggregation=AggregationMethod.FEDAVG,\n    framework=Framework.PYTORCH,\n    n_workers=5,\n    local_epochs=3,\n    global_rounds=10,\n    batch_size=32,\n    learning_rate=0.01,\n    client_fraction=1.0,\n    iid_data=False  # Non-IID data distribution\n)\n\n# Train with Non-IID data\nfl_non_iid = FederatedLearning(non_iid_config)\nfl_non_iid.setup(X_train, y_train, X_test, y_test, create_fl_model)\nhistory_non_iid = fl_non_iid.train(n_rounds=10, verbose=True)\n\n# Compare IID vs Non-IID\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Accuracy comparison\nax1 = axes[0]\nax1.plot(fl_system.history['round'], \n         [acc * 100 for acc in fl_system.history['test_accuracy']],\n         'b-o', label='IID', markersize=4)\nax1.plot(fl_non_iid.history['round'],\n         [acc * 100 for acc in fl_non_iid.history['test_accuracy']],\n         'r-s', label='Non-IID', markersize=4)\nax1.set_xlabel('Round')\nax1.set_ylabel('Test Accuracy (%)')\nax1.set_title('IID vs Non-IID: Test Accuracy')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Loss comparison\nax2 = axes[1]\nax2.plot(fl_system.history['round'], fl_system.history['test_loss'],\n         'b-o', label='IID', markersize=4)\nax2.plot(fl_non_iid.history['round'], fl_non_iid.history['test_loss'],\n         'r-s', label='Non-IID', markersize=4)\nax2.set_xlabel('Round')\nax2.set_ylabel('Test Loss')\nax2.set_title('IID vs Non-IID: Test Loss')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Summary\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SUMMARY\")\nprint(\"=\" * 60)\nprint(f\"IID Final Accuracy:     {fl_system.history['test_accuracy'][-1]*100:.2f}%\")\nprint(f\"Non-IID Final Accuracy: {fl_non_iid.history['test_accuracy'][-1]*100:.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Key Insight:** Non-IID data is a fundamental challenge in federated learning. When workers have heterogeneous data distributions, local models can diverge significantly, making aggregation less effective. This is why algorithms like **FedProx** add a proximal term to keep local models closer to the global model.\n\n---\n<a id='part6'></a>\n# Part 6: Data Parallelism Implementation\n---\n\nNow we implement **Data Parallelism** - the approach used in traditional distributed training where data is split but the model is synchronized.\n\n## 6.1 Synchronous SGD\n\nIn Synchronous SGD, all workers compute gradients, then we average them before updating the model.\n\n```\nAlgorithm: Synchronous Stochastic Gradient Descent\n───────────────────────────────────────────────────\n1. Split data equally across K workers\n2. For each iteration:\n   a. Each worker k computes gradient g_k on its batch\n   b. Aggregate: g = (1/K) * Σ g_k\n   c. Update model: w = w - η * g\n3. Repeat until convergence\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# DATA PARALLEL WORKER\n# ============================================================\n\nclass DataParallelWorker:\n    \"\"\"\n    Worker for data parallel training.\n    \n    Unlike federated workers, data parallel workers:\n    - Compute gradients on batches\n    - Send gradients (not weights) to parameter server\n    - Receive updated model synchronously\n    \"\"\"\n    \n    def __init__(self,\n                 worker_id: str,\n                 model_wrapper: ModelWrapper,\n                 config: Optional[DistributedMLConfig] = None):\n        \"\"\"\n        Initialize data parallel worker.\n        \n        Args:\n            worker_id: Unique identifier\n            model_wrapper: Wrapped model\n            config: Training configuration\n        \"\"\"\n        self.worker_id = worker_id\n        self.model = model_wrapper\n        self.config = config or DistributedMLConfig()\n        self.gradient_history: List[Dict[str, np.ndarray]] = []\n    \n    def compute_gradients(self, X_batch: np.ndarray, y_batch: np.ndarray) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Compute gradients for a batch without updating model.\n        \n        Args:\n            X_batch: Batch features\n            y_batch: Batch labels\n            \n        Returns:\n            Gradients dictionary\n        \"\"\"\n        gradients = self.model.get_gradients(X_batch, y_batch)\n        self.gradient_history.append(gradients)\n        return gradients\n    \n    def receive_weights(self, weights: Dict[str, np.ndarray]) -> None:\n        \"\"\"Update local model with new weights.\"\"\"\n        self.model.set_weights(weights)\n    \n    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:\n        \"\"\"Evaluate model on data.\"\"\"\n        return self.model.evaluate(X, y)\n\nprint(\"DataParallelWorker implemented!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6.2 Ring AllReduce\n\nRing AllReduce is a **bandwidth-optimal** algorithm for gradient aggregation. Instead of sending all data to one server, workers pass partial results around a ring.\n\n```\nRing AllReduce Steps:\n─────────────────────\n1. Scatter-Reduce: Each worker sends 1/N of gradients to next worker\n   - After N-1 steps, each worker has fully reduced 1/N of gradients\n   \n2. AllGather: Workers share their reduced chunks around ring\n   - After N-1 more steps, all workers have full reduced gradients\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# RING ALLREDUCE IMPLEMENTATION\n# ============================================================\n\nclass RingAllReduce:\n    \"\"\"\n    Simulated Ring AllReduce for gradient averaging.\n    \n    This simulates the ring allreduce algorithm used in\n    distributed training frameworks like Horovod.\n    \n    In a real implementation, this would use MPI or NCCL.\n    \"\"\"\n    \n    def __init__(self, n_workers: int):\n        \"\"\"\n        Initialize ring allreduce.\n        \n        Args:\n            n_workers: Number of workers in the ring\n        \"\"\"\n        self.n_workers = n_workers\n    \n    def allreduce(self, gradient_dicts: List[Dict[str, np.ndarray]]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Perform allreduce to average gradients across all workers.\n        \n        In simulation, this is equivalent to simple averaging.\n        In production, this would use the actual ring algorithm.\n        \n        Args:\n            gradient_dicts: Gradients from each worker\n            \n        Returns:\n            Averaged gradients\n        \"\"\"\n        if not gradient_dicts:\n            return {}\n        \n        # Get all layer names\n        layer_names = gradient_dicts[0].keys()\n        \n        # Average each layer (simulating the allreduce result)\n        averaged = {}\n        for name in layer_names:\n            stacked = np.stack([g[name] for g in gradient_dicts])\n            averaged[name] = np.mean(stacked, axis=0)\n        \n        return averaged\n    \n    def ring_reduce_simulation(self, \n                               gradient_dicts: List[Dict[str, np.ndarray]], \n                               verbose: bool = False) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Detailed simulation of ring allreduce steps.\n        \n        This shows how data flows in the actual algorithm.\n        \n        Args:\n            gradient_dicts: Gradients from each worker\n            verbose: Print progress\n            \n        Returns:\n            Reduced gradients\n        \"\"\"\n        if not gradient_dicts:\n            return {}\n        \n        n = self.n_workers\n        layer_names = list(gradient_dicts[0].keys())\n        \n        # For each layer, we'll simulate the ring\n        result = {}\n        \n        for layer_name in layer_names:\n            # Get gradients for this layer from all workers\n            layer_grads = [g[layer_name] for g in gradient_dicts]\n            \n            # Split each worker's gradient into N chunks\n            chunk_size = layer_grads[0].size // n\n            \n            # Phase 1: Scatter-Reduce\n            # Each worker ends up with one fully-reduced chunk\n            reduced_chunks = []\n            for chunk_idx in range(n):\n                # Simulate reduction of this chunk across workers\n                chunk_sum = np.zeros(chunk_size)\n                for worker_idx in range(n):\n                    flat_grad = layer_grads[worker_idx].flatten()\n                    start = chunk_idx * chunk_size\n                    end = start + chunk_size if chunk_idx < n - 1 else len(flat_grad)\n                    chunk = flat_grad[start:end]\n                    if len(chunk) < chunk_size:\n                        chunk = np.pad(chunk, (0, chunk_size - len(chunk)))\n                    chunk_sum += chunk\n                reduced_chunks.append(chunk_sum / n)\n            \n            # Phase 2: AllGather\n            # Combine all reduced chunks\n            full_reduced = np.concatenate(reduced_chunks)[:layer_grads[0].size]\n            result[layer_name] = full_reduced.reshape(layer_grads[0].shape)\n            \n            if verbose:\n                print(f\"  Layer {layer_name}: ring reduce complete\")\n        \n        return result\n\nprint(\"RingAllReduce implemented!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6.3 Data Parallel Trainer\n\nThe orchestrator for data parallel training with synchronous gradient updates.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# DATA PARALLEL TRAINER\n# ============================================================\n\nclass DataParallelTrainer:\n    \"\"\"\n    Orchestrator for data parallel training.\n    \n    Implements synchronous SGD where:\n    1. Data is split across workers\n    2. Each worker computes gradients on their batch\n    3. Gradients are averaged (via Ring AllReduce or parameter server)\n    4. Model is updated synchronously\n    \"\"\"\n    \n    def __init__(self, config: DistributedMLConfig):\n        \"\"\"\n        Initialize data parallel trainer.\n        \n        Args:\n            config: Training configuration\n        \"\"\"\n        self.config = config\n        self.global_model: Optional[ModelWrapper] = None\n        self.workers: List[DataParallelWorker] = []\n        self.ring_allreduce: Optional[RingAllReduce] = None\n        self.history: Dict[str, List] = {\n            'iteration': [],\n            'train_loss': [],\n            'test_loss': [],\n            'test_accuracy': []\n        }\n    \n    def setup(self,\n              X_train: np.ndarray,\n              y_train: np.ndarray,\n              X_test: np.ndarray,\n              y_test: np.ndarray,\n              model_fn: Callable[[], ModelWrapper]) -> None:\n        \"\"\"\n        Setup data parallel training.\n        \n        Args:\n            X_train: Training features\n            y_train: Training labels\n            X_test: Test features\n            y_test: Test labels\n            model_fn: Function that creates a ModelWrapper\n        \"\"\"\n        print(\"=\" * 60)\n        print(\"SETTING UP DATA PARALLEL TRAINING\")\n        print(\"=\" * 60)\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n        \n        # Create global model\n        print(\"\\nCreating global model...\")\n        self.global_model = model_fn()\n        \n        # Create workers (each with same model initially)\n        print(f\"Creating {self.config.n_workers} workers...\")\n        self.workers = []\n        for i in range(self.config.n_workers):\n            worker_id = f\"dp_worker_{i+1}\"\n            worker_model = model_fn()\n            # Initialize with global weights\n            worker_model.set_weights(self.global_model.get_weights())\n            worker = DataParallelWorker(worker_id, worker_model, self.config)\n            self.workers.append(worker)\n        \n        # Setup Ring AllReduce\n        if self.config.aggregation == AggregationMethod.RING_ALLREDUCE:\n            self.ring_allreduce = RingAllReduce(self.config.n_workers)\n            print(\"Using Ring AllReduce for gradient aggregation\")\n        else:\n            print(\"Using Parameter Server for gradient aggregation\")\n        \n        print(\"\\nSetup complete!\")\n    \n    def _get_worker_batches(self, iteration: int) -> List[Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"Get batches for each worker for this iteration.\"\"\"\n        n_samples = len(self.X_train)\n        batch_size = self.config.batch_size\n        \n        # Calculate global batch (across all workers)\n        total_batch_size = batch_size * self.config.n_workers\n        start_idx = (iteration * total_batch_size) % n_samples\n        \n        batches = []\n        for w in range(self.config.n_workers):\n            batch_start = (start_idx + w * batch_size) % n_samples\n            batch_end = batch_start + batch_size\n            \n            if batch_end <= n_samples:\n                X_batch = self.X_train[batch_start:batch_end]\n                y_batch = self.y_train[batch_start:batch_end]\n            else:\n                # Wrap around\n                X_batch = np.concatenate([\n                    self.X_train[batch_start:],\n                    self.X_train[:batch_end - n_samples]\n                ])\n                y_batch = np.concatenate([\n                    self.y_train[batch_start:],\n                    self.y_train[:batch_end - n_samples]\n                ])\n            \n            batches.append((X_batch, y_batch))\n        \n        return batches\n    \n    def train_step(self, iteration: int) -> Dict[str, float]:\n        \"\"\"\n        Perform one step of data parallel training.\n        \n        Args:\n            iteration: Current iteration number\n            \n        Returns:\n            Step statistics\n        \"\"\"\n        # Get batches for each worker\n        batches = self._get_worker_batches(iteration)\n        \n        # Each worker computes gradients\n        all_gradients = []\n        for worker, (X_batch, y_batch) in zip(self.workers, batches):\n            gradients = worker.compute_gradients(X_batch, y_batch)\n            all_gradients.append(gradients)\n        \n        # Aggregate gradients\n        if self.ring_allreduce:\n            avg_gradients = self.ring_allreduce.allreduce(all_gradients)\n        else:\n            avg_gradients = GradientAggregator.simple_average(all_gradients)\n        \n        # Update global model\n        self.global_model.apply_gradients(avg_gradients)\n        new_weights = self.global_model.get_weights()\n        \n        # Broadcast new weights to all workers\n        for worker in self.workers:\n            worker.receive_weights(new_weights)\n        \n        # Compute approximate training loss\n        # Use first batch as representative\n        metrics = self.workers[0].evaluate(batches[0][0], batches[0][1])\n        \n        return {\n            'iteration': iteration,\n            'train_loss': metrics['loss']\n        }\n    \n    def train(self, n_iterations: Optional[int] = None, \n              eval_frequency: int = 10,\n              verbose: bool = True) -> pd.DataFrame:\n        \"\"\"\n        Run data parallel training.\n        \n        Args:\n            n_iterations: Number of training iterations\n            eval_frequency: Evaluate every N iterations\n            verbose: Print progress\n            \n        Returns:\n            Training history DataFrame\n        \"\"\"\n        if n_iterations is None:\n            # Default: enough iterations for equivalent of global_rounds epochs\n            n_samples = len(self.X_train)\n            total_batch = self.config.batch_size * self.config.n_workers\n            n_iterations = (n_samples // total_batch) * self.config.global_rounds\n        \n        if verbose:\n            print(\"\\n\" + \"=\" * 70)\n            print(\"DATA PARALLEL TRAINING (Sync SGD)\")\n            print(\"=\" * 70)\n            print(f\"{'Iteration':<12}{'Train Loss':<15}{'Test Loss':<15}{'Test Acc':<12}\")\n            print(\"-\" * 70)\n        \n        for iteration in range(n_iterations):\n            step_result = self.train_step(iteration)\n            \n            # Evaluate periodically\n            if iteration % eval_frequency == 0 or iteration == n_iterations - 1:\n                test_metrics = self.global_model.evaluate(self.X_test, self.y_test)\n                \n                self.history['iteration'].append(iteration)\n                self.history['train_loss'].append(step_result['train_loss'])\n                self.history['test_loss'].append(test_metrics['loss'])\n                self.history['test_accuracy'].append(test_metrics['accuracy'])\n                \n                if verbose:\n                    print(f\"{iteration:<12}{step_result['train_loss']:<15.4f}\"\n                          f\"{test_metrics['loss']:<15.4f}\"\n                          f\"{test_metrics['accuracy']*100:<12.2f}%\")\n        \n        if verbose:\n            print(\"-\" * 70)\n            final_acc = self.history['test_accuracy'][-1] * 100\n            print(f\"Training complete! Final accuracy: {final_acc:.2f}%\")\n        \n        return pd.DataFrame(self.history)\n    \n    def plot_training_history(self) -> None:\n        \"\"\"Plot training curves.\"\"\"\n        if not self.history['iteration']:\n            print(\"No training history to plot!\")\n            return\n        \n        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n        \n        # Loss plot\n        ax1 = axes[0]\n        ax1.plot(self.history['iteration'], self.history['train_loss'], \n                'b-', label='Train Loss', alpha=0.7)\n        ax1.plot(self.history['iteration'], self.history['test_loss'],\n                'r-', label='Test Loss', alpha=0.7)\n        ax1.set_xlabel('Iteration')\n        ax1.set_ylabel('Loss')\n        ax1.set_title('Data Parallel Training: Loss vs Iterations')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # Accuracy plot\n        ax2 = axes[1]\n        ax2.plot(self.history['iteration'], \n                [acc * 100 for acc in self.history['test_accuracy']],\n                'g-')\n        ax2.set_xlabel('Iteration')\n        ax2.set_ylabel('Accuracy (%)')\n        ax2.set_title('Data Parallel Training: Test Accuracy')\n        ax2.grid(True, alpha=0.3)\n        ax2.set_ylim([0, 100])\n        \n        plt.tight_layout()\n        plt.show()\n\nprint(\"DataParallelTrainer implemented!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6.4 Demo: Data Parallel Training with Ring AllReduce",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# DEMO: DATA PARALLEL TRAINING\n# ============================================================\n\n# Configuration for data parallel training\ndp_config = DistributedMLConfig(\n    mode=ExecutionMode.SIMULATION,\n    strategy=TrainingStrategy.DATA_PARALLEL,\n    aggregation=AggregationMethod.RING_ALLREDUCE,\n    framework=Framework.PYTORCH,\n    n_workers=4,\n    batch_size=32,\n    global_rounds=10,\n    learning_rate=0.01\n)\n\n# Model factory\ndef create_dp_model():\n    return create_model_wrapper(\n        Framework.PYTORCH,\n        input_dim=X_train.shape[1],\n        n_classes=10,\n        learning_rate=dp_config.learning_rate\n    )\n\n# Initialize and train\ndp_trainer = DataParallelTrainer(dp_config)\ndp_trainer.setup(X_train, y_train, X_test, y_test, create_dp_model)\n\n# Train for 100 iterations\ndp_history = dp_trainer.train(n_iterations=100, eval_frequency=10)\n\n# Plot results\ndp_trainer.plot_training_history()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Key Insight:** Data parallel training with Ring AllReduce is bandwidth-optimal - each worker only sends and receives O(model_size) data regardless of the number of workers. This makes it scale much better than parameter server for large models.\n\n---\n<a id='part7'></a>\n# Part 7: Secure Aggregation\n---\n\nIn federated learning, we want to protect individual worker updates from being inspected. **Secure Aggregation** ensures the server only sees the aggregated result.\n\n## 7.1 Privacy Concerns\n\n| Concern | Description | Solution |\n|---------|-------------|----------|\n| **Gradient Leakage** | Gradients can reveal training data | Differential Privacy |\n| **Model Inversion** | Reconstruct inputs from model | Noise injection |\n| **Membership Inference** | Detect if data was in training | Limit overfitting |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# SECURE AGGREGATION COMPONENTS\n# ============================================================\n\nclass DifferentialPrivacy:\n    \"\"\"\n    Differential Privacy for gradient protection.\n    \n    Adds calibrated Gaussian noise to gradients to ensure\n    (ε, δ)-differential privacy.\n    \n    The noise scale is calculated as: σ = sensitivity * sqrt(2 * ln(1.25/δ)) / ε\n    \"\"\"\n    \n    def __init__(self, epsilon: float = 1.0, delta: float = 1e-5, \n                 clip_norm: float = 1.0):\n        \"\"\"\n        Initialize differential privacy.\n        \n        Args:\n            epsilon: Privacy budget (smaller = more private)\n            delta: Privacy failure probability\n            clip_norm: Maximum gradient norm (sensitivity)\n        \"\"\"\n        self.epsilon = epsilon\n        self.delta = delta\n        self.clip_norm = clip_norm\n        \n        # Calculate noise scale\n        self.noise_scale = self._compute_noise_scale()\n    \n    def _compute_noise_scale(self) -> float:\n        \"\"\"Compute Gaussian noise scale for (ε,δ)-DP.\"\"\"\n        # Gaussian mechanism: σ = Δf * sqrt(2 * ln(1.25/δ)) / ε\n        sensitivity = self.clip_norm\n        noise_multiplier = np.sqrt(2 * np.log(1.25 / self.delta)) / self.epsilon\n        return sensitivity * noise_multiplier\n    \n    def clip_gradients(self, gradients: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Clip gradients to bounded sensitivity.\n        \n        Args:\n            gradients: Raw gradients\n            \n        Returns:\n            Clipped gradients\n        \"\"\"\n        # Compute total gradient norm\n        total_norm = 0.0\n        for grad in gradients.values():\n            total_norm += np.sum(grad ** 2)\n        total_norm = np.sqrt(total_norm)\n        \n        # Clip if necessary\n        clip_factor = min(1.0, self.clip_norm / (total_norm + 1e-10))\n        \n        clipped = {}\n        for name, grad in gradients.items():\n            clipped[name] = grad * clip_factor\n        \n        return clipped\n    \n    def add_noise(self, gradients: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Add Gaussian noise to gradients.\n        \n        Args:\n            gradients: Clipped gradients\n            \n        Returns:\n            Noisy gradients\n        \"\"\"\n        noisy = {}\n        for name, grad in gradients.items():\n            noise = np.random.normal(0, self.noise_scale, size=grad.shape)\n            noisy[name] = grad + noise\n        \n        return noisy\n    \n    def privatize(self, gradients: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Apply differential privacy (clip + noise).\n        \n        Args:\n            gradients: Raw gradients\n            \n        Returns:\n            Private gradients\n        \"\"\"\n        clipped = self.clip_gradients(gradients)\n        noisy = self.add_noise(clipped)\n        return noisy\n\n\nclass SecureAggregator:\n    \"\"\"\n    Secure aggregation for federated learning.\n    \n    Combines:\n    - Gradient clipping (bounded sensitivity)\n    - Differential privacy (Gaussian noise)\n    - Simple masking (for demonstration)\n    \"\"\"\n    \n    def __init__(self, \n                 enable_dp: bool = True,\n                 epsilon: float = 1.0,\n                 delta: float = 1e-5,\n                 clip_norm: float = 1.0):\n        \"\"\"\n        Initialize secure aggregator.\n        \n        Args:\n            enable_dp: Enable differential privacy\n            epsilon: Privacy budget\n            delta: Privacy parameter\n            clip_norm: Gradient clipping norm\n        \"\"\"\n        self.enable_dp = enable_dp\n        \n        if enable_dp:\n            self.dp = DifferentialPrivacy(epsilon, delta, clip_norm)\n        else:\n            self.dp = None\n        \n        self.privacy_budget_spent = 0.0\n        self.rounds_processed = 0\n    \n    def secure_aggregate(self, \n                         weight_dicts: List[Dict[str, np.ndarray]],\n                         sample_counts: List[int]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Securely aggregate weights with privacy protection.\n        \n        Args:\n            weight_dicts: Weights from each worker\n            sample_counts: Sample counts per worker\n            \n        Returns:\n            Aggregated weights\n        \"\"\"\n        if not weight_dicts:\n            return {}\n        \n        # Apply differential privacy if enabled\n        if self.dp:\n            privatized = []\n            for weights in weight_dicts:\n                private_weights = self.dp.privatize(weights)\n                privatized.append(private_weights)\n            weight_dicts = privatized\n            self.privacy_budget_spent += self.dp.epsilon\n        \n        # Standard FedAvg aggregation\n        aggregated = GradientAggregator.fedavg(weight_dicts, sample_counts)\n        \n        self.rounds_processed += 1\n        \n        return aggregated\n    \n    def get_privacy_report(self) -> Dict[str, float]:\n        \"\"\"Get report on privacy budget usage.\"\"\"\n        return {\n            'rounds_processed': self.rounds_processed,\n            'epsilon_per_round': self.dp.epsilon if self.dp else 0,\n            'total_epsilon_spent': self.privacy_budget_spent,\n            'noise_scale': self.dp.noise_scale if self.dp else 0\n        }\n\nprint(\"DifferentialPrivacy and SecureAggregator implemented!\")\nprint(f\"Available privacy controls: epsilon, delta, clip_norm\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7.2 Demo: Privacy-Preserving Federated Learning",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# DEMO: PRIVACY-UTILITY TRADEOFF\n# ============================================================\n\nprint(\"=\" * 60)\nprint(\"DEMO: DIFFERENTIAL PRIVACY EFFECT ON MODEL ACCURACY\")\nprint(\"=\" * 60)\n\n# Test different epsilon values\nepsilon_values = [0.1, 0.5, 1.0, 5.0, 10.0]\nresults = []\n\nfor epsilon in epsilon_values:\n    print(f\"\\nTesting epsilon = {epsilon}...\")\n    \n    # Create secure aggregator\n    secure_agg = SecureAggregator(\n        enable_dp=True,\n        epsilon=epsilon,\n        delta=1e-5,\n        clip_norm=1.0\n    )\n    \n    # Create simple federated setup\n    config = DistributedMLConfig(\n        n_workers=5,\n        local_epochs=2,\n        global_rounds=5,\n        iid_data=True\n    )\n    \n    fl = FederatedLearning(config)\n    fl.setup(X_train, y_train, X_test, y_test, create_fl_model)\n    \n    # Run training with secure aggregation\n    for round_num in range(5):\n        # Workers train\n        worker_updates = []\n        for worker in fl.workers:\n            worker.receive_global_model(fl.server.global_weights)\n            result = worker.train_local()\n            worker_updates.append({\n                'worker_id': worker.worker_id,\n                'weights': result['weights'],\n                'n_samples': result['n_samples'],\n                'loss': result['avg_loss']\n            })\n        \n        # Secure aggregation\n        sample_counts = [u['n_samples'] for u in worker_updates]\n        weight_dicts = [u['weights'] for u in worker_updates]\n        new_weights = secure_agg.secure_aggregate(weight_dicts, sample_counts)\n        fl.server.update_global_model(new_weights)\n    \n    # Evaluate\n    final_metrics = fl.server.evaluate_global_model(X_test, y_test)\n    privacy_report = secure_agg.get_privacy_report()\n    \n    results.append({\n        'epsilon': epsilon,\n        'accuracy': final_metrics['accuracy'] * 100,\n        'noise_scale': privacy_report['noise_scale']\n    })\n    \n    print(f\"  Final accuracy: {final_metrics['accuracy']*100:.2f}%\")\n    print(f\"  Noise scale: {privacy_report['noise_scale']:.4f}\")\n\n# Plot results\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Accuracy vs Epsilon\nax1 = axes[0]\nepsilons = [r['epsilon'] for r in results]\naccuracies = [r['accuracy'] for r in results]\nax1.semilogx(epsilons, accuracies, 'b-o', markersize=8, linewidth=2)\nax1.set_xlabel('Privacy Budget (ε)')\nax1.set_ylabel('Test Accuracy (%)')\nax1.set_title('Privacy-Utility Tradeoff')\nax1.grid(True, alpha=0.3)\nax1.axhline(y=accuracies[-1], color='g', linestyle='--', alpha=0.5, label='High ε (Less Private)')\n\n# Noise Scale vs Epsilon\nax2 = axes[1]\nnoise_scales = [r['noise_scale'] for r in results]\nax2.semilogx(epsilons, noise_scales, 'r-s', markersize=8, linewidth=2)\nax2.set_xlabel('Privacy Budget (ε)')\nax2.set_ylabel('Noise Scale (σ)')\nax2.set_title('Noise Scale vs Privacy Budget')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Summary table\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PRIVACY-UTILITY TRADEOFF SUMMARY\")\nprint(\"=\" * 60)\nprint(f\"{'Epsilon':<15}{'Accuracy':<15}{'Noise Scale':<15}{'Privacy Level':<15}\")\nprint(\"-\" * 60)\nfor r in results:\n    privacy = \"Very High\" if r['epsilon'] < 0.5 else \"High\" if r['epsilon'] < 2 else \"Medium\" if r['epsilon'] < 5 else \"Low\"\n    print(f\"{r['epsilon']:<15}{r['accuracy']:.2f}%{'':<7}{r['noise_scale']:<15.4f}{privacy:<15}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Key Insight:** Differential privacy provides a mathematically rigorous privacy guarantee, but there's always a tradeoff between privacy (low ε) and utility (high accuracy). In practice, ε values between 1-10 are commonly used.\n\n---\n<a id='part8'></a>\n# Part 8: Fault Tolerance\n---\n\nDistributed systems must handle failures gracefully. Workers can crash, networks can partition, and training must continue.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# FAULT TOLERANCE: CHECKPOINT MANAGER\n# ============================================================\n\nclass CheckpointManager:\n    \"\"\"\n    Manages checkpoints for fault-tolerant training.\n    \n    Saves:\n    - Model weights\n    - Training state (round, history)\n    - Worker states\n    \"\"\"\n    \n    def __init__(self, checkpoint_dir: str = \"./checkpoints\"):\n        \"\"\"\n        Initialize checkpoint manager.\n        \n        Args:\n            checkpoint_dir: Directory to store checkpoints\n        \"\"\"\n        self.checkpoint_dir = checkpoint_dir\n        self.checkpoints: Dict[int, Dict[str, Any]] = {}\n    \n    def save_checkpoint(self, \n                        round_num: int,\n                        global_weights: Dict[str, np.ndarray],\n                        training_history: List[Dict],\n                        worker_states: Optional[Dict] = None) -> str:\n        \"\"\"\n        Save a training checkpoint.\n        \n        Args:\n            round_num: Current training round\n            global_weights: Global model weights\n            training_history: Training history so far\n            worker_states: Optional worker states\n            \n        Returns:\n            Checkpoint ID\n        \"\"\"\n        checkpoint_id = f\"checkpoint_round_{round_num}\"\n        \n        checkpoint = {\n            'round_num': round_num,\n            'global_weights': {k: v.copy() for k, v in global_weights.items()},\n            'training_history': training_history.copy(),\n            'worker_states': worker_states,\n            'timestamp': time.time()\n        }\n        \n        self.checkpoints[round_num] = checkpoint\n        \n        return checkpoint_id\n    \n    def load_checkpoint(self, round_num: Optional[int] = None) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Load a checkpoint.\n        \n        Args:\n            round_num: Specific round to load (None = latest)\n            \n        Returns:\n            Checkpoint data or None\n        \"\"\"\n        if not self.checkpoints:\n            return None\n        \n        if round_num is None:\n            round_num = max(self.checkpoints.keys())\n        \n        return self.checkpoints.get(round_num)\n    \n    def get_available_checkpoints(self) -> List[int]:\n        \"\"\"Get list of available checkpoint rounds.\"\"\"\n        return sorted(self.checkpoints.keys())\n    \n    def cleanup_old_checkpoints(self, keep_last_n: int = 3) -> int:\n        \"\"\"\n        Remove old checkpoints to save memory.\n        \n        Args:\n            keep_last_n: Number of recent checkpoints to keep\n            \n        Returns:\n            Number of checkpoints removed\n        \"\"\"\n        rounds = sorted(self.checkpoints.keys())\n        to_remove = rounds[:-keep_last_n] if len(rounds) > keep_last_n else []\n        \n        for round_num in to_remove:\n            del self.checkpoints[round_num]\n        \n        return len(to_remove)\n\n\nclass WorkerHealthMonitor:\n    \"\"\"\n    Monitors worker health and detects failures.\n    \n    Tracks:\n    - Last heartbeat time\n    - Response times\n    - Failure count\n    \"\"\"\n    \n    def __init__(self, \n                 heartbeat_timeout: float = 30.0,\n                 max_failures: int = 3):\n        \"\"\"\n        Initialize health monitor.\n        \n        Args:\n            heartbeat_timeout: Time before considering worker dead\n            max_failures: Max failures before removing worker\n        \"\"\"\n        self.heartbeat_timeout = heartbeat_timeout\n        self.max_failures = max_failures\n        \n        self.worker_status: Dict[str, Dict[str, Any]] = {}\n        self.failed_workers: List[str] = []\n    \n    def register_worker(self, worker_id: str) -> None:\n        \"\"\"Register a new worker.\"\"\"\n        self.worker_status[worker_id] = {\n            'last_heartbeat': time.time(),\n            'failure_count': 0,\n            'is_active': True,\n            'response_times': []\n        }\n    \n    def record_heartbeat(self, worker_id: str) -> None:\n        \"\"\"Record a heartbeat from worker.\"\"\"\n        if worker_id in self.worker_status:\n            self.worker_status[worker_id]['last_heartbeat'] = time.time()\n            self.worker_status[worker_id]['is_active'] = True\n    \n    def record_response(self, worker_id: str, response_time: float) -> None:\n        \"\"\"Record response time from worker.\"\"\"\n        if worker_id in self.worker_status:\n            times = self.worker_status[worker_id]['response_times']\n            times.append(response_time)\n            # Keep last 10 response times\n            if len(times) > 10:\n                self.worker_status[worker_id]['response_times'] = times[-10:]\n    \n    def record_failure(self, worker_id: str) -> bool:\n        \"\"\"\n        Record a failure for worker.\n        \n        Returns:\n            True if worker should be removed\n        \"\"\"\n        if worker_id not in self.worker_status:\n            return False\n        \n        self.worker_status[worker_id]['failure_count'] += 1\n        \n        if self.worker_status[worker_id]['failure_count'] >= self.max_failures:\n            self.worker_status[worker_id]['is_active'] = False\n            self.failed_workers.append(worker_id)\n            return True\n        \n        return False\n    \n    def check_worker_health(self) -> Dict[str, bool]:\n        \"\"\"\n        Check all workers for timeout.\n        \n        Returns:\n            Dict mapping worker_id to is_healthy\n        \"\"\"\n        current_time = time.time()\n        health_status = {}\n        \n        for worker_id, status in self.worker_status.items():\n            time_since_heartbeat = current_time - status['last_heartbeat']\n            is_healthy = (time_since_heartbeat < self.heartbeat_timeout and \n                         status['is_active'])\n            health_status[worker_id] = is_healthy\n        \n        return health_status\n    \n    def get_active_workers(self) -> List[str]:\n        \"\"\"Get list of active workers.\"\"\"\n        return [w for w, s in self.worker_status.items() if s['is_active']]\n    \n    def get_health_report(self) -> pd.DataFrame:\n        \"\"\"Get health report as DataFrame.\"\"\"\n        records = []\n        for worker_id, status in self.worker_status.items():\n            avg_response = np.mean(status['response_times']) if status['response_times'] else 0\n            records.append({\n                'worker_id': worker_id,\n                'is_active': status['is_active'],\n                'failure_count': status['failure_count'],\n                'avg_response_time': avg_response\n            })\n        return pd.DataFrame(records)\n\nprint(\"CheckpointManager and WorkerHealthMonitor implemented!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n<a id='part9'></a>\n# Part 9: Communication Optimization\n---\n\nCommunication is often the bottleneck in distributed training. We implement **gradient compression** to reduce bandwidth.\n\n## 9.1 Compression Techniques\n\n| Technique | Description | Compression Ratio |\n|-----------|-------------|-------------------|\n| **Top-K Sparsification** | Send only top K% values | Up to 99% |\n| **Quantization** | Reduce precision (32→8 bit) | 4x |\n| **Random Sparsification** | Randomly sample gradients | Variable |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# GRADIENT COMPRESSION\n# ============================================================\n\nclass GradientCompressor:\n    \"\"\"\n    Compresses gradients to reduce communication overhead.\n    \n    Implements:\n    - Top-K sparsification\n    - Random sparsification\n    - Quantization\n    \"\"\"\n    \n    def __init__(self, \n                 compression_type: str = \"topk\",\n                 compression_ratio: float = 0.1):\n        \"\"\"\n        Initialize compressor.\n        \n        Args:\n            compression_type: \"topk\", \"random\", or \"quantize\"\n            compression_ratio: Fraction of gradients to keep (for sparsification)\n        \"\"\"\n        self.compression_type = compression_type\n        self.compression_ratio = compression_ratio\n        \n        # Error feedback buffer (for error accumulation)\n        self.error_buffer: Dict[str, np.ndarray] = {}\n    \n    def topk_sparsify(self, \n                      gradients: Dict[str, np.ndarray],\n                      k_ratio: float = None) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray]]:\n        \"\"\"\n        Keep only top-K% of gradient values by magnitude.\n        \n        Args:\n            gradients: Full gradients\n            k_ratio: Fraction to keep (default: compression_ratio)\n            \n        Returns:\n            Tuple of (sparse_gradients, mask)\n        \"\"\"\n        k_ratio = k_ratio or self.compression_ratio\n        \n        sparse_grads = {}\n        masks = {}\n        \n        for name, grad in gradients.items():\n            flat_grad = grad.flatten()\n            k = max(1, int(len(flat_grad) * k_ratio))\n            \n            # Find top-k indices\n            top_k_indices = np.argsort(np.abs(flat_grad))[-k:]\n            \n            # Create sparse gradient (zeros except top-k)\n            sparse = np.zeros_like(flat_grad)\n            sparse[top_k_indices] = flat_grad[top_k_indices]\n            \n            # Create mask\n            mask = np.zeros_like(flat_grad, dtype=bool)\n            mask[top_k_indices] = True\n            \n            sparse_grads[name] = sparse.reshape(grad.shape)\n            masks[name] = mask.reshape(grad.shape)\n        \n        return sparse_grads, masks\n    \n    def random_sparsify(self, \n                        gradients: Dict[str, np.ndarray],\n                        keep_ratio: float = None) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Randomly keep a fraction of gradients.\n        \n        Args:\n            gradients: Full gradients\n            keep_ratio: Fraction to keep\n            \n        Returns:\n            Sparse gradients (scaled to maintain expectation)\n        \"\"\"\n        keep_ratio = keep_ratio or self.compression_ratio\n        \n        sparse_grads = {}\n        \n        for name, grad in gradients.items():\n            mask = np.random.random(grad.shape) < keep_ratio\n            # Scale by 1/p to maintain unbiased estimate\n            sparse = grad * mask / keep_ratio\n            sparse_grads[name] = sparse\n        \n        return sparse_grads\n    \n    def quantize(self, \n                 gradients: Dict[str, np.ndarray],\n                 n_bits: int = 8) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Quantize gradients to lower precision.\n        \n        Args:\n            gradients: Full precision gradients\n            n_bits: Number of bits for quantization\n            \n        Returns:\n            Quantized (and dequantized) gradients\n        \"\"\"\n        n_levels = 2 ** n_bits\n        \n        quantized_grads = {}\n        \n        for name, grad in gradients.items():\n            # Normalize to [0, 1]\n            min_val = grad.min()\n            max_val = grad.max()\n            range_val = max_val - min_val + 1e-10\n            \n            normalized = (grad - min_val) / range_val\n            \n            # Quantize\n            quantized = np.round(normalized * (n_levels - 1))\n            \n            # Dequantize back\n            dequantized = quantized / (n_levels - 1) * range_val + min_val\n            \n            quantized_grads[name] = dequantized\n        \n        return quantized_grads\n    \n    def compress(self, gradients: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Apply configured compression.\n        \n        Args:\n            gradients: Full gradients\n            \n        Returns:\n            Compressed gradients\n        \"\"\"\n        if self.compression_type == \"topk\":\n            sparse, _ = self.topk_sparsify(gradients)\n            return sparse\n        elif self.compression_type == \"random\":\n            return self.random_sparsify(gradients)\n        elif self.compression_type == \"quantize\":\n            return self.quantize(gradients)\n        else:\n            return gradients\n    \n    def get_compression_stats(self, \n                              original: Dict[str, np.ndarray],\n                              compressed: Dict[str, np.ndarray]) -> Dict[str, float]:\n        \"\"\"\n        Calculate compression statistics.\n        \n        Args:\n            original: Original gradients\n            compressed: Compressed gradients\n            \n        Returns:\n            Compression statistics\n        \"\"\"\n        original_size = sum(g.size for g in original.values())\n        \n        if self.compression_type in [\"topk\", \"random\"]:\n            # Count non-zeros\n            compressed_size = sum(np.count_nonzero(g) for g in compressed.values())\n        else:\n            compressed_size = original_size  # Quantization doesn't reduce count\n        \n        return {\n            'original_size': original_size,\n            'compressed_size': compressed_size,\n            'compression_ratio': original_size / max(compressed_size, 1),\n            'sparsity': 1 - (compressed_size / original_size)\n        }\n\nprint(\"GradientCompressor implemented!\")\nprint(\"Available methods: topk_sparsify, random_sparsify, quantize\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n<a id='part10'></a>\n# Part 10: Complete Distributed ML System\n---\n\nNow we bring everything together into a **unified system** that supports all features.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# COMPLETE DISTRIBUTED ML SYSTEM\n# ============================================================\n\nclass DistributedMLSystem:\n    \"\"\"\n    Unified Distributed Machine Learning System.\n    \n    Supports:\n    - Federated Learning (FedAvg, FedProx)\n    - Data Parallelism (Sync SGD, Ring AllReduce)\n    - Secure Aggregation\n    - Fault Tolerance\n    - Communication Optimization\n    - Both PyTorch and TensorFlow\n    \"\"\"\n    \n    def __init__(self, config: DistributedMLConfig):\n        \"\"\"\n        Initialize the distributed ML system.\n        \n        Args:\n            config: System configuration\n        \"\"\"\n        self.config = config\n        \n        # Core components\n        self.fl_system: Optional[FederatedLearning] = None\n        self.dp_trainer: Optional[DataParallelTrainer] = None\n        \n        # Advanced features\n        self.secure_aggregator: Optional[SecureAggregator] = None\n        self.checkpoint_manager: Optional[CheckpointManager] = None\n        self.health_monitor: Optional[WorkerHealthMonitor] = None\n        self.gradient_compressor: Optional[GradientCompressor] = None\n        \n        # Training state\n        self.is_setup = False\n        self.training_complete = False\n        self.results: Dict[str, Any] = {}\n        \n        self._setup_advanced_features()\n    \n    def _setup_advanced_features(self) -> None:\n        \"\"\"Setup optional advanced features based on config.\"\"\"\n        if self.config.secure_aggregation or self.config.differential_privacy:\n            self.secure_aggregator = SecureAggregator(\n                enable_dp=self.config.differential_privacy,\n                epsilon=self.config.dp_epsilon,\n                delta=self.config.dp_delta\n            )\n        \n        if self.config.fault_tolerance:\n            self.checkpoint_manager = CheckpointManager()\n            self.health_monitor = WorkerHealthMonitor()\n        \n        if self.config.gradient_compression:\n            self.gradient_compressor = GradientCompressor(\n                compression_type=\"topk\",\n                compression_ratio=self.config.compression_ratio\n            )\n    \n    def setup(self,\n              X_train: np.ndarray,\n              y_train: np.ndarray,\n              X_test: np.ndarray,\n              y_test: np.ndarray,\n              model_fn: Optional[Callable[[], ModelWrapper]] = None) -> None:\n        \"\"\"\n        Setup the distributed training system.\n        \n        Args:\n            X_train: Training features\n            y_train: Training labels\n            X_test: Test features\n            y_test: Test labels\n            model_fn: Function to create model (auto-created if None)\n        \"\"\"\n        print(\"=\" * 70)\n        print(\"DISTRIBUTED ML SYSTEM - SETUP\")\n        print(\"=\" * 70)\n        \n        # Store data references\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n        \n        # Create model factory if not provided\n        if model_fn is None:\n            input_dim = X_train.shape[1]\n            n_classes = len(np.unique(y_train))\n            model_fn = lambda: create_model_wrapper(\n                self.config.framework, input_dim, n_classes, self.config.learning_rate\n            )\n        self.model_fn = model_fn\n        \n        # Setup based on strategy\n        if self.config.strategy == TrainingStrategy.FEDERATED:\n            print(\"\\nSetting up Federated Learning...\")\n            self.fl_system = FederatedLearning(self.config)\n            self.fl_system.setup(X_train, y_train, X_test, y_test, model_fn)\n            \n            # Register workers with health monitor\n            if self.health_monitor:\n                for worker in self.fl_system.workers:\n                    self.health_monitor.register_worker(worker.worker_id)\n        \n        else:  # Data Parallel\n            print(\"\\nSetting up Data Parallel Training...\")\n            self.dp_trainer = DataParallelTrainer(self.config)\n            self.dp_trainer.setup(X_train, y_train, X_test, y_test, model_fn)\n        \n        # Print config summary\n        self._print_config_summary()\n        \n        self.is_setup = True\n        print(\"\\nSetup complete!\")\n    \n    def _print_config_summary(self) -> None:\n        \"\"\"Print configuration summary.\"\"\"\n        print(\"\\n\" + \"-\" * 50)\n        print(\"CONFIGURATION SUMMARY\")\n        print(\"-\" * 50)\n        print(f\"Strategy: {self.config.strategy.value}\")\n        print(f\"Framework: {self.config.framework.value}\")\n        print(f\"Workers: {self.config.n_workers}\")\n        print(f\"Aggregation: {self.config.aggregation.value}\")\n        print(f\"Secure Aggregation: {self.config.secure_aggregation or self.config.differential_privacy}\")\n        print(f\"Fault Tolerance: {self.config.fault_tolerance}\")\n        print(f\"Gradient Compression: {self.config.gradient_compression}\")\n        print(\"-\" * 50)\n    \n    def train(self, verbose: bool = True) -> pd.DataFrame:\n        \"\"\"\n        Run distributed training.\n        \n        Args:\n            verbose: Print progress\n            \n        Returns:\n            Training history DataFrame\n        \"\"\"\n        if not self.is_setup:\n            raise RuntimeError(\"System not set up. Call setup() first.\")\n        \n        if self.config.strategy == TrainingStrategy.FEDERATED:\n            return self._train_federated(verbose)\n        else:\n            return self._train_data_parallel(verbose)\n    \n    def _train_federated(self, verbose: bool) -> pd.DataFrame:\n        \"\"\"Run federated learning training.\"\"\"\n        print(\"\\n\" + \"=\" * 70)\n        print(\"FEDERATED LEARNING TRAINING\")\n        print(\"=\" * 70)\n        \n        history = {\n            'round': [], 'train_loss': [], 'test_loss': [], 'test_accuracy': []\n        }\n        \n        if verbose:\n            print(f\"{'Round':<8}{'Workers':<10}{'Train Loss':<15}{'Test Loss':<15}{'Test Acc':<12}\")\n            print(\"-\" * 70)\n        \n        for round_num in range(self.config.global_rounds):\n            # Checkpoint at configured frequency\n            if self.checkpoint_manager and round_num % self.config.checkpoint_frequency == 0:\n                self.checkpoint_manager.save_checkpoint(\n                    round_num,\n                    self.fl_system.server.global_weights,\n                    self.fl_system.server.round_history\n                )\n            \n            # Get active workers\n            if self.health_monitor:\n                active_workers = [w for w in self.fl_system.workers \n                                 if w.worker_id in self.health_monitor.get_active_workers()]\n            else:\n                active_workers = self.fl_system.workers\n            \n            # Run round with selected workers\n            worker_updates = []\n            for worker in active_workers:\n                worker.receive_global_model(self.fl_system.server.global_weights)\n                worker.current_round = round_num\n                \n                start_time = time.time()\n                result = worker.train_local()\n                elapsed = time.time() - start_time\n                \n                # Apply compression if enabled\n                if self.gradient_compressor:\n                    result['weights'] = self.gradient_compressor.compress(result['weights'])\n                \n                worker_updates.append({\n                    'worker_id': worker.worker_id,\n                    'weights': result['weights'],\n                    'n_samples': result['n_samples'],\n                    'loss': result['avg_loss']\n                })\n                \n                if self.health_monitor:\n                    self.health_monitor.record_heartbeat(worker.worker_id)\n                    self.health_monitor.record_response(worker.worker_id, elapsed)\n            \n            # Aggregate with security if enabled\n            if self.secure_aggregator:\n                weight_dicts = [u['weights'] for u in worker_updates]\n                sample_counts = [u['n_samples'] for u in worker_updates]\n                new_weights = self.secure_aggregator.secure_aggregate(weight_dicts, sample_counts)\n            else:\n                new_weights = self.fl_system.server.aggregate_updates(worker_updates)\n            \n            self.fl_system.server.update_global_model(new_weights)\n            \n            # Evaluate\n            test_metrics = self.fl_system.server.evaluate_global_model(self.X_test, self.y_test)\n            avg_train_loss = np.mean([u['loss'] for u in worker_updates])\n            \n            # Record\n            history['round'].append(round_num)\n            history['train_loss'].append(avg_train_loss)\n            history['test_loss'].append(test_metrics['loss'])\n            history['test_accuracy'].append(test_metrics['accuracy'])\n            \n            if verbose:\n                print(f\"{round_num+1:<8}{len(worker_updates):<10}\"\n                      f\"{avg_train_loss:<15.4f}{test_metrics['loss']:<15.4f}\"\n                      f\"{test_metrics['accuracy']*100:<12.2f}%\")\n        \n        if verbose:\n            print(\"-\" * 70)\n            print(f\"Training complete! Final accuracy: {history['test_accuracy'][-1]*100:.2f}%\")\n        \n        self.training_complete = True\n        self.results['history'] = pd.DataFrame(history)\n        return self.results['history']\n    \n    def _train_data_parallel(self, verbose: bool) -> pd.DataFrame:\n        \"\"\"Run data parallel training.\"\"\"\n        return self.dp_trainer.train(verbose=verbose)\n    \n    def get_model(self) -> ModelWrapper:\n        \"\"\"Get the trained global model.\"\"\"\n        if self.config.strategy == TrainingStrategy.FEDERATED:\n            return self.fl_system.server.global_model\n        else:\n            return self.dp_trainer.global_model\n    \n    def get_system_report(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive system report.\"\"\"\n        report = {\n            'config': self.config.to_dict(),\n            'training_complete': self.training_complete\n        }\n        \n        if self.training_complete and 'history' in self.results:\n            final_history = self.results['history']\n            report['final_accuracy'] = final_history['test_accuracy'].iloc[-1]\n            report['final_loss'] = final_history['test_loss'].iloc[-1]\n        \n        if self.secure_aggregator:\n            report['privacy'] = self.secure_aggregator.get_privacy_report()\n        \n        if self.health_monitor:\n            report['worker_health'] = self.health_monitor.get_health_report().to_dict()\n        \n        if self.checkpoint_manager:\n            report['checkpoints'] = self.checkpoint_manager.get_available_checkpoints()\n        \n        return report\n\nprint(\"DistributedMLSystem - Complete unified system implemented!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n<a id='part11'></a>\n# Part 11: Comprehensive Demos\n---\n\nLet's run the complete system with all features enabled!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# DEMO 1: FULL FEDERATED LEARNING WITH ALL FEATURES\n# ============================================================\n\nprint(\"=\" * 70)\nprint(\"DEMO 1: FEDERATED LEARNING WITH ALL ADVANCED FEATURES\")\nprint(\"=\" * 70)\n\n# Configuration with all features\nfull_fl_config = DistributedMLConfig(\n    mode=ExecutionMode.SIMULATION,\n    strategy=TrainingStrategy.FEDERATED,\n    aggregation=AggregationMethod.FEDAVG,\n    framework=Framework.PYTORCH,\n    n_workers=5,\n    local_epochs=3,\n    global_rounds=10,\n    batch_size=32,\n    learning_rate=0.01,\n    iid_data=True,\n    # Advanced features\n    differential_privacy=True,\n    dp_epsilon=5.0,\n    fault_tolerance=True,\n    checkpoint_frequency=3,\n    gradient_compression=True,\n    compression_ratio=0.5\n)\n\n# Create and run system\nfull_system = DistributedMLSystem(full_fl_config)\nfull_system.setup(X_train, y_train, X_test, y_test)\nfull_history = full_system.train()\n\n# Get system report\nreport = full_system.get_system_report()\nprint(\"\\n\" + \"=\" * 50)\nprint(\"SYSTEM REPORT\")\nprint(\"=\" * 50)\nprint(f\"Final Accuracy: {report['final_accuracy']*100:.2f}%\")\nprint(f\"Training Rounds: {full_fl_config.global_rounds}\")\nif 'privacy' in report:\n    print(f\"Privacy Budget Spent: ε = {report['privacy']['total_epsilon_spent']:.2f}\")\nif 'checkpoints' in report:\n    print(f\"Checkpoints Saved: {len(report['checkpoints'])}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# DEMO 2: COMPARISON OF DIFFERENT CONFIGURATIONS\n# ============================================================\n\nprint(\"=\" * 70)\nprint(\"DEMO 2: COMPARING DIFFERENT DISTRIBUTED TRAINING APPROACHES\")\nprint(\"=\" * 70)\n\n# Store results for comparison\ncomparison_results = {}\n\n# Config 1: Basic Federated Learning\nconfig1 = DistributedMLConfig(\n    strategy=TrainingStrategy.FEDERATED,\n    n_workers=5, local_epochs=2, global_rounds=8,\n    iid_data=True\n)\nsystem1 = DistributedMLSystem(config1)\nsystem1.setup(X_train, y_train, X_test, y_test)\nhist1 = system1.train(verbose=False)\ncomparison_results['FL Basic (IID)'] = hist1['test_accuracy'].iloc[-1]\n\n# Config 2: FL with Non-IID data\nconfig2 = DistributedMLConfig(\n    strategy=TrainingStrategy.FEDERATED,\n    n_workers=5, local_epochs=2, global_rounds=8,\n    iid_data=False\n)\nsystem2 = DistributedMLSystem(config2)\nsystem2.setup(X_train, y_train, X_test, y_test)\nhist2 = system2.train(verbose=False)\ncomparison_results['FL Non-IID'] = hist2['test_accuracy'].iloc[-1]\n\n# Config 3: FL with Differential Privacy\nconfig3 = DistributedMLConfig(\n    strategy=TrainingStrategy.FEDERATED,\n    n_workers=5, local_epochs=2, global_rounds=8,\n    differential_privacy=True, dp_epsilon=1.0\n)\nsystem3 = DistributedMLSystem(config3)\nsystem3.setup(X_train, y_train, X_test, y_test)\nhist3 = system3.train(verbose=False)\ncomparison_results['FL + DP (ε=1)'] = hist3['test_accuracy'].iloc[-1]\n\n# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Training curves\nax1 = axes[0]\nax1.plot(hist1['round'], [a*100 for a in hist1['test_accuracy']], 'b-o', label='FL Basic (IID)', markersize=4)\nax1.plot(hist2['round'], [a*100 for a in hist2['test_accuracy']], 'r-s', label='FL Non-IID', markersize=4)\nax1.plot(hist3['round'], [a*100 for a in hist3['test_accuracy']], 'g-^', label='FL + DP', markersize=4)\nax1.set_xlabel('Round')\nax1.set_ylabel('Test Accuracy (%)')\nax1.set_title('Training Progress Comparison')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Final accuracy comparison\nax2 = axes[1]\nnames = list(comparison_results.keys())\naccs = [comparison_results[n] * 100 for n in names]\ncolors = ['#3498db', '#e74c3c', '#2ecc71']\nbars = ax2.bar(names, accs, color=colors, edgecolor='black')\nax2.set_ylabel('Final Accuracy (%)')\nax2.set_title('Final Accuracy Comparison')\nax2.set_ylim([0, 100])\nfor bar, acc in zip(bars, accs):\n    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n             f'{acc:.1f}%', ha='center', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nComparison Summary:\")\nfor name, acc in comparison_results.items():\n    print(f\"  {name}: {acc*100:.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n<a id='part12'></a>\n# Part 12: Summary and Conclusions\n---\n\n## What We Built\n\nWe implemented a **complete distributed machine learning system** from scratch!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# FINAL SUMMARY DASHBOARD\n# ============================================================\n\nprint(\"=\" * 80)\nprint(\"                    DISTRIBUTED ML SYSTEM - FINAL SUMMARY\")\nprint(\"=\" * 80)\n\n# Components summary\ncomponents = [\n    (\"DistributedMLConfig\", \"Configuration management with dataclasses\"),\n    (\"ModelWrapper\", \"Framework-agnostic model interface (PyTorch + TensorFlow)\"),\n    (\"Communicator\", \"Message passing abstraction (Queue + Socket)\"),\n    (\"DataPartitioner\", \"IID and Non-IID data distribution\"),\n    (\"GradientAggregator\", \"FedAvg, FedProx, simple averaging\"),\n    (\"FederatedWorker\", \"Local training with weight updates\"),\n    (\"FederatedServer\", \"Global model coordination\"),\n    (\"FederatedLearning\", \"Complete FL orchestrator\"),\n    (\"DataParallelWorker\", \"Gradient computation worker\"),\n    (\"RingAllReduce\", \"Bandwidth-optimal gradient reduction\"),\n    (\"DataParallelTrainer\", \"Sync SGD training orchestrator\"),\n    (\"DifferentialPrivacy\", \"Gradient clipping + noise injection\"),\n    (\"SecureAggregator\", \"Privacy-preserving aggregation\"),\n    (\"CheckpointManager\", \"Training state persistence\"),\n    (\"WorkerHealthMonitor\", \"Failure detection and tracking\"),\n    (\"GradientCompressor\", \"Top-K, random sparsification, quantization\"),\n    (\"DistributedMLSystem\", \"Unified system combining all features\")\n]\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"COMPONENTS IMPLEMENTED\")\nprint(\"=\" * 80)\nprint(f\"{'Class':<25}{'Description':<55}\")\nprint(\"-\" * 80)\nfor name, desc in components:\n    print(f\"{name:<25}{desc:<55}\")\n\nprint(f\"\\nTotal Classes Implemented: {len(components)}\")\n\n# Features summary\nfeatures = pd.DataFrame({\n    'Feature': [\n        'Federated Learning (FedAvg)',\n        'Data Parallelism (Sync SGD)',\n        'Ring AllReduce',\n        'PyTorch Support',\n        'TensorFlow Support',\n        'IID Data Partitioning',\n        'Non-IID Data Partitioning',\n        'Differential Privacy',\n        'Gradient Clipping',\n        'Checkpointing',\n        'Worker Health Monitoring',\n        'Top-K Gradient Compression',\n        'Quantization',\n        'Simulated Communication',\n        'Network Communication (Reference)'\n    ],\n    'Status': ['Implemented'] * 15,\n    'Category': [\n        'Training Strategy', 'Training Strategy', 'Aggregation',\n        'Framework', 'Framework',\n        'Data', 'Data',\n        'Privacy', 'Privacy',\n        'Fault Tolerance', 'Fault Tolerance',\n        'Optimization', 'Optimization',\n        'Communication', 'Communication'\n    ]\n})\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FEATURE MATRIX\")\nprint(\"=\" * 80)\nprint(features.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Key Formulas Reference\n\n### FedAvg (Federated Averaging)\n$$w_{t+1} = \\sum_{k=1}^{K} \\frac{n_k}{n} w_k^{t+1}$$\n\n### FedProx (Proximal Term)\n$$\\min_w F_k(w) + \\frac{\\mu}{2} ||w - w_t||^2$$\n\n### Differential Privacy (Gaussian Mechanism)\n$$\\sigma = \\frac{\\Delta f \\cdot \\sqrt{2 \\ln(1.25/\\delta)}}{\\epsilon}$$\n\n### Top-K Sparsification\n$$\\text{sparse}(g) = g \\odot \\mathbb{1}_{|g| \\in \\text{TopK}(|g|)}$$\n\n## Production Tools Mapping\n\n| Our Implementation | Production Tool |\n|-------------------|-----------------|\n| FederatedLearning | TensorFlow Federated, PySyft, FATE |\n| DataParallelTrainer | PyTorch DDP, Horovod, DeepSpeed |\n| RingAllReduce | NCCL, Gloo, MPI |\n| DifferentialPrivacy | Opacus, TensorFlow Privacy |\n| CheckpointManager | Ray, MLflow |\n| GradientCompressor | PowerSGD, Deep Gradient Compression |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# NOTEBOOK COMPLETION CHECKLIST\n# ============================================================\n\nchecklist = {\n    \"Core Distributed Training\": {\n        \"Federated Learning (FedAvg)\": True,\n        \"Data Parallelism (Sync SGD)\": True,\n        \"Ring AllReduce\": True,\n        \"Parameter Server Architecture\": True\n    },\n    \"Framework Support\": {\n        \"PyTorch Integration\": True,\n        \"TensorFlow Integration\": True,\n        \"Framework-Agnostic API\": True\n    },\n    \"Privacy & Security\": {\n        \"Differential Privacy\": True,\n        \"Gradient Clipping\": True,\n        \"Secure Aggregation\": True\n    },\n    \"Fault Tolerance\": {\n        \"Checkpointing\": True,\n        \"Worker Health Monitoring\": True,\n        \"Failure Recovery\": True\n    },\n    \"Communication Optimization\": {\n        \"Top-K Sparsification\": True,\n        \"Random Sparsification\": True,\n        \"Gradient Quantization\": True\n    },\n    \"Demonstrations\": {\n        \"IID vs Non-IID Comparison\": True,\n        \"Privacy-Utility Tradeoff\": True,\n        \"Full System Demo\": True\n    }\n}\n\nprint(\"=\" * 70)\nprint(\"                PROJECT COMPLETION CHECKLIST\")\nprint(\"=\" * 70)\n\nfor category, items in checklist.items():\n    print(f\"\\n{category}:\")\n    for item, completed in items.items():\n        status = \"[DONE]\" if completed else \"[    ]\"\n        print(f\"  {status} {item}\")\n\ntotal_items = sum(len(items) for items in checklist.values())\ncompleted_items = sum(sum(items.values()) for items in checklist.values())\nprint(f\"\\n{'='*70}\")\nprint(f\"COMPLETION: {completed_items}/{total_items} ({completed_items/total_items*100:.0f}%)\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Congratulations!\n\nYou have successfully implemented a **complete distributed machine learning system** that includes:\n\n| Component | What You Learned |\n|-----------|-----------------|\n| **Federated Learning** | Training models on decentralized data while preserving privacy |\n| **Data Parallelism** | Scaling training across multiple workers with synchronized updates |\n| **Ring AllReduce** | Bandwidth-optimal gradient aggregation algorithms |\n| **Secure Aggregation** | Differential privacy for gradient protection |\n| **Fault Tolerance** | Checkpointing and worker health monitoring |\n| **Communication Optimization** | Gradient compression techniques |\n\n### Key Takeaways\n\n1. **Distributed training is essential** for handling large datasets and complex models\n2. **Privacy-preserving ML** is increasingly important in real-world applications\n3. **Framework-agnostic design** makes systems more flexible and maintainable\n4. **Trade-offs exist** between privacy, accuracy, and communication efficiency\n\n### Next Steps\n\n- Deploy this system on actual distributed infrastructure (Kubernetes, AWS, etc.)\n- Implement additional aggregation methods (FedProx, SCAFFOLD, FedOpt)\n- Add model parallelism for very large models\n- Integrate with production frameworks (TensorFlow Federated, PySyft)\n\n---\n\n**Author:** Anik Tahabilder  \n**Project:** 22/22 - Distributed ML System  \n**Difficulty:** 10/10  \n**Learning Value:** 10/10  \n**Resume Value:** 10/10\n\n---",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}