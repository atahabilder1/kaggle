{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 20: Build Your Own AutoML System\n",
    "\n",
    "**Combine everything you've learned into a single automated ML system**\n",
    "\n",
    "This is the ultimate capstone project! We'll build a complete AutoML system from scratch that automates:\n",
    "\n",
    "```\n",
    "┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n",
    "│    Data     │───▶│   Feature   │───▶│   Feature   │───▶│    Model    │\n",
    "│ Preparation │    │ Engineering │    │  Selection  │    │  Selection  │\n",
    "└─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘\n",
    "                                                                │\n",
    "                                                                ▼\n",
    "┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n",
    "│    Best     │◀───│  Ensemble   │◀───│   Model     │◀───│ Hyperparameter\n",
    "│   Model     │    │   Methods   │    │  Training   │    │   Tuning    │\n",
    "└─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘\n",
    "```\n",
    "\n",
    "**Our AutoML System Features:**\n",
    "- Auto problem type detection (classification/regression)\n",
    "- Auto data preprocessing (missing values, encoding, scaling)\n",
    "- Auto feature engineering\n",
    "- Auto feature selection\n",
    "- Multi-model training & comparison\n",
    "- Hyperparameter optimization\n",
    "- Ensemble methods\n",
    "- Auto report generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Installation](#1-setup-and-installation)\n",
    "2. [AutoML Architecture Overview](#2-automl-architecture-overview)\n",
    "3. [Auto Data Type Detection](#3-auto-data-type-detection)\n",
    "4. [Auto Exploratory Data Analysis](#4-auto-exploratory-data-analysis)\n",
    "5. [Auto Data Preprocessing](#5-auto-data-preprocessing)\n",
    "6. [Auto Feature Engineering](#6-auto-feature-engineering)\n",
    "7. [Auto Feature Selection](#7-auto-feature-selection)\n",
    "8. [Multi-Model Training](#8-multi-model-training)\n",
    "9. [Hyperparameter Tuning](#9-hyperparameter-tuning)\n",
    "10. [Ensemble Methods](#10-ensemble-methods)\n",
    "11. [Complete AutoML Class](#11-complete-automl-class)\n",
    "12. [Demo: AutoML in Action](#12-demo-automl-in-action)\n",
    "13. [Summary](#13-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q scikit-learn xgboost lightgbm optuna category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, StratifiedKFold, KFold\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, RobustScaler, \n",
    "    LabelEncoder, OneHotEncoder\n",
    ")\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, f_classif, f_regression, mutual_info_classif,\n",
    "    mutual_info_regression, RFE, SelectFromModel, VarianceThreshold\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression, Ridge, Lasso, ElasticNet,\n",
    "    SGDClassifier, PassiveAggressiveClassifier\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "    AdaBoostClassifier, AdaBoostRegressor,\n",
    "    ExtraTreesClassifier, ExtraTreesRegressor,\n",
    "    VotingClassifier, VotingRegressor,\n",
    "    StackingClassifier, StackingRegressor\n",
    ")\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# XGBoost & LightGBM\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    make_scorer\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "except:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"Optuna not available, using GridSearch instead\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Set seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Optuna available: {OPTUNA_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AutoML Architecture Overview\n",
    "\n",
    "Our AutoML system consists of modular components that can be used independently or together:\n",
    "\n",
    "| Module | Responsibility |\n",
    "|--------|---------------|\n",
    "| `DataTypeDetector` | Detect column types (numeric, categorical, text, datetime) |\n",
    "| `AutoEDA` | Automatic exploratory data analysis |\n",
    "| `AutoPreprocessor` | Handle missing values, encoding, scaling |\n",
    "| `AutoFeatureEngineer` | Create new features automatically |\n",
    "| `AutoFeatureSelector` | Select best features |\n",
    "| `ModelTrainer` | Train multiple models |\n",
    "| `HyperparameterTuner` | Optimize hyperparameters |\n",
    "| `Ensembler` | Create ensemble models |\n",
    "| `AutoML` | Main class combining all modules |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define problem types\n",
    "class ProblemType(Enum):\n",
    "    BINARY_CLASSIFICATION = \"binary_classification\"\n",
    "    MULTICLASS_CLASSIFICATION = \"multiclass_classification\"\n",
    "    REGRESSION = \"regression\"\n",
    "\n",
    "# Define column types\n",
    "class ColumnType(Enum):\n",
    "    NUMERIC = \"numeric\"\n",
    "    CATEGORICAL = \"categorical\"\n",
    "    TEXT = \"text\"\n",
    "    DATETIME = \"datetime\"\n",
    "    BINARY = \"binary\"\n",
    "    ID = \"id\"\n",
    "\n",
    "@dataclass\n",
    "class AutoMLConfig:\n",
    "    \"\"\"Configuration for AutoML system.\"\"\"\n",
    "    # General\n",
    "    random_state: int = 42\n",
    "    test_size: float = 0.2\n",
    "    cv_folds: int = 5\n",
    "    \n",
    "    # Preprocessing\n",
    "    handle_missing: bool = True\n",
    "    encode_categorical: bool = True\n",
    "    scale_numeric: bool = True\n",
    "    \n",
    "    # Feature Engineering\n",
    "    create_interactions: bool = True\n",
    "    create_polynomial: bool = False\n",
    "    \n",
    "    # Feature Selection\n",
    "    feature_selection: bool = True\n",
    "    max_features: Optional[int] = None\n",
    "    \n",
    "    # Model Training\n",
    "    quick_mode: bool = False  # Use fewer models for speed\n",
    "    tune_hyperparameters: bool = True\n",
    "    tuning_trials: int = 50\n",
    "    \n",
    "    # Ensemble\n",
    "    create_ensemble: bool = True\n",
    "    \n",
    "print(\"AutoML configuration defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Auto Data Type Detection\n",
    "\n",
    "First, we need to automatically detect:\n",
    "- Problem type (classification vs regression)\n",
    "- Column types (numeric, categorical, text, datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTypeDetector:\n",
    "    \"\"\"\n",
    "    Automatically detect data types for columns and problem type.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, categorical_threshold: int = 20, id_threshold: float = 0.9):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            categorical_threshold: Max unique values to consider as categorical\n",
    "            id_threshold: If unique ratio > threshold, likely an ID column\n",
    "        \"\"\"\n",
    "        self.categorical_threshold = categorical_threshold\n",
    "        self.id_threshold = id_threshold\n",
    "    \n",
    "    def detect_problem_type(self, y: pd.Series) -> ProblemType:\n",
    "        \"\"\"\n",
    "        Detect if problem is classification or regression.\n",
    "        \"\"\"\n",
    "        n_unique = y.nunique()\n",
    "        dtype = y.dtype\n",
    "        \n",
    "        # Check if target is numeric with many unique values\n",
    "        if np.issubdtype(dtype, np.floating) and n_unique > 20:\n",
    "            return ProblemType.REGRESSION\n",
    "        \n",
    "        # Check if target is integer with many unique values\n",
    "        if np.issubdtype(dtype, np.integer) and n_unique > 20:\n",
    "            return ProblemType.REGRESSION\n",
    "        \n",
    "        # Classification\n",
    "        if n_unique == 2:\n",
    "            return ProblemType.BINARY_CLASSIFICATION\n",
    "        else:\n",
    "            return ProblemType.MULTICLASS_CLASSIFICATION\n",
    "    \n",
    "    def detect_column_types(self, df: pd.DataFrame, target_col: str = None) -> Dict[str, ColumnType]:\n",
    "        \"\"\"\n",
    "        Detect type for each column.\n",
    "        \"\"\"\n",
    "        column_types = {}\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if col == target_col:\n",
    "                continue\n",
    "            \n",
    "            column_types[col] = self._detect_single_column(df[col], col)\n",
    "        \n",
    "        return column_types\n",
    "    \n",
    "    def _detect_single_column(self, series: pd.Series, col_name: str) -> ColumnType:\n",
    "        \"\"\"\n",
    "        Detect type for a single column.\n",
    "        \"\"\"\n",
    "        # Check for datetime\n",
    "        if pd.api.types.is_datetime64_any_dtype(series):\n",
    "            return ColumnType.DATETIME\n",
    "        \n",
    "        # Check if column name suggests ID\n",
    "        if any(id_word in col_name.lower() for id_word in ['id', '_id', 'index', 'key']):\n",
    "            if series.nunique() / len(series) > self.id_threshold:\n",
    "                return ColumnType.ID\n",
    "        \n",
    "        # Check for numeric\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            n_unique = series.nunique()\n",
    "            \n",
    "            # Binary\n",
    "            if n_unique == 2:\n",
    "                return ColumnType.BINARY\n",
    "            \n",
    "            # Categorical (low cardinality numeric)\n",
    "            if n_unique <= self.categorical_threshold:\n",
    "                return ColumnType.CATEGORICAL\n",
    "            \n",
    "            return ColumnType.NUMERIC\n",
    "        \n",
    "        # Check for text/categorical\n",
    "        if pd.api.types.is_string_dtype(series) or series.dtype == 'object':\n",
    "            n_unique = series.nunique()\n",
    "            avg_len = series.astype(str).str.len().mean()\n",
    "            \n",
    "            # Long text (likely text data)\n",
    "            if avg_len > 50:\n",
    "                return ColumnType.TEXT\n",
    "            \n",
    "            # High cardinality might be ID\n",
    "            if series.nunique() / len(series) > self.id_threshold:\n",
    "                return ColumnType.ID\n",
    "            \n",
    "            return ColumnType.CATEGORICAL\n",
    "        \n",
    "        return ColumnType.CATEGORICAL\n",
    "    \n",
    "    def get_summary(self, column_types: Dict[str, ColumnType]) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Group columns by type.\n",
    "        \"\"\"\n",
    "        summary = {ct.value: [] for ct in ColumnType}\n",
    "        for col, col_type in column_types.items():\n",
    "            summary[col_type.value].append(col)\n",
    "        return {k: v for k, v in summary.items() if v}  # Remove empty\n",
    "\n",
    "# Test\n",
    "print(\"DataTypeDetector created!\")\n",
    "print(\"\\nCapabilities:\")\n",
    "print(\"  - Detect problem type (binary/multiclass classification, regression)\")\n",
    "print(\"  - Detect column types (numeric, categorical, text, datetime, ID)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Auto Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEDA:\n",
    "    \"\"\"\n",
    "    Automatic Exploratory Data Analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.report = {}\n",
    "    \n",
    "    def analyze(self, df: pd.DataFrame, target_col: str = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform comprehensive EDA.\n",
    "        \"\"\"\n",
    "        self.report = {\n",
    "            'basic_info': self._basic_info(df),\n",
    "            'missing_values': self._missing_analysis(df),\n",
    "            'duplicates': self._duplicate_analysis(df),\n",
    "            'numeric_stats': self._numeric_analysis(df),\n",
    "            'categorical_stats': self._categorical_analysis(df),\n",
    "        }\n",
    "        \n",
    "        if target_col and target_col in df.columns:\n",
    "            self.report['target_analysis'] = self._target_analysis(df, target_col)\n",
    "        \n",
    "        return self.report\n",
    "    \n",
    "    def _basic_info(self, df: pd.DataFrame) -> Dict:\n",
    "        return {\n",
    "            'n_rows': len(df),\n",
    "            'n_columns': len(df.columns),\n",
    "            'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "            'dtypes': df.dtypes.value_counts().to_dict()\n",
    "        }\n",
    "    \n",
    "    def _missing_analysis(self, df: pd.DataFrame) -> Dict:\n",
    "        missing = df.isnull().sum()\n",
    "        missing_pct = (missing / len(df) * 100).round(2)\n",
    "        return {\n",
    "            'total_missing': missing.sum(),\n",
    "            'columns_with_missing': (missing > 0).sum(),\n",
    "            'missing_by_column': missing[missing > 0].to_dict(),\n",
    "            'missing_pct_by_column': missing_pct[missing_pct > 0].to_dict()\n",
    "        }\n",
    "    \n",
    "    def _duplicate_analysis(self, df: pd.DataFrame) -> Dict:\n",
    "        return {\n",
    "            'n_duplicates': df.duplicated().sum(),\n",
    "            'duplicate_pct': (df.duplicated().sum() / len(df) * 100).round(2)\n",
    "        }\n",
    "    \n",
    "    def _numeric_analysis(self, df: pd.DataFrame) -> Dict:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if not numeric_cols:\n",
    "            return {}\n",
    "        \n",
    "        stats = df[numeric_cols].describe().to_dict()\n",
    "        \n",
    "        # Check for outliers (IQR method)\n",
    "        outliers = {}\n",
    "        for col in numeric_cols:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outlier_count = ((df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)).sum()\n",
    "            if outlier_count > 0:\n",
    "                outliers[col] = outlier_count\n",
    "        \n",
    "        return {\n",
    "            'columns': numeric_cols,\n",
    "            'statistics': stats,\n",
    "            'outliers': outliers\n",
    "        }\n",
    "    \n",
    "    def _categorical_analysis(self, df: pd.DataFrame) -> Dict:\n",
    "        cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        if not cat_cols:\n",
    "            return {}\n",
    "        \n",
    "        analysis = {}\n",
    "        for col in cat_cols:\n",
    "            analysis[col] = {\n",
    "                'n_unique': df[col].nunique(),\n",
    "                'top_values': df[col].value_counts().head(5).to_dict()\n",
    "            }\n",
    "        \n",
    "        return {'columns': cat_cols, 'analysis': analysis}\n",
    "    \n",
    "    def _target_analysis(self, df: pd.DataFrame, target_col: str) -> Dict:\n",
    "        target = df[target_col]\n",
    "        return {\n",
    "            'dtype': str(target.dtype),\n",
    "            'n_unique': target.nunique(),\n",
    "            'value_counts': target.value_counts().to_dict(),\n",
    "            'distribution': target.describe().to_dict() if np.issubdtype(target.dtype, np.number) else None\n",
    "        }\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print formatted EDA report.\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"AUTO EDA REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Basic Info\n",
    "        info = self.report['basic_info']\n",
    "        print(f\"\\nBasic Information:\")\n",
    "        print(f\"  Rows: {info['n_rows']:,}\")\n",
    "        print(f\"  Columns: {info['n_columns']}\")\n",
    "        print(f\"  Memory: {info['memory_mb']:.2f} MB\")\n",
    "        \n",
    "        # Missing Values\n",
    "        missing = self.report['missing_values']\n",
    "        print(f\"\\nMissing Values:\")\n",
    "        print(f\"  Total: {missing['total_missing']:,}\")\n",
    "        print(f\"  Columns with missing: {missing['columns_with_missing']}\")\n",
    "        \n",
    "        # Duplicates\n",
    "        dups = self.report['duplicates']\n",
    "        print(f\"\\nDuplicates:\")\n",
    "        print(f\"  Count: {dups['n_duplicates']:,} ({dups['duplicate_pct']}%)\")\n",
    "        \n",
    "        # Numeric\n",
    "        if self.report['numeric_stats']:\n",
    "            print(f\"\\nNumeric Columns: {len(self.report['numeric_stats']['columns'])}\")\n",
    "            if self.report['numeric_stats']['outliers']:\n",
    "                print(f\"  Columns with outliers: {len(self.report['numeric_stats']['outliers'])}\")\n",
    "        \n",
    "        # Categorical\n",
    "        if self.report['categorical_stats']:\n",
    "            print(f\"\\nCategorical Columns: {len(self.report['categorical_stats']['columns'])}\")\n",
    "\n",
    "print(\"AutoEDA created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Auto Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoPreprocessor:\n",
    "    \"\"\"\n",
    "    Automatic data preprocessing pipeline.\n",
    "    \n",
    "    Handles:\n",
    "    - Missing values (imputation)\n",
    "    - Categorical encoding\n",
    "    - Numeric scaling\n",
    "    - Outlier handling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: AutoMLConfig = None):\n",
    "        self.config = config or AutoMLConfig()\n",
    "        self.column_types = {}\n",
    "        self.imputers = {}\n",
    "        self.encoders = {}\n",
    "        self.scalers = {}\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, df: pd.DataFrame, column_types: Dict[str, ColumnType]) -> 'AutoPreprocessor':\n",
    "        \"\"\"\n",
    "        Fit preprocessing transformers.\n",
    "        \"\"\"\n",
    "        self.column_types = column_types\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Group columns by type\n",
    "        numeric_cols = [c for c, t in column_types.items() if t == ColumnType.NUMERIC]\n",
    "        categorical_cols = [c for c, t in column_types.items() if t == ColumnType.CATEGORICAL]\n",
    "        binary_cols = [c for c, t in column_types.items() if t == ColumnType.BINARY]\n",
    "        \n",
    "        # Fit imputers\n",
    "        if self.config.handle_missing:\n",
    "            # Numeric: median imputation\n",
    "            if numeric_cols:\n",
    "                self.imputers['numeric'] = SimpleImputer(strategy='median')\n",
    "                self.imputers['numeric'].fit(df[numeric_cols])\n",
    "            \n",
    "            # Categorical: mode imputation\n",
    "            if categorical_cols:\n",
    "                self.imputers['categorical'] = SimpleImputer(strategy='most_frequent')\n",
    "                self.imputers['categorical'].fit(df[categorical_cols])\n",
    "        \n",
    "        # Fit encoders\n",
    "        if self.config.encode_categorical:\n",
    "            for col in categorical_cols:\n",
    "                le = LabelEncoder()\n",
    "                # Handle NaN for fitting\n",
    "                non_null = df[col].dropna().astype(str)\n",
    "                le.fit(non_null)\n",
    "                self.encoders[col] = le\n",
    "        \n",
    "        # Fit scalers\n",
    "        if self.config.scale_numeric and numeric_cols:\n",
    "            # Use RobustScaler for outlier resistance\n",
    "            self.scalers['numeric'] = RobustScaler()\n",
    "            # Impute first, then fit scaler\n",
    "            if 'numeric' in self.imputers:\n",
    "                imputed = self.imputers['numeric'].transform(df[numeric_cols])\n",
    "            else:\n",
    "                imputed = df[numeric_cols].values\n",
    "            self.scalers['numeric'].fit(imputed)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform data using fitted transformers.\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Preprocessor not fitted. Call fit() first.\")\n",
    "        \n",
    "        df = df.copy()\n",
    "        \n",
    "        # Group columns\n",
    "        numeric_cols = [c for c, t in self.column_types.items() if t == ColumnType.NUMERIC and c in df.columns]\n",
    "        categorical_cols = [c for c, t in self.column_types.items() if t == ColumnType.CATEGORICAL and c in df.columns]\n",
    "        \n",
    "        # Impute missing values\n",
    "        if self.config.handle_missing:\n",
    "            if 'numeric' in self.imputers and numeric_cols:\n",
    "                df[numeric_cols] = self.imputers['numeric'].transform(df[numeric_cols])\n",
    "            if 'categorical' in self.imputers and categorical_cols:\n",
    "                df[categorical_cols] = self.imputers['categorical'].transform(df[categorical_cols])\n",
    "        \n",
    "        # Encode categorical\n",
    "        if self.config.encode_categorical:\n",
    "            for col in categorical_cols:\n",
    "                if col in self.encoders:\n",
    "                    le = self.encoders[col]\n",
    "                    # Handle unseen categories\n",
    "                    df[col] = df[col].astype(str)\n",
    "                    df[col] = df[col].apply(\n",
    "                        lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "                    )\n",
    "        \n",
    "        # Scale numeric\n",
    "        if self.config.scale_numeric and 'numeric' in self.scalers and numeric_cols:\n",
    "            df[numeric_cols] = self.scalers['numeric'].transform(df[numeric_cols])\n",
    "        \n",
    "        # Remove ID columns\n",
    "        id_cols = [c for c, t in self.column_types.items() if t == ColumnType.ID and c in df.columns]\n",
    "        df = df.drop(columns=id_cols, errors='ignore')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def fit_transform(self, df: pd.DataFrame, column_types: Dict[str, ColumnType]) -> pd.DataFrame:\n",
    "        \"\"\"Fit and transform in one step.\"\"\"\n",
    "        self.fit(df, column_types)\n",
    "        return self.transform(df)\n",
    "    \n",
    "    def get_feature_names(self) -> List[str]:\n",
    "        \"\"\"Get names of output features.\"\"\"\n",
    "        return [c for c, t in self.column_types.items() if t != ColumnType.ID]\n",
    "\n",
    "print(\"AutoPreprocessor created!\")\n",
    "print(\"\\nCapabilities:\")\n",
    "print(\"  - Missing value imputation (median/mode)\")\n",
    "print(\"  - Categorical encoding (Label Encoding)\")\n",
    "print(\"  - Numeric scaling (Robust Scaler)\")\n",
    "print(\"  - ID column removal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Auto Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Automatic feature engineering.\n",
    "    \n",
    "    Creates:\n",
    "    - Interaction features\n",
    "    - Polynomial features\n",
    "    - Aggregation features\n",
    "    - Date features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: AutoMLConfig = None):\n",
    "        self.config = config or AutoMLConfig()\n",
    "        self.created_features = []\n",
    "        \n",
    "    def create_features(self, df: pd.DataFrame, column_types: Dict[str, ColumnType]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create new features automatically.\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        original_cols = df.columns.tolist()\n",
    "        \n",
    "        # Get numeric columns\n",
    "        numeric_cols = [c for c, t in column_types.items() \n",
    "                       if t == ColumnType.NUMERIC and c in df.columns]\n",
    "        \n",
    "        # Create interaction features (top numeric columns)\n",
    "        if self.config.create_interactions and len(numeric_cols) >= 2:\n",
    "            # Limit to prevent explosion\n",
    "            top_cols = numeric_cols[:5]\n",
    "            for i, col1 in enumerate(top_cols):\n",
    "                for col2 in top_cols[i+1:]:\n",
    "                    # Multiplication\n",
    "                    df[f'{col1}_x_{col2}'] = df[col1] * df[col2]\n",
    "                    # Ratio (with small epsilon)\n",
    "                    df[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 1e-8)\n",
    "        \n",
    "        # Create aggregation features\n",
    "        if len(numeric_cols) >= 3:\n",
    "            df['numeric_mean'] = df[numeric_cols].mean(axis=1)\n",
    "            df['numeric_std'] = df[numeric_cols].std(axis=1)\n",
    "            df['numeric_max'] = df[numeric_cols].max(axis=1)\n",
    "            df['numeric_min'] = df[numeric_cols].min(axis=1)\n",
    "            df['numeric_range'] = df['numeric_max'] - df['numeric_min']\n",
    "        \n",
    "        # Create datetime features\n",
    "        datetime_cols = [c for c, t in column_types.items() \n",
    "                        if t == ColumnType.DATETIME and c in df.columns]\n",
    "        for col in datetime_cols:\n",
    "            df = self._create_datetime_features(df, col)\n",
    "        \n",
    "        # Track created features\n",
    "        self.created_features = [c for c in df.columns if c not in original_cols]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _create_datetime_features(self, df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract features from datetime column.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dt = pd.to_datetime(df[col])\n",
    "            df[f'{col}_year'] = dt.dt.year\n",
    "            df[f'{col}_month'] = dt.dt.month\n",
    "            df[f'{col}_day'] = dt.dt.day\n",
    "            df[f'{col}_dayofweek'] = dt.dt.dayofweek\n",
    "            df[f'{col}_hour'] = dt.dt.hour\n",
    "            df[f'{col}_is_weekend'] = (dt.dt.dayofweek >= 5).astype(int)\n",
    "        except:\n",
    "            pass\n",
    "        return df\n",
    "\n",
    "print(\"AutoFeatureEngineer created!\")\n",
    "print(\"\\nCapabilities:\")\n",
    "print(\"  - Interaction features (multiplication, ratio)\")\n",
    "print(\"  - Aggregation features (mean, std, max, min, range)\")\n",
    "print(\"  - Datetime features (year, month, day, dayofweek, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Auto Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoFeatureSelector:\n",
    "    \"\"\"\n",
    "    Automatic feature selection using multiple methods.\n",
    "    \n",
    "    Methods:\n",
    "    - Variance threshold\n",
    "    - Correlation-based\n",
    "    - Statistical tests (f_classif, f_regression)\n",
    "    - Mutual information\n",
    "    - Model-based (Random Forest importance)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, problem_type: ProblemType, max_features: int = None):\n",
    "        self.problem_type = problem_type\n",
    "        self.max_features = max_features\n",
    "        self.selected_features = []\n",
    "        self.feature_importance = {}\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'AutoFeatureSelector':\n",
    "        \"\"\"\n",
    "        Fit feature selector and determine best features.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Ensure numeric\n",
    "        X = X.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Handle any remaining NaN\n",
    "        X = X.fillna(X.median())\n",
    "        \n",
    "        # Replace inf with large values\n",
    "        X = X.replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        # Step 1: Remove zero variance features\n",
    "        variances = X.var()\n",
    "        zero_var_cols = variances[variances == 0].index.tolist()\n",
    "        X = X.drop(columns=zero_var_cols)\n",
    "        \n",
    "        if X.empty:\n",
    "            self.selected_features = []\n",
    "            self.is_fitted = True\n",
    "            return self\n",
    "        \n",
    "        # Step 2: Remove highly correlated features\n",
    "        corr_matrix = X.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        high_corr_cols = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
    "        X = X.drop(columns=high_corr_cols[:len(high_corr_cols)//2])  # Keep half\n",
    "        \n",
    "        # Step 3: Statistical feature selection\n",
    "        if self.problem_type == ProblemType.REGRESSION:\n",
    "            score_func = f_regression\n",
    "        else:\n",
    "            score_func = f_classif\n",
    "        \n",
    "        k = self.max_features or min(20, len(X.columns))\n",
    "        k = min(k, len(X.columns))\n",
    "        \n",
    "        selector = SelectKBest(score_func=score_func, k=k)\n",
    "        selector.fit(X, y)\n",
    "        \n",
    "        # Get feature scores\n",
    "        scores = selector.scores_\n",
    "        for i, col in enumerate(X.columns):\n",
    "            self.feature_importance[col] = scores[i] if not np.isnan(scores[i]) else 0\n",
    "        \n",
    "        # Step 4: Model-based selection (Random Forest)\n",
    "        if self.problem_type == ProblemType.REGRESSION:\n",
    "            rf = RandomForestRegressor(n_estimators=50, max_depth=5, random_state=42, n_jobs=-1)\n",
    "        else:\n",
    "            rf = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42, n_jobs=-1)\n",
    "        \n",
    "        rf.fit(X, y)\n",
    "        rf_importance = dict(zip(X.columns, rf.feature_importances_))\n",
    "        \n",
    "        # Combine scores (normalize and average)\n",
    "        for col in X.columns:\n",
    "            stat_score = self.feature_importance.get(col, 0)\n",
    "            rf_score = rf_importance.get(col, 0)\n",
    "            \n",
    "            # Normalize\n",
    "            max_stat = max(self.feature_importance.values()) or 1\n",
    "            max_rf = max(rf_importance.values()) or 1\n",
    "            \n",
    "            combined = (stat_score / max_stat + rf_score / max_rf) / 2\n",
    "            self.feature_importance[col] = combined\n",
    "        \n",
    "        # Select top features\n",
    "        sorted_features = sorted(self.feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.selected_features = [f[0] for f in sorted_features[:k]]\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Select only the chosen features.\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Selector not fitted. Call fit() first.\")\n",
    "        \n",
    "        available = [f for f in self.selected_features if f in X.columns]\n",
    "        return X[available]\n",
    "    \n",
    "    def fit_transform(self, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"Fit and transform in one step.\"\"\"\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def get_importance_df(self) -> pd.DataFrame:\n",
    "        \"\"\"Get feature importance as DataFrame.\"\"\"\n",
    "        return pd.DataFrame([\n",
    "            {'feature': k, 'importance': v}\n",
    "            for k, v in sorted(self.feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        ])\n",
    "\n",
    "print(\"AutoFeatureSelector created!\")\n",
    "print(\"\\nMethods used:\")\n",
    "print(\"  - Variance threshold\")\n",
    "print(\"  - Correlation filtering\")\n",
    "print(\"  - Statistical tests (F-test)\")\n",
    "print(\"  - Random Forest importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple models automatically.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, problem_type: ProblemType, config: AutoMLConfig = None):\n",
    "        self.problem_type = problem_type\n",
    "        self.config = config or AutoMLConfig()\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.best_model_name = None\n",
    "        self.best_model = None\n",
    "    \n",
    "    def get_models(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get dictionary of models to train.\n",
    "        \"\"\"\n",
    "        if self.problem_type == ProblemType.REGRESSION:\n",
    "            models = {\n",
    "                'Ridge': Ridge(random_state=self.config.random_state),\n",
    "                'Lasso': Lasso(random_state=self.config.random_state),\n",
    "                'ElasticNet': ElasticNet(random_state=self.config.random_state),\n",
    "                'RandomForest': RandomForestRegressor(\n",
    "                    n_estimators=100, random_state=self.config.random_state, n_jobs=-1\n",
    "                ),\n",
    "                'GradientBoosting': GradientBoostingRegressor(\n",
    "                    n_estimators=100, random_state=self.config.random_state\n",
    "                ),\n",
    "                'XGBoost': xgb.XGBRegressor(\n",
    "                    n_estimators=100, random_state=self.config.random_state, n_jobs=-1\n",
    "                ),\n",
    "                'LightGBM': lgb.LGBMRegressor(\n",
    "                    n_estimators=100, random_state=self.config.random_state, n_jobs=-1, verbose=-1\n",
    "                ),\n",
    "            }\n",
    "            if not self.config.quick_mode:\n",
    "                models.update({\n",
    "                    'ExtraTrees': ExtraTreesRegressor(\n",
    "                        n_estimators=100, random_state=self.config.random_state, n_jobs=-1\n",
    "                    ),\n",
    "                    'KNN': KNeighborsRegressor(n_jobs=-1),\n",
    "                    'SVR': SVR(),\n",
    "                })\n",
    "        else:\n",
    "            models = {\n",
    "                'LogisticRegression': LogisticRegression(\n",
    "                    random_state=self.config.random_state, max_iter=1000, n_jobs=-1\n",
    "                ),\n",
    "                'RandomForest': RandomForestClassifier(\n",
    "                    n_estimators=100, random_state=self.config.random_state, n_jobs=-1\n",
    "                ),\n",
    "                'GradientBoosting': GradientBoostingClassifier(\n",
    "                    n_estimators=100, random_state=self.config.random_state\n",
    "                ),\n",
    "                'XGBoost': xgb.XGBClassifier(\n",
    "                    n_estimators=100, random_state=self.config.random_state, \n",
    "                    n_jobs=-1, eval_metric='logloss'\n",
    "                ),\n",
    "                'LightGBM': lgb.LGBMClassifier(\n",
    "                    n_estimators=100, random_state=self.config.random_state, \n",
    "                    n_jobs=-1, verbose=-1\n",
    "                ),\n",
    "            }\n",
    "            if not self.config.quick_mode:\n",
    "                models.update({\n",
    "                    'ExtraTrees': ExtraTreesClassifier(\n",
    "                        n_estimators=100, random_state=self.config.random_state, n_jobs=-1\n",
    "                    ),\n",
    "                    'KNN': KNeighborsClassifier(n_jobs=-1),\n",
    "                    'NaiveBayes': GaussianNB(),\n",
    "                    'SVC': SVC(probability=True, random_state=self.config.random_state),\n",
    "                })\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def train_all(self, X_train: pd.DataFrame, y_train: pd.Series,\n",
    "                  X_val: pd.DataFrame = None, y_val: pd.Series = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Train all models and evaluate.\n",
    "        \"\"\"\n",
    "        self.models = self.get_models()\n",
    "        \n",
    "        # Ensure numeric\n",
    "        X_train = X_train.select_dtypes(include=[np.number])\n",
    "        if X_val is not None:\n",
    "            X_val = X_val.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Handle any remaining issues\n",
    "        X_train = X_train.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "        if X_val is not None:\n",
    "            X_val = X_val.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        print(f\"Training {len(self.models)} models...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Train\n",
    "                model.fit(X_train, y_train)\n",
    "                train_time = time.time() - start_time\n",
    "                \n",
    "                # Evaluate\n",
    "                if X_val is not None:\n",
    "                    scores = self._evaluate_model(model, X_val, y_val)\n",
    "                else:\n",
    "                    # Use cross-validation\n",
    "                    scores = self._cross_validate(model, X_train, y_train)\n",
    "                \n",
    "                scores['train_time'] = train_time\n",
    "                self.results[name] = scores\n",
    "                \n",
    "                # Print progress\n",
    "                metric = 'r2' if self.problem_type == ProblemType.REGRESSION else 'accuracy'\n",
    "                print(f\"  {name:20} | {metric}: {scores[metric]:.4f} | Time: {train_time:.2f}s\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {name:20} | ERROR: {str(e)[:30]}\")\n",
    "                self.results[name] = {'error': str(e)}\n",
    "        \n",
    "        # Find best model\n",
    "        self._select_best_model()\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def _evaluate_model(self, model, X, y) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate model on validation set.\n",
    "        \"\"\"\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        if self.problem_type == ProblemType.REGRESSION:\n",
    "            return {\n",
    "                'r2': r2_score(y, y_pred),\n",
    "                'rmse': np.sqrt(mean_squared_error(y, y_pred)),\n",
    "                'mae': mean_absolute_error(y, y_pred)\n",
    "            }\n",
    "        else:\n",
    "            scores = {\n",
    "                'accuracy': accuracy_score(y, y_pred),\n",
    "                'f1': f1_score(y, y_pred, average='weighted'),\n",
    "                'precision': precision_score(y, y_pred, average='weighted'),\n",
    "                'recall': recall_score(y, y_pred, average='weighted')\n",
    "            }\n",
    "            \n",
    "            # ROC-AUC for binary\n",
    "            if self.problem_type == ProblemType.BINARY_CLASSIFICATION:\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    y_proba = model.predict_proba(X)[:, 1]\n",
    "                    scores['roc_auc'] = roc_auc_score(y, y_proba)\n",
    "            \n",
    "            return scores\n",
    "    \n",
    "    def _cross_validate(self, model, X, y) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform cross-validation.\n",
    "        \"\"\"\n",
    "        if self.problem_type == ProblemType.REGRESSION:\n",
    "            scoring = 'r2'\n",
    "            cv = KFold(n_splits=self.config.cv_folds, shuffle=True, random_state=self.config.random_state)\n",
    "        else:\n",
    "            scoring = 'accuracy'\n",
    "            cv = StratifiedKFold(n_splits=self.config.cv_folds, shuffle=True, random_state=self.config.random_state)\n",
    "        \n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "        \n",
    "        if self.problem_type == ProblemType.REGRESSION:\n",
    "            return {'r2': scores.mean(), 'r2_std': scores.std()}\n",
    "        else:\n",
    "            return {'accuracy': scores.mean(), 'accuracy_std': scores.std()}\n",
    "    \n",
    "    def _select_best_model(self):\n",
    "        \"\"\"\n",
    "        Select the best performing model.\n",
    "        \"\"\"\n",
    "        metric = 'r2' if self.problem_type == ProblemType.REGRESSION else 'accuracy'\n",
    "        \n",
    "        best_score = -np.inf\n",
    "        for name, scores in self.results.items():\n",
    "            if 'error' not in scores and scores.get(metric, -np.inf) > best_score:\n",
    "                best_score = scores[metric]\n",
    "                self.best_model_name = name\n",
    "        \n",
    "        if self.best_model_name:\n",
    "            self.best_model = self.models[self.best_model_name]\n",
    "            print(f\"\\nBest Model: {self.best_model_name} ({metric}: {best_score:.4f})\")\n",
    "    \n",
    "    def get_leaderboard(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get model comparison leaderboard.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        for name, scores in self.results.items():\n",
    "            if 'error' not in scores:\n",
    "                row = {'Model': name, **scores}\n",
    "                rows.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(rows)\n",
    "        \n",
    "        # Sort by primary metric\n",
    "        metric = 'r2' if self.problem_type == ProblemType.REGRESSION else 'accuracy'\n",
    "        if metric in df.columns:\n",
    "            df = df.sort_values(metric, ascending=False)\n",
    "        \n",
    "        return df\n",
    "\n",
    "print(\"ModelTrainer created!\")\n",
    "print(f\"\\nClassification models: LogisticRegression, RandomForest, GradientBoosting, XGBoost, LightGBM, ...\")\n",
    "print(f\"Regression models: Ridge, Lasso, ElasticNet, RandomForest, XGBoost, LightGBM, ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterTuner:\n",
    "    \"\"\"\n",
    "    Automatic hyperparameter tuning using Optuna or GridSearch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, problem_type: ProblemType, n_trials: int = 50):\n",
    "        self.problem_type = problem_type\n",
    "        self.n_trials = n_trials\n",
    "        self.best_params = {}\n",
    "        self.best_model = None\n",
    "    \n",
    "    def tune(self, model_name: str, X_train: pd.DataFrame, y_train: pd.Series,\n",
    "             X_val: pd.DataFrame = None, y_val: pd.Series = None):\n",
    "        \"\"\"\n",
    "        Tune hyperparameters for specified model.\n",
    "        \"\"\"\n",
    "        # Ensure numeric\n",
    "        X_train = X_train.select_dtypes(include=[np.number]).fillna(0).replace([np.inf, -np.inf], 0)\n",
    "        if X_val is not None:\n",
    "            X_val = X_val.select_dtypes(include=[np.number]).fillna(0).replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        if OPTUNA_AVAILABLE:\n",
    "            return self._tune_optuna(model_name, X_train, y_train, X_val, y_val)\n",
    "        else:\n",
    "            return self._tune_gridsearch(model_name, X_train, y_train)\n",
    "    \n",
    "    def _tune_optuna(self, model_name: str, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Tune using Optuna (Bayesian optimization).\n",
    "        \"\"\"\n",
    "        def objective(trial):\n",
    "            params = self._get_param_space_optuna(model_name, trial)\n",
    "            model = self._create_model(model_name, params)\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            if X_val is not None:\n",
    "                y_pred = model.predict(X_val)\n",
    "                y_true = y_val\n",
    "            else:\n",
    "                y_pred = model.predict(X_train)\n",
    "                y_true = y_train\n",
    "            \n",
    "            if self.problem_type == ProblemType.REGRESSION:\n",
    "                return r2_score(y_true, y_pred)\n",
    "            else:\n",
    "                return accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # Run optimization\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=self.n_trials, show_progress_bar=True)\n",
    "        \n",
    "        self.best_params = study.best_params\n",
    "        self.best_model = self._create_model(model_name, self.best_params)\n",
    "        self.best_model.fit(X_train, y_train)\n",
    "        \n",
    "        return self.best_model, self.best_params, study.best_value\n",
    "    \n",
    "    def _tune_gridsearch(self, model_name: str, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Tune using GridSearchCV (fallback).\n",
    "        \"\"\"\n",
    "        param_grid = self._get_param_space_grid(model_name)\n",
    "        model = self._create_model(model_name, {})\n",
    "        \n",
    "        scoring = 'r2' if self.problem_type == ProblemType.REGRESSION else 'accuracy'\n",
    "        \n",
    "        search = RandomizedSearchCV(\n",
    "            model, param_grid, n_iter=min(20, self.n_trials),\n",
    "            scoring=scoring, cv=3, n_jobs=-1, random_state=42\n",
    "        )\n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        self.best_params = search.best_params_\n",
    "        self.best_model = search.best_estimator_\n",
    "        \n",
    "        return self.best_model, self.best_params, search.best_score_\n",
    "    \n",
    "    def _get_param_space_optuna(self, model_name: str, trial) -> Dict:\n",
    "        \"\"\"\n",
    "        Define parameter search space for Optuna.\n",
    "        \"\"\"\n",
    "        spaces = {\n",
    "            'RandomForest': {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            },\n",
    "            'XGBoost': {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            },\n",
    "            'LightGBM': {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            },\n",
    "            'GradientBoosting': {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            },\n",
    "        }\n",
    "        return spaces.get(model_name, {})\n",
    "    \n",
    "    def _get_param_space_grid(self, model_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Define parameter grid for GridSearch.\n",
    "        \"\"\"\n",
    "        spaces = {\n",
    "            'RandomForest': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [5, 10, 15, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "            },\n",
    "            'XGBoost': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [3, 6, 10],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "            },\n",
    "            'LightGBM': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [3, 6, 10],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "            },\n",
    "        }\n",
    "        return spaces.get(model_name, {})\n",
    "    \n",
    "    def _create_model(self, model_name: str, params: Dict):\n",
    "        \"\"\"\n",
    "        Create model instance with given parameters.\n",
    "        \"\"\"\n",
    "        if self.problem_type == ProblemType.REGRESSION:\n",
    "            models = {\n",
    "                'RandomForest': RandomForestRegressor(random_state=42, n_jobs=-1, **params),\n",
    "                'XGBoost': xgb.XGBRegressor(random_state=42, n_jobs=-1, **params),\n",
    "                'LightGBM': lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1, **params),\n",
    "                'GradientBoosting': GradientBoostingRegressor(random_state=42, **params),\n",
    "            }\n",
    "        else:\n",
    "            models = {\n",
    "                'RandomForest': RandomForestClassifier(random_state=42, n_jobs=-1, **params),\n",
    "                'XGBoost': xgb.XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss', **params),\n",
    "                'LightGBM': lgb.LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1, **params),\n",
    "                'GradientBoosting': GradientBoostingClassifier(random_state=42, **params),\n",
    "            }\n",
    "        return models.get(model_name)\n",
    "\n",
    "print(\"HyperparameterTuner created!\")\n",
    "print(f\"\\nOptuna available: {OPTUNA_AVAILABLE}\")\n",
    "print(\"Supports: RandomForest, XGBoost, LightGBM, GradientBoosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensembler:\n",
    "    \"\"\"\n",
    "    Create ensemble models from trained base models.\n",
    "    \n",
    "    Methods:\n",
    "    - Voting (average predictions)\n",
    "    - Stacking (meta-learner)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, problem_type: ProblemType):\n",
    "        self.problem_type = problem_type\n",
    "        self.ensemble_model = None\n",
    "    \n",
    "    def create_voting_ensemble(self, models: Dict, weights: List[float] = None):\n",
    "        \"\"\"\n",
    "        Create voting ensemble from models.\n",
    "        \"\"\"\n",
    "        estimators = [(name, model) for name, model in models.items()]\n",
    "        \n",
    "        if self.problem_type == ProblemType.REGRESSION:\n",
    "            self.ensemble_model = VotingRegressor(estimators=estimators, weights=weights, n_jobs=-1)\n",
    "        else:\n",
    "            self.ensemble_model = VotingClassifier(\n",
    "                estimators=estimators, voting='soft', weights=weights, n_jobs=-1\n",
    "            )\n",
    "        \n",
    "        return self.ensemble_model\n",
    "    \n",
    "    def create_stacking_ensemble(self, models: Dict, meta_model=None):\n",
    "        \"\"\"\n",
    "        Create stacking ensemble with meta-learner.\n",
    "        \"\"\"\n",
    "        estimators = [(name, model) for name, model in models.items()]\n",
    "        \n",
    "        if self.problem_type == ProblemType.REGRESSION:\n",
    "            final_estimator = meta_model or Ridge()\n",
    "            self.ensemble_model = StackingRegressor(\n",
    "                estimators=estimators, final_estimator=final_estimator, n_jobs=-1\n",
    "            )\n",
    "        else:\n",
    "            final_estimator = meta_model or LogisticRegression(max_iter=1000)\n",
    "            self.ensemble_model = StackingClassifier(\n",
    "                estimators=estimators, final_estimator=final_estimator, n_jobs=-1\n",
    "            )\n",
    "        \n",
    "        return self.ensemble_model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit ensemble model.\"\"\"\n",
    "        if self.ensemble_model is None:\n",
    "            raise ValueError(\"Create ensemble first using create_voting_ensemble or create_stacking_ensemble\")\n",
    "        self.ensemble_model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return self.ensemble_model.predict(X)\n",
    "\n",
    "print(\"Ensembler created!\")\n",
    "print(\"\\nMethods: Voting Ensemble, Stacking Ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Complete AutoML Class\n",
    "\n",
    "Now let's combine everything into a single, easy-to-use AutoML class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoML:\n",
    "    \"\"\"\n",
    "    Complete AutoML System.\n",
    "    \n",
    "    Automatically handles:\n",
    "    - Data type detection\n",
    "    - EDA\n",
    "    - Preprocessing\n",
    "    - Feature engineering\n",
    "    - Feature selection\n",
    "    - Model training & comparison\n",
    "    - Hyperparameter tuning\n",
    "    - Ensemble creation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: AutoMLConfig = None):\n",
    "        self.config = config or AutoMLConfig()\n",
    "        \n",
    "        # Components\n",
    "        self.type_detector = DataTypeDetector()\n",
    "        self.eda = AutoEDA()\n",
    "        self.preprocessor = AutoPreprocessor(self.config)\n",
    "        self.feature_engineer = AutoFeatureEngineer(self.config)\n",
    "        self.feature_selector = None\n",
    "        self.model_trainer = None\n",
    "        self.tuner = None\n",
    "        self.ensembler = None\n",
    "        \n",
    "        # State\n",
    "        self.problem_type = None\n",
    "        self.column_types = {}\n",
    "        self.target_col = None\n",
    "        self.best_model = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        # Results\n",
    "        self.eda_report = {}\n",
    "        self.model_results = {}\n",
    "        self.leaderboard = None\n",
    "    \n",
    "    def fit(self, df: pd.DataFrame, target_col: str, \n",
    "            tune_best: bool = True, create_ensemble: bool = True) -> 'AutoML':\n",
    "        \"\"\"\n",
    "        Fit AutoML pipeline on data.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            target_col: Name of target column\n",
    "            tune_best: Whether to tune best model's hyperparameters\n",
    "            create_ensemble: Whether to create ensemble of top models\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"AUTOML PIPELINE STARTING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.target_col = target_col\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Step 1: Detect problem type\n",
    "        print(\"\\n[1/8] Detecting problem type...\")\n",
    "        self.problem_type = self.type_detector.detect_problem_type(df[target_col])\n",
    "        print(f\"  Problem type: {self.problem_type.value}\")\n",
    "        \n",
    "        # Step 2: Detect column types\n",
    "        print(\"\\n[2/8] Detecting column types...\")\n",
    "        self.column_types = self.type_detector.detect_column_types(df, target_col)\n",
    "        summary = self.type_detector.get_summary(self.column_types)\n",
    "        for col_type, cols in summary.items():\n",
    "            print(f\"  {col_type}: {len(cols)} columns\")\n",
    "        \n",
    "        # Step 3: EDA\n",
    "        print(\"\\n[3/8] Performing EDA...\")\n",
    "        self.eda_report = self.eda.analyze(df, target_col)\n",
    "        print(f\"  Missing values: {self.eda_report['missing_values']['total_missing']}\")\n",
    "        print(f\"  Duplicates: {self.eda_report['duplicates']['n_duplicates']}\")\n",
    "        \n",
    "        # Step 4: Split data\n",
    "        print(\"\\n[4/8] Splitting data...\")\n",
    "        X = df.drop(columns=[target_col])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Encode target if needed\n",
    "        if self.problem_type != ProblemType.REGRESSION:\n",
    "            if y.dtype == 'object':\n",
    "                self.target_encoder = LabelEncoder()\n",
    "                y = pd.Series(self.target_encoder.fit_transform(y), index=y.index)\n",
    "            else:\n",
    "                self.target_encoder = None\n",
    "        else:\n",
    "            self.target_encoder = None\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=self.config.test_size, random_state=self.config.random_state,\n",
    "            stratify=y if self.problem_type != ProblemType.REGRESSION else None\n",
    "        )\n",
    "        print(f\"  Train: {len(X_train):,} samples\")\n",
    "        print(f\"  Test: {len(X_test):,} samples\")\n",
    "        \n",
    "        # Step 5: Preprocess\n",
    "        print(\"\\n[5/8] Preprocessing data...\")\n",
    "        X_train = self.preprocessor.fit_transform(X_train, self.column_types)\n",
    "        X_test = self.preprocessor.transform(X_test)\n",
    "        print(f\"  Features after preprocessing: {X_train.shape[1]}\")\n",
    "        \n",
    "        # Step 6: Feature engineering\n",
    "        print(\"\\n[6/8] Engineering features...\")\n",
    "        X_train = self.feature_engineer.create_features(X_train, self.column_types)\n",
    "        X_test = self.feature_engineer.create_features(X_test, self.column_types)\n",
    "        print(f\"  New features created: {len(self.feature_engineer.created_features)}\")\n",
    "        print(f\"  Total features: {X_train.shape[1]}\")\n",
    "        \n",
    "        # Step 7: Feature selection\n",
    "        if self.config.feature_selection:\n",
    "            print(\"\\n[7/8] Selecting features...\")\n",
    "            self.feature_selector = AutoFeatureSelector(\n",
    "                self.problem_type, self.config.max_features\n",
    "            )\n",
    "            X_train = self.feature_selector.fit_transform(X_train, y_train)\n",
    "            X_test = self.feature_selector.transform(X_test)\n",
    "            print(f\"  Selected features: {len(self.feature_selector.selected_features)}\")\n",
    "        else:\n",
    "            print(\"\\n[7/8] Skipping feature selection...\")\n",
    "        \n",
    "        # Step 8: Train models\n",
    "        print(\"\\n[8/8] Training models...\")\n",
    "        self.model_trainer = ModelTrainer(self.problem_type, self.config)\n",
    "        self.model_results = self.model_trainer.train_all(X_train, y_train, X_test, y_test)\n",
    "        self.leaderboard = self.model_trainer.get_leaderboard()\n",
    "        self.best_model = self.model_trainer.best_model\n",
    "        \n",
    "        # Optional: Tune best model\n",
    "        if tune_best and self.config.tune_hyperparameters:\n",
    "            print(\"\\n[BONUS] Tuning best model hyperparameters...\")\n",
    "            self.tuner = HyperparameterTuner(self.problem_type, self.config.tuning_trials)\n",
    "            try:\n",
    "                tuned_model, best_params, best_score = self.tuner.tune(\n",
    "                    self.model_trainer.best_model_name, X_train, y_train, X_test, y_test\n",
    "                )\n",
    "                print(f\"  Best params: {best_params}\")\n",
    "                print(f\"  Tuned score: {best_score:.4f}\")\n",
    "                self.best_model = tuned_model\n",
    "            except Exception as e:\n",
    "                print(f\"  Tuning failed: {e}\")\n",
    "        \n",
    "        # Optional: Create ensemble\n",
    "        if create_ensemble and self.config.create_ensemble:\n",
    "            print(\"\\n[BONUS] Creating ensemble...\")\n",
    "            try:\n",
    "                # Get top 3 models\n",
    "                top_models = {name: self.model_trainer.models[name] \n",
    "                             for name in self.leaderboard['Model'].head(3).tolist()\n",
    "                             if name in self.model_trainer.models}\n",
    "                \n",
    "                if len(top_models) >= 2:\n",
    "                    self.ensembler = Ensembler(self.problem_type)\n",
    "                    self.ensembler.create_voting_ensemble(top_models)\n",
    "                    self.ensembler.fit(X_train, y_train)\n",
    "                    \n",
    "                    # Evaluate ensemble\n",
    "                    y_pred = self.ensembler.predict(X_test)\n",
    "                    if self.problem_type == ProblemType.REGRESSION:\n",
    "                        score = r2_score(y_test, y_pred)\n",
    "                        print(f\"  Ensemble R2: {score:.4f}\")\n",
    "                    else:\n",
    "                        score = accuracy_score(y_test, y_pred)\n",
    "                        print(f\"  Ensemble Accuracy: {score:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Ensemble creation failed: {e}\")\n",
    "        \n",
    "        # Store test data for later use\n",
    "        self._X_test = X_test\n",
    "        self._y_test = y_test\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"AUTOML COMPLETE! Total time: {total_time:.1f}s\")\n",
    "        print(f\"Best Model: {self.model_trainer.best_model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"AutoML not fitted. Call fit() first.\")\n",
    "        \n",
    "        # Preprocess\n",
    "        X = self.preprocessor.transform(df)\n",
    "        X = self.feature_engineer.create_features(X, self.column_types)\n",
    "        \n",
    "        if self.feature_selector:\n",
    "            X = self.feature_selector.transform(X)\n",
    "        \n",
    "        # Ensure numeric\n",
    "        X = X.select_dtypes(include=[np.number]).fillna(0).replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = self.best_model.predict(X)\n",
    "        \n",
    "        # Decode target if needed\n",
    "        if self.target_encoder:\n",
    "            predictions = self.target_encoder.inverse_transform(predictions.astype(int))\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def get_leaderboard(self) -> pd.DataFrame:\n",
    "        \"\"\"Get model comparison leaderboard.\"\"\"\n",
    "        return self.leaderboard\n",
    "    \n",
    "    def get_feature_importance(self) -> pd.DataFrame:\n",
    "        \"\"\"Get feature importance from best model.\"\"\"\n",
    "        if hasattr(self.best_model, 'feature_importances_'):\n",
    "            if self.feature_selector:\n",
    "                features = self.feature_selector.selected_features\n",
    "            else:\n",
    "                features = list(range(len(self.best_model.feature_importances_)))\n",
    "            \n",
    "            return pd.DataFrame({\n",
    "                'feature': features[:len(self.best_model.feature_importances_)],\n",
    "                'importance': self.best_model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "        return None\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate comprehensive AutoML report.\n",
    "        \"\"\"\n",
    "        report = []\n",
    "        report.append(\"=\"*60)\n",
    "        report.append(\"AUTOML REPORT\")\n",
    "        report.append(\"=\"*60)\n",
    "        \n",
    "        report.append(f\"\\nProblem Type: {self.problem_type.value}\")\n",
    "        report.append(f\"Target Column: {self.target_col}\")\n",
    "        \n",
    "        report.append(f\"\\n--- Data Summary ---\")\n",
    "        report.append(f\"Total samples: {self.eda_report['basic_info']['n_rows']:,}\")\n",
    "        report.append(f\"Total features: {self.eda_report['basic_info']['n_columns']}\")\n",
    "        report.append(f\"Missing values: {self.eda_report['missing_values']['total_missing']}\")\n",
    "        \n",
    "        report.append(f\"\\n--- Model Leaderboard ---\")\n",
    "        report.append(self.leaderboard.to_string())\n",
    "        \n",
    "        report.append(f\"\\n--- Best Model ---\")\n",
    "        report.append(f\"Model: {self.model_trainer.best_model_name}\")\n",
    "        \n",
    "        if self.tuner and self.tuner.best_params:\n",
    "            report.append(f\"\\n--- Tuned Hyperparameters ---\")\n",
    "            for k, v in self.tuner.best_params.items():\n",
    "                report.append(f\"  {k}: {v}\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AUTOML SYSTEM READY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "Usage:\n",
    "    automl = AutoML()\n",
    "    automl.fit(df, target_col='target')\n",
    "    predictions = automl.predict(new_df)\n",
    "    print(automl.get_leaderboard())\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Demo: AutoML in Action\n",
    "\n",
    "Let's test our AutoML system on real datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1: Classification (Titanic-style)\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "print(\"DEMO 1: Binary Classification (Breast Cancer Dataset)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "df_demo = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df_demo['target'] = data.target\n",
    "\n",
    "print(f\"Dataset shape: {df_demo.shape}\")\n",
    "print(f\"Target distribution: {df_demo['target'].value_counts().to_dict()}\")\n",
    "\n",
    "# Run AutoML\n",
    "config = AutoMLConfig(\n",
    "    quick_mode=True,  # Use fewer models for demo\n",
    "    tune_hyperparameters=True,\n",
    "    tuning_trials=20  # Fewer trials for demo\n",
    ")\n",
    "\n",
    "automl_clf = AutoML(config)\n",
    "automl_clf.fit(df_demo, target_col='target', tune_best=True, create_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show leaderboard\n",
    "print(\"\\nModel Leaderboard:\")\n",
    "print(automl_clf.get_leaderboard().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance_df = automl_clf.get_feature_importance()\n",
    "if importance_df is not None:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_features = importance_df.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'].values)\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 15 Feature Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: Regression\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEMO 2: Regression (California Housing Dataset)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load data (use subset for speed)\n",
    "data = fetch_california_housing()\n",
    "df_reg = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df_reg['target'] = data.target\n",
    "\n",
    "# Use subset\n",
    "df_reg = df_reg.sample(n=5000, random_state=42)\n",
    "\n",
    "print(f\"Dataset shape: {df_reg.shape}\")\n",
    "print(f\"Target range: {df_reg['target'].min():.2f} - {df_reg['target'].max():.2f}\")\n",
    "\n",
    "# Run AutoML\n",
    "config_reg = AutoMLConfig(\n",
    "    quick_mode=True,\n",
    "    tune_hyperparameters=True,\n",
    "    tuning_trials=20\n",
    ")\n",
    "\n",
    "automl_reg = AutoML(config_reg)\n",
    "automl_reg.fit(df_reg, target_col='target', tune_best=True, create_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show regression leaderboard\n",
    "print(\"\\nRegression Model Leaderboard:\")\n",
    "print(automl_reg.get_leaderboard().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate full report\n",
    "print(automl_clf.generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "### What We Built\n",
    "\n",
    "A complete **AutoML System** that automates the entire machine learning pipeline:\n",
    "\n",
    "| Component | Functionality |\n",
    "|-----------|---------------|\n",
    "| `DataTypeDetector` | Auto-detect column types and problem type |\n",
    "| `AutoEDA` | Automatic exploratory data analysis |\n",
    "| `AutoPreprocessor` | Missing values, encoding, scaling |\n",
    "| `AutoFeatureEngineer` | Create interaction and aggregation features |\n",
    "| `AutoFeatureSelector` | Select best features using multiple methods |\n",
    "| `ModelTrainer` | Train and compare 10+ models |\n",
    "| `HyperparameterTuner` | Bayesian optimization with Optuna |\n",
    "| `Ensembler` | Create voting and stacking ensembles |\n",
    "| `AutoML` | Main class combining everything |\n",
    "\n",
    "### Models Supported\n",
    "\n",
    "**Classification:**\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- Extra Trees\n",
    "- KNN\n",
    "- Naive Bayes\n",
    "- SVM\n",
    "\n",
    "**Regression:**\n",
    "- Ridge, Lasso, ElasticNet\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- Extra Trees\n",
    "- KNN\n",
    "- SVR\n",
    "\n",
    "### Usage\n",
    "\n",
    "```python\n",
    "# Simple usage\n",
    "automl = AutoML()\n",
    "automl.fit(df, target_col='target')\n",
    "predictions = automl.predict(new_df)\n",
    "\n",
    "# Get results\n",
    "print(automl.get_leaderboard())\n",
    "print(automl.generate_report())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"AUTOML SYSTEM - COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "Components Built:\n",
    "─────────────────\n",
    "1. DataTypeDetector   - Auto-detect data types\n",
    "2. AutoEDA            - Automatic EDA\n",
    "3. AutoPreprocessor   - Handle missing, encode, scale\n",
    "4. AutoFeatureEngineer - Create new features\n",
    "5. AutoFeatureSelector - Select best features\n",
    "6. ModelTrainer       - Train multiple models\n",
    "7. HyperparameterTuner - Bayesian optimization\n",
    "8. Ensembler          - Voting & Stacking\n",
    "9. AutoML             - Main orchestrator\n",
    "\n",
    "Capabilities:\n",
    "─────────────\n",
    "• Classification (binary & multiclass)\n",
    "• Regression\n",
    "• Auto preprocessing\n",
    "• Auto feature engineering\n",
    "• Auto feature selection\n",
    "• 10+ model comparison\n",
    "• Hyperparameter tuning (Optuna)\n",
    "• Ensemble methods\n",
    "• Report generation\n",
    "\n",
    "This AutoML system combines EVERYTHING learned\n",
    "in the previous 19 projects into one powerful tool!\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
