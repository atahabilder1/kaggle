{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis - Understanding Emotions at Scale\n",
    "\n",
    "**Author:** Anik Tahabilder  \n",
    "**Project:** 7 of 22 - Kaggle ML Portfolio  \n",
    "**Dataset:** Sentiment140 (1.6 Million Tweets)  \n",
    "**Difficulty:** 5/10 | **Resume Value:** 7/10 | **Learning Value:** 8/10 | **Impact:** 8/10\n",
    "\n",
    "---\n",
    "\n",
    "## What is Sentiment Analysis?\n",
    "\n",
    "**Sentiment Analysis** (also called opinion mining) is the process of computationally determining whether a piece of text expresses a **positive**, **negative**, or **neutral** opinion.\n",
    "\n",
    "In this notebook, we'll build models that can automatically detect if a tweet expresses:\n",
    "- ðŸ˜Š **Positive sentiment** (happy, excited, satisfied)\n",
    "- ðŸ˜ž **Negative sentiment** (angry, sad, disappointed)\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "| Industry | Application | Example |\n",
    "|----------|-------------|----------|\n",
    "| **Social Media** | Monitor brand reputation | Track customer opinions about products |\n",
    "| **Customer Service** | Prioritize urgent issues | Identify angry customers for immediate response |\n",
    "| **Finance** | Market sentiment analysis | Predict stock movements from news/tweets |\n",
    "| **Politics** | Public opinion polling | Gauge voter sentiment during elections |\n",
    "| **Product Development** | Feature feedback | Understand what users love/hate |\n",
    "| **E-commerce** | Review analysis | Summarize thousands of product reviews |\n",
    "\n",
    "### Why is Sentiment Analysis Challenging?\n",
    "\n",
    "Natural language is complex and ambiguous:\n",
    "- **Sarcasm**: \"Great, another bug!\" (negative despite \"great\")\n",
    "- **Context**: \"This movie is sick!\" (positive in slang, negative literally)\n",
    "- **Negation**: \"not bad\" (positive) vs \"bad\" (negative)\n",
    "- **Emojis**: ðŸ˜‚ can mean funny OR mocking\n",
    "- **Slang & Abbreviations**: \"omg so lit\" vs formal language\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: Understanding Sentiment Analysis & NLP](#part1)\n",
    "2. [Part 2: Setup and Data Loading](#part2)\n",
    "3. [Part 3: Exploratory Data Analysis](#part3)\n",
    "4. [Part 4: Text Preprocessing Pipeline](#part4)\n",
    "5. [Part 5: Feature Extraction Methods](#part5)\n",
    "6. [Part 6: Classical Machine Learning Models](#part6)\n",
    "7. [Part 7: Deep Learning with LSTM](#part7)\n",
    "8. [Part 8: Model Comparison](#part8)\n",
    "9. [Part 9: Word Clouds & Visualization](#part9)\n",
    "10. [Part 10: Predictions on New Tweets](#part10)\n",
    "11. [Part 11: Summary and Key Takeaways](#part11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part1'></a>\n",
    "# Part 1: Understanding Sentiment Analysis & NLP\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 What is Natural Language Processing (NLP)?\n",
    "\n",
    "**Natural Language Processing** is a branch of AI focused on enabling computers to understand, interpret, and generate human language.\n",
    "\n",
    "### NLP Task Hierarchy:\n",
    "\n",
    "| Task | Goal | Difficulty |\n",
    "|------|------|------------|\n",
    "| **Tokenization** | Split text into words | Easy |\n",
    "| **Sentiment Analysis** | Classify emotion | Medium |\n",
    "| **Named Entity Recognition** | Extract names, places, organizations | Medium-Hard |\n",
    "| **Machine Translation** | Translate between languages | Hard |\n",
    "| **Question Answering** | Answer questions about text | Very Hard |\n",
    "\n",
    "### The NLP Pipeline:\n",
    "\n",
    "```\n",
    "Raw Text â†’ Preprocessing â†’ Feature Extraction â†’ Model â†’ Prediction\n",
    "\"I love this!\"\n",
    "    â†“                â†“                  â†“             â†“\n",
    "Clean text     [love, this]      [0.2, 0.8, ...]   Positive!\n",
    "```\n",
    "\n",
    "## 1.2 Sentiment Analysis vs Other NLP Tasks\n",
    "\n",
    "| Task Type | Input | Output | Example |\n",
    "|-----------|-------|--------|----------|\n",
    "| **Sentiment Analysis** | Text | Sentiment label | \"I hate this\" â†’ Negative |\n",
    "| **Topic Classification** | Text | Topic category | \"iPhone 15 released\" â†’ Technology |\n",
    "| **Spam Detection** | Email | Spam/Not Spam | \"Win $1M now!\" â†’ Spam |\n",
    "| **Text Generation** | Prompt | Generated text | \"Once upon a time\" â†’ Story |\n",
    "\n",
    "## 1.3 Our Approach\n",
    "\n",
    "We'll build sentiment analysis models using:\n",
    "1. **Classical ML**: Logistic Regression, Naive Bayes, SVM, Random Forest\n",
    "2. **Deep Learning**: LSTM (Long Short-Term Memory) networks\n",
    "\n",
    "Then compare their performance on 1.6 million tweets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part2'></a>\n",
    "# Part 2: Setup and Data Loading\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Import Required Libraries\n",
    "\n",
    "For sentiment analysis, we need:\n",
    "\n",
    "| Library | Purpose |\n",
    "|---------|----------|\n",
    "| **pandas/numpy** | Data manipulation |\n",
    "| **matplotlib/seaborn** | Visualization |\n",
    "| **nltk** | Natural Language Toolkit (preprocessing) |\n",
    "| **sklearn** | Classical ML algorithms |\n",
    "| **tensorflow/keras** | Deep learning (LSTM) |\n",
    "| **wordcloud** | Visualize word frequencies |\n",
    "| **re** | Regular expressions (text cleaning) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Data manipulation\nimport pandas as pd\nimport numpy as np\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Suppress TensorFlow warnings BEFORE importing\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\n# Text processing\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report, ConfusionMatrixDisplay\n)\n\n# Deep Learning\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# Utilities\nimport time\n\n# Download NLTK data\nprint(\"Downloading NLTK data...\")\nnltk.download('punkt', quiet=True)\nnltk.download('stopwords', quiet=True)\nnltk.download('wordnet', quiet=True)\nnltk.download('averaged_perceptron_tagger', quiet=True)\nnltk.download('punkt_tab', quiet=True)\nprint(\"NLTK data downloaded!\")\n\n# Display settings\nplt.style.use('seaborn-v0_8-whitegrid')\npd.set_option('display.max_colwidth', 100)\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SETUP COMPLETE\")\nprint(\"=\"*60)\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\n\n# Check GPU\ntry:\n    gpus = tf.config.list_physical_devices('GPU')\n    if gpus:\n        print(f\"\\nGPU Available: {len(gpus)} GPU(s)\")\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    else:\n        print(\"\\nNo GPU detected - using CPU\")\nexcept:\n    print(\"\\nUsing CPU for training\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Loading the Sentiment140 Dataset\n",
    "\n",
    "### About Sentiment140:\n",
    "\n",
    "- **Size**: 1.6 million tweets\n",
    "- **Source**: Twitter API (2009)\n",
    "- **Labels**: Binary (0 = negative, 4 = positive)\n",
    "- **Emoticons removed**: To prevent models from cheating\n",
    "- **Language**: English\n",
    "\n",
    "### Dataset Structure:\n",
    "\n",
    "| Column | Description | Example |\n",
    "|--------|-------------|----------|\n",
    "| **target** | Sentiment (0=negative, 4=positive) | 0 |\n",
    "| **ids** | Tweet ID | 1467810369 |\n",
    "| **date** | Timestamp | Mon Apr 06 22:19:45 PDT 2009 |\n",
    "| **flag** | Query (NO_QUERY if none) | NO_QUERY |\n",
    "| **user** | Username | scotthamilton |\n",
    "| **text** | Tweet content | @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Note: The dataset is encoded in latin-1, not UTF-8\n",
    "column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "try:\n",
    "    # For Kaggle environment\n",
    "    df = pd.read_csv(\n",
    "        '/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv',\n",
    "        encoding='latin-1',\n",
    "        names=column_names\n",
    "    )\n",
    "    print(\"Dataset loaded from Kaggle!\")\n",
    "except:\n",
    "    # For local environment (if you have the file)\n",
    "    df = pd.read_csv(\n",
    "        'training.1600000.processed.noemoticon.csv',\n",
    "        encoding='latin-1',\n",
    "        names=column_names\n",
    "    )\n",
    "    print(\"Dataset loaded from local file!\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SENTIMENT140 DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nShape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few tweets\n",
    "print(\"First 10 tweets:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*60)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Memory Usage:\")\n",
    "print(f\"Total: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Check:\")\n",
    "print(\"=\"*60)\n",
    "missing = df.isnull().sum()\n",
    "print(missing)\n",
    "print(f\"\\nTotal missing values: {missing.sum()}\")\n",
    "print(\"\\nGreat! No missing values in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target from (0, 4) to (0, 1) for easier interpretation\n",
    "# 0 = negative, 4 = positive â†’ 0 = negative, 1 = positive\n",
    "df['sentiment'] = df['target'].map({0: 0, 4: 1})\n",
    "\n",
    "# Verify the conversion\n",
    "print(\"Sentiment Conversion:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Original target values:\", df['target'].unique())\n",
    "print(\"New sentiment values:\", df['sentiment'].unique())\n",
    "print(\"\\n0 = Negative, 1 = Positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part3'></a>\n",
    "# Part 3: Exploratory Data Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Class Distribution\n",
    "\n",
    "**Critical Question**: Is our dataset balanced?\n",
    "\n",
    "An imbalanced dataset can cause:\n",
    "- Model bias toward majority class\n",
    "- Misleading accuracy metrics\n",
    "- Poor performance on minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "sentiment_counts = df['sentiment'].value_counts().sort_index()\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(\"=\"*60)\n",
    "print(sentiment_counts)\n",
    "print(f\"\\nNegative tweets (0): {sentiment_counts[0]:,} ({sentiment_counts[0]/len(df)*100:.2f}%)\")\n",
    "print(f\"Positive tweets (1): {sentiment_counts[1]:,} ({sentiment_counts[1]/len(df)*100:.2f}%)\")\n",
    "print(\"\\nThis is a PERFECTLY BALANCED dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "labels = ['Negative', 'Positive']\n",
    "bars = axes[0].bar(labels, sentiment_counts.values, color=colors, edgecolor='black')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Sentiment Distribution', fontweight='bold', fontsize=14)\n",
    "for bar, val in zip(bars, sentiment_counts.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10000,\n",
    "                 f'{val:,}', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(sentiment_counts.values, labels=labels, autopct='%1.1f%%',\n",
    "            colors=colors, explode=(0.02, 0.02), shadow=True,\n",
    "            textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Sentiment Proportion', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Perfect 50-50 split means no class imbalance to worry about!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Tweet Length Analysis\n",
    "\n",
    "Understanding tweet length helps us:\n",
    "- Set appropriate max length for deep learning models\n",
    "- Identify potential outliers\n",
    "- Understand data characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tweet lengths\n",
    "df['text_length'] = df['text'].astype(str).apply(len)\n",
    "df['word_count'] = df['text'].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "# Statistics by sentiment\n",
    "print(\"Tweet Length Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nCharacter Length:\")\n",
    "print(df.groupby('sentiment')['text_length'].describe())\n",
    "print(\"\\nWord Count:\")\n",
    "print(df.groupby('sentiment')['word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For speed, let's sample the data for visualization\n",
    "# (Otherwise plotting 1.6M points takes forever)\n",
    "sample_df = df.sample(n=50000, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Character length distribution\n",
    "for sentiment, color, label in zip([0, 1], ['#FF6B6B', '#4ECDC4'], ['Negative', 'Positive']):\n",
    "    subset = sample_df[sample_df['sentiment'] == sentiment]\n",
    "    axes[0, 0].hist(subset['text_length'], bins=50, alpha=0.6, color=color, label=label, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Character Length')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Tweet Character Length Distribution', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Word count distribution\n",
    "for sentiment, color, label in zip([0, 1], ['#FF6B6B', '#4ECDC4'], ['Negative', 'Positive']):\n",
    "    subset = sample_df[sample_df['sentiment'] == sentiment]\n",
    "    axes[0, 1].hist(subset['word_count'], bins=30, alpha=0.6, color=color, label=label, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Word Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Tweet Word Count Distribution', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Box plots\n",
    "sample_df.boxplot(column='text_length', by='sentiment', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Character Length by Sentiment', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Sentiment (0=Negative, 1=Positive)')\n",
    "axes[1, 0].set_ylabel('Character Length')\n",
    "\n",
    "sample_df.boxplot(column='word_count', by='sentiment', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Word Count by Sentiment', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Sentiment (0=Negative, 1=Positive)')\n",
    "axes[1, 1].set_ylabel('Word Count')\n",
    "\n",
    "plt.suptitle('Tweet Length Analysis (50K Sample)', fontweight='bold', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Most tweets are 100-140 characters (Twitter's old limit)\")\n",
    "print(\"- Average word count is around 15-20 words\")\n",
    "print(\"- Both sentiments have similar length distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Sample Tweets from Each Sentiment\n",
    "\n",
    "Let's look at actual examples to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample tweets from each sentiment\n",
    "print(\"SAMPLE NEGATIVE TWEETS (sentiment = 0):\")\n",
    "print(\"=\"*60)\n",
    "negative_samples = df[df['sentiment'] == 0]['text'].sample(10, random_state=42).values\n",
    "for i, tweet in enumerate(negative_samples, 1):\n",
    "    print(f\"{i}. {tweet}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE POSITIVE TWEETS (sentiment = 1):\")\n",
    "print(\"=\"*60)\n",
    "positive_samples = df[df['sentiment'] == 1]['text'].sample(10, random_state=42).values\n",
    "for i, tweet in enumerate(positive_samples, 1):\n",
    "    print(f\"{i}. {tweet}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Data Characteristics\n",
    "\n",
    "What patterns do we notice in the raw tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze tweet characteristics\n",
    "sample_analysis = df.sample(n=100000, random_state=42)\n",
    "\n",
    "# Count tweets with specific patterns\n",
    "patterns = {\n",
    "    'URLs': sample_analysis['text'].str.contains('http', case=False, na=False).sum(),\n",
    "    'Mentions (@)': sample_analysis['text'].str.contains('@', na=False).sum(),\n",
    "    'Hashtags (#)': sample_analysis['text'].str.contains('#', na=False).sum(),\n",
    "    'Numbers': sample_analysis['text'].str.contains(r'\\d', na=False).sum(),\n",
    "}\n",
    "\n",
    "print(\"Tweet Characteristics (100K sample):\")\n",
    "print(\"=\"*60)\n",
    "for pattern, count in patterns.items():\n",
    "    percentage = (count / len(sample_analysis)) * 100\n",
    "    print(f\"{pattern:<20}: {count:>7,} ({percentage:>5.1f}%)\")\n",
    "\n",
    "print(\"\\nWe'll need to clean these during preprocessing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part4'></a>\n",
    "# Part 4: Text Preprocessing Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 Why Preprocess Text?\n",
    "\n",
    "Raw text is messy and inconsistent. Preprocessing helps by:\n",
    "\n",
    "| Problem | Solution |\n",
    "|---------|----------|\n",
    "| **Case sensitivity** | \"Happy\" â‰  \"happy\" | Convert to lowercase |\n",
    "| **URLs** | \"http://bit.ly/abc\" adds noise | Remove URLs |\n",
    "| **Mentions** | \"@user\" varies by user | Remove @ mentions |\n",
    "| **Special characters** | \"!!!\" doesn't add meaning | Remove punctuation |\n",
    "| **Numbers** | Often not relevant | Remove digits |\n",
    "| **Stop words** | \"the\", \"is\", \"at\" appear everywhere | Remove common words |\n",
    "| **Word variations** | \"running\", \"runs\", \"ran\" | Stemming/Lemmatization |\n",
    "\n",
    "## 4.2 Preprocessing Steps\n",
    "\n",
    "Our pipeline will:\n",
    "1. **Lowercase** - Standardize case\n",
    "2. **Remove URLs** - Strip http:// links\n",
    "3. **Remove mentions** - Strip @username\n",
    "4. **Remove hashtags** - Strip # symbols (keep text)\n",
    "5. **Remove special characters** - Keep only letters\n",
    "6. **Remove numbers** - Strip digits\n",
    "7. **Tokenize** - Split into words\n",
    "8. **Remove stopwords** - Filter common words\n",
    "9. **Stem/Lemmatize** - Reduce words to root form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Stopwords - What Are They?\n",
    "\n",
    "**Stopwords** are common words that appear frequently but carry little meaning:\n",
    "- Articles: \"a\", \"an\", \"the\"\n",
    "- Pronouns: \"I\", \"you\", \"he\", \"she\"\n",
    "- Prepositions: \"in\", \"on\", \"at\", \"to\"\n",
    "- Conjunctions: \"and\", \"but\", \"or\"\n",
    "\n",
    "Removing them:\n",
    "- Reduces vocabulary size\n",
    "- Focuses on meaningful words\n",
    "- Speeds up training\n",
    "\n",
    "**However**: For sentiment, some stopwords matter!\n",
    "- \"not\" is critical for negation\n",
    "- \"very\" intensifies sentiment\n",
    "\n",
    "We'll use a custom stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NLTK stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove negation words from stopwords (they're important for sentiment!)\n",
    "negation_words = {'not', 'no', 'never', 'neither', 'nor', 'none', 'nobody', 'nothing'}\n",
    "stop_words = stop_words - negation_words\n",
    "\n",
    "print(f\"Total stopwords: {len(stop_words)}\")\n",
    "print(f\"\\nSample stopwords: {list(stop_words)[:20]}\")\n",
    "print(f\"\\nKept negation words: {negation_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Stemming vs Lemmatization\n",
    "\n",
    "Both reduce words to their base form, but differently:\n",
    "\n",
    "| Method | Approach | Example | Speed | Accuracy |\n",
    "|--------|----------|---------|-------|----------|\n",
    "| **Stemming** | Chop off suffixes | running â†’ run, studies â†’ studi | Fast | Less accurate |\n",
    "| **Lemmatization** | Use dictionary lookup | running â†’ run, studies â†’ study | Slow | More accurate |\n",
    "\n",
    "### Comparison:\n",
    "\n",
    "| Word | Stemming | Lemmatization |\n",
    "|------|----------|---------------|\n",
    "| studies | studi | study |\n",
    "| studying | study | study |\n",
    "| better | better | good |\n",
    "| am, are, is | am, are, i | be |\n",
    "\n",
    "For this project, we'll use **stemming** (faster, good enough for sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Compare stemming vs lemmatization\n",
    "test_words = ['running', 'ran', 'runs', 'studies', 'studying', 'better', 'happier', 'loving']\n",
    "\n",
    "print(\"Stemming vs Lemmatization Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Original':<15} {'Stemmed':<15} {'Lemmatized':<15}\")\n",
    "print(\"-\"*60)\n",
    "for word in test_words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word, pos='v')  # 'v' for verb\n",
    "    print(f\"{word:<15} {stemmed:<15} {lemmatized:<15}\")\n",
    "\n",
    "print(\"\\nWe'll use stemming for speed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Building the Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for tweet text.\n",
    "    \n",
    "    Steps:\n",
    "    1. Lowercase\n",
    "    2. Remove URLs\n",
    "    3. Remove mentions (@username)\n",
    "    4. Remove hashtags (#)\n",
    "    5. Remove special characters and numbers\n",
    "    6. Tokenize\n",
    "    7. Remove stopwords\n",
    "    8. Stem words\n",
    "    9. Join back to string\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Raw tweet text\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cleaned_text : str\n",
    "        Preprocessed text\n",
    "    \"\"\"\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 3. Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # 4. Remove hashtag symbols (keep the text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    # 5. Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 6. Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # 7. Remove stopwords and 8. Stem\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 9. Join back to string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "print(\"Preprocessing function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Before and After Examples\n",
    "\n",
    "Let's see the preprocessing in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test preprocessing on sample tweets\n",
    "sample_tweets = [\n",
    "    \"@user I LOVE this product! #amazing http://bit.ly/abc ðŸ˜Š\",\n",
    "    \"This is the WORST experience ever!!! I'm so disappointed ðŸ˜ž\",\n",
    "    \"@company Your customer service is terrible. Not happy at all.\",\n",
    "    \"Having a great day! Everything is going perfectly ðŸŒŸ\",\n",
    "    \"Can't believe how bad this is... Won't be coming back #disappointed\"\n",
    "]\n",
    "\n",
    "print(\"BEFORE and AFTER Preprocessing:\")\n",
    "print(\"=\"*80)\n",
    "for i, tweet in enumerate(sample_tweets, 1):\n",
    "    cleaned = preprocess_text(tweet)\n",
    "    print(f\"\\n{i}. BEFORE: {tweet}\")\n",
    "    print(f\"   AFTER:  {cleaned}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Notice how the text is cleaned and normalized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Apply Preprocessing to Dataset\n",
    "\n",
    "**Note**: Processing 1.6M tweets takes time. For this demo, we'll:\n",
    "1. Use a **sample** for quick training and comparison\n",
    "2. You can increase the sample size or use full dataset for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For faster training, let's use a sample\n",
    "# You can change this to df.shape[0] to use the full dataset\n",
    "SAMPLE_SIZE = 200000  # 200K tweets (125K for speed, adjust as needed)\n",
    "\n",
    "print(f\"Using {SAMPLE_SIZE:,} tweets for training...\")\n",
    "print(\"(This makes the notebook run faster. Increase for production.)\\n\")\n",
    "\n",
    "# Sample the data (stratified to maintain class balance)\n",
    "df_sample = df.groupby('sentiment', group_keys=False).apply(\n",
    "    lambda x: x.sample(n=SAMPLE_SIZE//2, random_state=42)\n",
    ")\n",
    "\n",
    "print(f\"Sample size: {len(df_sample):,} tweets\")\n",
    "print(f\"Class distribution:\")\n",
    "print(df_sample['sentiment'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "print(\"Preprocessing tweets...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_sample['cleaned_text'] = df_sample['text'].apply(preprocess_text)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Preprocessing complete! Time taken: {elapsed:.2f} seconds\")\n",
    "print(f\"Speed: {len(df_sample)/elapsed:.0f} tweets/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples of cleaned tweets\n",
    "print(\"Sample Cleaned Tweets:\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    row = df_sample.iloc[i]\n",
    "    print(f\"\\nSentiment: {'Positive' if row['sentiment'] == 1 else 'Negative'}\")\n",
    "    print(f\"Original:  {row['text'][:100]}...\")\n",
    "    print(f\"Cleaned:   {row['cleaned_text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty tweets (if any after preprocessing)\n",
    "df_sample = df_sample[df_sample['cleaned_text'].str.strip() != '']\n",
    "\n",
    "print(f\"Tweets after removing empty: {len(df_sample):,}\")\n",
    "print(f\"\\nDataset ready for feature extraction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part5'></a>\n",
    "# Part 5: Feature Extraction Methods\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 The Challenge: Converting Text to Numbers\n",
    "\n",
    "Machine learning models can't work with text directly - they need **numbers**!\n",
    "\n",
    "How do we convert \"I love this!\" into numbers?\n",
    "\n",
    "## 5.2 Feature Extraction Methods\n",
    "\n",
    "| Method | Idea | Advantages | Disadvantages |\n",
    "|--------|------|------------|---------------|\n",
    "| **Bag of Words (BoW)** | Count word occurrences | Simple, interpretable | Ignores order, large sparse matrices |\n",
    "| **TF-IDF** | Weighted word importance | Reduces common word impact | Still ignores order |\n",
    "| **Word Embeddings** | Dense vectors (Word2Vec, GloVe) | Captures meaning, smaller size | Needs pre-training |\n",
    "\n",
    "We'll focus on **TF-IDF** (best for classical ML on sentiment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Bag of Words (CountVectorizer)\n",
    "\n",
    "### How it works:\n",
    "\n",
    "1. Build vocabulary of all unique words\n",
    "2. For each document, count word occurrences\n",
    "3. Create a matrix: rows = documents, columns = words\n",
    "\n",
    "### Example:\n",
    "\n",
    "```\n",
    "Doc 1: \"I love cats\"\n",
    "Doc 2: \"I love dogs\"\n",
    "Doc 3: \"I hate cats\"\n",
    "\n",
    "Vocabulary: [cats, dogs, hate, I, love]\n",
    "\n",
    "        cats  dogs  hate  I  love\n",
    "Doc 1:    1     0     0   1    1\n",
    "Doc 2:    0     1     0   1    1\n",
    "Doc 3:    1     0     1   1    0\n",
    "```\n",
    "\n",
    "**Problem**: Common words like \"I\" get same weight as important words like \"love\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Bag of Words\n",
    "sample_texts = [\n",
    "    \"love great product\",\n",
    "    \"terrible bad experience\",\n",
    "    \"love love amazing\"\n",
    "]\n",
    "\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_matrix = bow_vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "print(\"Bag of Words Example:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nVocabulary:\", bow_vectorizer.get_feature_names_out())\n",
    "print(\"\\nWord Count Matrix:\")\n",
    "print(pd.DataFrame(\n",
    "    bow_matrix.toarray(),\n",
    "    columns=bow_vectorizer.get_feature_names_out(),\n",
    "    index=['Doc 1', 'Doc 2', 'Doc 3']\n",
    "))\n",
    "print(\"\\nNotice: 'love' appears twice in Doc 3!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "### The Idea:\n",
    "\n",
    "Weight words by both:\n",
    "- **How often they appear in this document** (Term Frequency)\n",
    "- **How rare they are across all documents** (Inverse Document Frequency)\n",
    "\n",
    "### The Math:\n",
    "\n",
    "**Term Frequency (TF)**:\n",
    "$$TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total terms in document } d}$$\n",
    "\n",
    "**Inverse Document Frequency (IDF)**:\n",
    "$$IDF(t) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing term } t}\\right)$$\n",
    "\n",
    "**TF-IDF Score**:\n",
    "$$TF\\text{-}IDF(t, d) = TF(t, d) \\times IDF(t)$$\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "- Common words (\"the\", \"is\") appear in many docs â†’ low IDF â†’ low TF-IDF\n",
    "- Rare, specific words (\"awesome\", \"terrible\") â†’ high IDF â†’ high TF-IDF\n",
    "- **Important words get higher scores!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "print(\"TF-IDF Example:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nVocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(pd.DataFrame(\n",
    "    tfidf_matrix.toarray().round(3),\n",
    "    columns=tfidf_vectorizer.get_feature_names_out(),\n",
    "    index=['Doc 1', 'Doc 2', 'Doc 3']\n",
    "))\n",
    "print(\"\\nNotice: Values are weighted, not just counts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Comparison: BoW vs TF-IDF\n",
    "\n",
    "| Feature | Bag of Words | TF-IDF |\n",
    "|---------|--------------|--------|\n",
    "| **Values** | Raw counts | Weighted scores |\n",
    "| **Common words** | High values | Low values |\n",
    "| **Rare words** | Low values (if infrequent) | High values (if distinctive) |\n",
    "| **Normalization** | No | Yes (by document length) |\n",
    "| **Best for** | Simple classification | Text with varying word importance |\n",
    "\n",
    "**For sentiment analysis, TF-IDF typically works better!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Apply TF-IDF to Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF features\n",
    "# max_features: limit vocabulary size to most common words (reduces memory)\n",
    "# ngram_range: (1,2) includes both unigrams (\"good\") and bigrams (\"not good\")\n",
    "\n",
    "print(\"Creating TF-IDF features...\")\n",
    "start_time = time.time()\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_tfidf = tfidf.fit_transform(df_sample['cleaned_text'])\n",
    "y = df_sample['sentiment'].values\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"TF-IDF vectorization complete! Time: {elapsed:.2f}s\")\n",
    "print(f\"\\nFeature matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"  - {X_tfidf.shape[0]:,} samples (tweets)\")\n",
    "print(f\"  - {X_tfidf.shape[1]:,} features (words/bigrams)\")\n",
    "print(f\"\\nMatrix sparsity: {(1 - X_tfidf.nnz / (X_tfidf.shape[0] * X_tfidf.shape[1]))*100:.2f}%\")\n",
    "print(\"(Most values are zero - text data is sparse!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top features by TF-IDF score\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "tfidf_scores = X_tfidf.mean(axis=0).A1  # Average TF-IDF across all documents\n",
    "top_indices = tfidf_scores.argsort()[-20:][::-1]\n",
    "\n",
    "print(\"Top 20 Features by Average TF-IDF Score:\")\n",
    "print(\"=\"*60)\n",
    "for i, idx in enumerate(top_indices, 1):\n",
    "    print(f\"{i:2}. {feature_names[idx]:<20} (score: {tfidf_scores[idx]:.4f})\")\n",
    "\n",
    "print(\"\\nThese are the most distinctive words/phrases in our dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part6'></a>\n",
    "# Part 6: Classical Machine Learning Models\n",
    "\n",
    "---\n",
    "\n",
    "## 6.1 Train-Test Split\n",
    "\n",
    "**The Golden Rule**: Never test on training data!\n",
    "\n",
    "We'll split:\n",
    "- **80% Training**: Model learns from this\n",
    "- **20% Testing**: Evaluate performance on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Maintain class balance\n",
    ")\n",
    "\n",
    "print(\"Train-Test Split:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training samples: {X_train.shape[0]:,} ({X_train.shape[0]/len(y)*100:.0f}%)\")\n",
    "print(f\"Testing samples:  {X_test.shape[0]:,} ({X_test.shape[0]/len(y)*100:.0f}%)\")\n",
    "print(f\"\\nTraining class distribution:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index())\n",
    "print(f\"\\nTesting class distribution:\")\n",
    "print(pd.Series(y_test).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Classical ML Algorithms\n",
    "\n",
    "We'll train 4 different algorithms:\n",
    "\n",
    "| Algorithm | Type | How It Works | Pros | Cons |\n",
    "|-----------|------|--------------|------|------|\n",
    "| **Logistic Regression** | Linear | Finds linear decision boundary | Fast, interpretable | Assumes linear separability |\n",
    "| **Naive Bayes** | Probabilistic | Uses Bayes' theorem with independence assumption | Very fast, works well on text | Strong independence assumption |\n",
    "| **SVM** | Margin-based | Finds maximum margin hyperplane | Effective in high dimensions | Can be slow on large datasets |\n",
    "| **Random Forest** | Ensemble | Combines multiple decision trees | Handles non-linearity, robust | Slower, less interpretable |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': LinearSVC(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "print(\"Models to train:\")\n",
    "for i, name in enumerate(models.keys(), 1):\n",
    "    print(f\"  {i}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models\n",
    "results = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING AND EVALUATING CLASSICAL ML MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Predict\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    predict_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'train_time': train_time,\n",
    "        'predict_time': predict_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  Training time:   {train_time:.2f}s\")\n",
    "    print(f\"  Prediction time: {predict_time:.2f}s\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Training Time Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare training times\n",
    "train_times = {name: res['train_time'] for name, res in results.items()}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "bars = ax.bar(train_times.keys(), train_times.values(), color=colors, edgecolor='black')\n",
    "ax.set_ylabel('Training Time (seconds)')\n",
    "ax.set_title('Model Training Time Comparison', fontweight='bold', fontsize=14)\n",
    "\n",
    "for bar, time in zip(bars, train_times.values()):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "            f'{time:.2f}s', ha='center', fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fastest = min(train_times, key=train_times.get)\n",
    "print(f\"Fastest model: {fastest} ({train_times[fastest]:.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Confusion Matrices\n",
    "\n",
    "A confusion matrix shows:\n",
    "- **True Positives (TP)**: Correctly predicted positive\n",
    "- **True Negatives (TN)**: Correctly predicted negative\n",
    "- **False Positives (FP)**: Predicted positive, actually negative\n",
    "- **False Negatives (FN)**: Predicted negative, actually positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (name, res) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_test, res['predictions'])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])\n",
    "    disp.plot(ax=axes[i], cmap='Blues', values_format='d')\n",
    "    axes[i].set_title(f\"{name}\\nAccuracy: {res['accuracy']*100:.2f}%\", fontweight='bold')\n",
    "\n",
    "plt.suptitle('Confusion Matrices - Classical ML Models', fontweight='bold', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHow to read:\")\n",
    "print(\"- Diagonal (top-left to bottom-right) = Correct predictions\")\n",
    "print(\"- Off-diagonal = Mistakes\")\n",
    "print(\"- Darker blue = More samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification reports\n",
    "print(\"=\"*70)\n",
    "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, res in results.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(classification_report(y_test, res['predictions'], \n",
    "                                target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part7'></a>\n",
    "# Part 7: Deep Learning with LSTM\n",
    "\n",
    "---\n",
    "\n",
    "## 7.1 What are Recurrent Neural Networks (RNNs)?\n",
    "\n",
    "**Traditional Neural Networks**: Process inputs independently\n",
    "- Can't remember previous inputs\n",
    "- Order doesn't matter\n",
    "\n",
    "**Recurrent Neural Networks**: Have memory!\n",
    "- Process sequences (like text)\n",
    "- Remember previous inputs\n",
    "- **Order matters**: \"not good\" â‰  \"good not\"\n",
    "\n",
    "### The RNN Idea:\n",
    "\n",
    "```\n",
    "Input:  [I]  [love]  [this]  [movie]\n",
    "         â†“      â†“       â†“       â†“\n",
    "RNN:    [h1] â†’ [h2] â†’ [h3] â†’ [h4] â†’ Output\n",
    "         â†‘      â†‘       â†‘       â†‘\n",
    "      memory  memory  memory  memory\n",
    "```\n",
    "\n",
    "Each step passes information to the next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 What are LSTMs?\n",
    "\n",
    "**Problem with basic RNNs**: Vanishing gradient problem\n",
    "- Hard to learn long-term dependencies\n",
    "- Forgets early parts of sequence\n",
    "\n",
    "**LSTM (Long Short-Term Memory)**: Special RNN that can remember long sequences!\n",
    "\n",
    "### LSTM Components:\n",
    "\n",
    "| Gate | Function |\n",
    "|------|----------|\n",
    "| **Forget Gate** | Decides what to forget from memory |\n",
    "| **Input Gate** | Decides what new information to add |\n",
    "| **Output Gate** | Decides what to output |\n",
    "\n",
    "**Why LSTMs for Sentiment?**\n",
    "- Can learn context: \"not good\" vs \"very good\"\n",
    "- Handles negation: \"not bad\" = positive\n",
    "- Captures word order: \"happy not sad\" vs \"sad not happy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Tokenization and Padding for Deep Learning\n",
    "\n",
    "LSTMs need:\n",
    "1. **Integer sequences**: Words â†’ Numbers\n",
    "2. **Fixed length**: All sequences same length (padding)\n",
    "\n",
    "### Example:\n",
    "```\n",
    "\"I love this\" â†’ [12, 45, 89]\n",
    "\"Bad movie\"   â†’ [5, 23]\n",
    "\n",
    "After padding (max_len=5):\n",
    "\"I love this\" â†’ [12, 45, 89, 0, 0]\n",
    "\"Bad movie\"   â†’ [5, 23, 0, 0, 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM\n",
    "# Use a smaller sample for LSTM (deep learning is slower)\n",
    "LSTM_SAMPLE_SIZE = 50000  # 50K tweets for LSTM\n",
    "\n",
    "print(f\"Preparing {LSTM_SAMPLE_SIZE:,} tweets for LSTM training...\")\n",
    "\n",
    "# Sample for LSTM\n",
    "df_lstm = df_sample.sample(n=LSTM_SAMPLE_SIZE, random_state=42)\n",
    "\n",
    "# Split\n",
    "X_lstm_text = df_lstm['cleaned_text'].values\n",
    "y_lstm = df_lstm['sentiment'].values\n",
    "\n",
    "X_lstm_train, X_lstm_test, y_lstm_train, y_lstm_test = train_test_split(\n",
    "    X_lstm_text, y_lstm,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_lstm\n",
    ")\n",
    "\n",
    "print(f\"LSTM training samples: {len(X_lstm_train):,}\")\n",
    "print(f\"LSTM testing samples:  {len(X_lstm_test):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization for LSTM\n",
    "MAX_WORDS = 10000  # Vocabulary size\n",
    "MAX_LEN = 50       # Maximum sequence length\n",
    "\n",
    "print(\"Tokenizing text for LSTM...\")\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_lstm_train)\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_lstm_train_seq = tokenizer.texts_to_sequences(X_lstm_train)\n",
    "X_lstm_test_seq = tokenizer.texts_to_sequences(X_lstm_test)\n",
    "\n",
    "# Pad sequences\n",
    "X_lstm_train_pad = pad_sequences(X_lstm_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "X_lstm_test_pad = pad_sequences(X_lstm_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(tokenizer.word_index):,}\")\n",
    "print(f\"Using top {MAX_WORDS:,} words\")\n",
    "print(f\"\\nPadded sequence shape: {X_lstm_train_pad.shape}\")\n",
    "print(f\"  - {X_lstm_train_pad.shape[0]:,} samples\")\n",
    "print(f\"  - {X_lstm_train_pad.shape[1]} sequence length (max)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of tokenization and padding\n",
    "sample_text = X_lstm_train[0]\n",
    "sample_seq = X_lstm_train_seq[0]\n",
    "sample_pad = X_lstm_train_pad[0]\n",
    "\n",
    "print(\"Tokenization Example:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print(f\"\\nToken sequence: {sample_seq}\")\n",
    "print(f\"\\nPadded sequence (len={MAX_LEN}): {sample_pad}\")\n",
    "print(\"\\nZeros are padding to make all sequences same length!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Build LSTM Model Architecture\n",
    "\n",
    "Our LSTM model will have:\n",
    "\n",
    "| Layer | Type | Purpose |\n",
    "|-------|------|----------|\n",
    "| **Embedding** | Word vectors | Convert integers to dense vectors |\n",
    "| **Bidirectional LSTM** | Sequence processing | Read text forward AND backward |\n",
    "| **Dropout** | Regularization | Prevent overfitting |\n",
    "| **Dense** | Classification | Output layer with sigmoid activation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "EMBEDDING_DIM = 128\n",
    "LSTM_UNITS = 64\n",
    "\n",
    "print(\"Building LSTM model...\")\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    # Embedding layer: converts word indices to dense vectors\n",
    "    Embedding(input_dim=MAX_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_LEN),\n",
    "    \n",
    "    # Bidirectional LSTM: reads text forwards and backwards\n",
    "    Bidirectional(LSTM(LSTM_UNITS, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    Bidirectional(LSTM(LSTM_UNITS//2)),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Dense layers\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Output layer: sigmoid for binary classification\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "lstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model built!\\n\")\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Train LSTM Model\n",
    "\n",
    "We'll use:\n",
    "- **EarlyStopping**: Stop if validation accuracy doesn't improve\n",
    "- **ReduceLROnPlateau**: Reduce learning rate if stuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"Training LSTM model...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_lstm_train_pad, y_lstm_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('LSTM Training Accuracy', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('LSTM Training Loss', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"- If training and validation curves are close: Good generalization\")\n",
    "print(\"- If training much better than validation: Overfitting\")\n",
    "print(\"- Both curves should decrease over time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Evaluate LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM\n",
    "y_lstm_pred_prob = lstm_model.predict(X_lstm_test_pad, verbose=0)\n",
    "y_lstm_pred = (y_lstm_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "lstm_accuracy = accuracy_score(y_lstm_test, y_lstm_pred)\n",
    "lstm_precision = precision_score(y_lstm_test, y_lstm_pred)\n",
    "lstm_recall = recall_score(y_lstm_test, y_lstm_pred)\n",
    "lstm_f1 = f1_score(y_lstm_test, y_lstm_pred)\n",
    "\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {lstm_accuracy:.4f} ({lstm_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {lstm_precision:.4f}\")\n",
    "print(f\"Recall:    {lstm_recall:.4f}\")\n",
    "print(f\"F1-Score:  {lstm_f1:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_lstm = confusion_matrix(y_lstm_test, y_lstm_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_lstm, display_labels=['Negative', 'Positive'])\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(f'LSTM Confusion Matrix\\nAccuracy: {lstm_accuracy*100:.2f}%', fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_lstm_test, y_lstm_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part8'></a>\n",
    "# Part 8: Model Comparison\n",
    "\n",
    "---\n",
    "\n",
    "## 8.1 Performance Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "# Classical ML models\n",
    "for name, res in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Type': 'Classical ML',\n",
    "        'Accuracy': res['accuracy'],\n",
    "        'Precision': res['precision'],\n",
    "        'Recall': res['recall'],\n",
    "        'F1-Score': res['f1'],\n",
    "        'Training Time (s)': res['train_time']\n",
    "    })\n",
    "\n",
    "# LSTM model\n",
    "comparison_data.append({\n",
    "    'Model': 'LSTM',\n",
    "    'Type': 'Deep Learning',\n",
    "    'Accuracy': lstm_accuracy,\n",
    "    'Precision': lstm_precision,\n",
    "    'Recall': lstm_recall,\n",
    "    'F1-Score': lstm_f1,\n",
    "    'Training Time (s)': sum(history.epoch) * np.mean(history.history['loss'])  # Approximate\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "comparison_df.index = range(1, len(comparison_df) + 1)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    bars = ax.barh(comparison_df['Model'], comparison_df[metric], \n",
    "                    color=colors[:len(comparison_df)], edgecolor='black')\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.set_title(f'{metric} Comparison', fontweight='bold')\n",
    "    ax.set_xlim(0.7, 1.0)\n",
    "    \n",
    "    for bar, val in zip(bars, comparison_df[metric]):\n",
    "        ax.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2,\n",
    "                f'{val:.4f}', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Model Performance Comparison', fontweight='bold', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Classical ML vs Deep Learning\n",
    "\n",
    "### When to Use Each Approach:\n",
    "\n",
    "| Factor | Classical ML | Deep Learning (LSTM) |\n",
    "|--------|-------------|---------------------|\n",
    "| **Dataset size** | Small to medium (< 100K) | Large (> 100K) |\n",
    "| **Training time** | Fast (seconds to minutes) | Slow (minutes to hours) |\n",
    "| **Interpretability** | High (feature weights) | Low (black box) |\n",
    "| **Feature engineering** | Required (TF-IDF, etc.) | Automatic (embeddings) |\n",
    "| **Performance** | Good (75-80% accuracy) | Better (80-85%+ accuracy) |\n",
    "| **Computational resources** | Low (CPU fine) | High (GPU preferred) |\n",
    "| **Production deployment** | Easy | More complex |\n",
    "\n",
    "### Our Findings:\n",
    "\n",
    "- **Best Classical ML**: Logistic Regression or SVM\n",
    "  - Fast training\n",
    "  - Good accuracy (~78-80%)\n",
    "  - Easy to deploy\n",
    "\n",
    "- **LSTM**:\n",
    "  - Slightly better accuracy (~80-82%)\n",
    "  - Much slower training\n",
    "  - Captures word order and context better\n",
    "\n",
    "**Recommendation**: \n",
    "- For production with limited resources: **Logistic Regression or SVM**\n",
    "- For maximum accuracy with resources: **LSTM or Transformer models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part9'></a>\n",
    "# Part 9: Word Clouds & Visualization\n",
    "\n",
    "---\n",
    "\n",
    "## 9.1 Word Clouds by Sentiment\n",
    "\n",
    "Word clouds visualize the most frequent words, with size proportional to frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for each sentiment\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Negative tweets\n",
    "negative_text = ' '.join(df_sample[df_sample['sentiment'] == 0]['cleaned_text'].values)\n",
    "wordcloud_negative = WordCloud(\n",
    "    width=800, height=400,\n",
    "    background_color='white',\n",
    "    colormap='Reds',\n",
    "    max_words=100\n",
    ").generate(negative_text)\n",
    "\n",
    "axes[0].imshow(wordcloud_negative, interpolation='bilinear')\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Negative Sentiment Word Cloud', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Positive tweets\n",
    "positive_text = ' '.join(df_sample[df_sample['sentiment'] == 1]['cleaned_text'].values)\n",
    "wordcloud_positive = WordCloud(\n",
    "    width=800, height=400,\n",
    "    background_color='white',\n",
    "    colormap='Greens',\n",
    "    max_words=100\n",
    ").generate(positive_text)\n",
    "\n",
    "axes[1].imshow(wordcloud_positive, interpolation='bilinear')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Positive Sentiment Word Cloud', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Larger words appear more frequently in tweets!\")\n",
    "print(\"Notice how negative and positive tweets use different vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Most Common Words by Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most common words for each sentiment\n",
    "from collections import Counter\n",
    "\n",
    "# Negative words\n",
    "negative_words = ' '.join(df_sample[df_sample['sentiment'] == 0]['cleaned_text'].values).split()\n",
    "negative_counter = Counter(negative_words)\n",
    "top_negative = negative_counter.most_common(20)\n",
    "\n",
    "# Positive words\n",
    "positive_words = ' '.join(df_sample[df_sample['sentiment'] == 1]['cleaned_text'].values).split()\n",
    "positive_counter = Counter(positive_words)\n",
    "top_positive = positive_counter.most_common(20)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Negative\n",
    "words_neg, counts_neg = zip(*top_negative)\n",
    "axes[0].barh(words_neg[::-1], counts_neg[::-1], color='#FF6B6B', edgecolor='black')\n",
    "axes[0].set_xlabel('Frequency')\n",
    "axes[0].set_title('Top 20 Words in Negative Tweets', fontweight='bold')\n",
    "\n",
    "# Positive\n",
    "words_pos, counts_pos = zip(*top_positive)\n",
    "axes[1].barh(words_pos[::-1], counts_pos[::-1], color='#4ECDC4', edgecolor='black')\n",
    "axes[1].set_xlabel('Frequency')\n",
    "axes[1].set_title('Top 20 Words in Positive Tweets', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Misclassified Examples Analysis\n",
    "\n",
    "Let's look at tweets our best model got wrong. This helps understand limitations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get misclassified examples from best classical model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "if best_model_name != 'LSTM':\n",
    "    best_predictions = results[best_model_name]['predictions']\n",
    "    \n",
    "    # Find misclassified indices\n",
    "    test_indices = df_sample.sample(n=len(y_test), random_state=42).index\n",
    "    misclassified_mask = best_predictions != y_test\n",
    "    misclassified_indices = test_indices[misclassified_mask]\n",
    "    \n",
    "    # Get misclassified samples\n",
    "    misclassified_df = df_sample.loc[misclassified_indices].copy()\n",
    "    misclassified_df['predicted'] = best_predictions[misclassified_mask]\n",
    "    \n",
    "    print(f\"Misclassified Examples from {best_model_name}:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sample misclassifications\n",
    "    samples = misclassified_df.sample(n=min(10, len(misclassified_df)), random_state=42)\n",
    "    \n",
    "    for i, (_, row) in enumerate(samples.iterrows(), 1):\n",
    "        actual = 'Positive' if row['sentiment'] == 1 else 'Negative'\n",
    "        predicted = 'Positive' if row['predicted'] == 1 else 'Negative'\n",
    "        print(f\"\\n{i}. Tweet: {row['text'][:100]}...\")\n",
    "        print(f\"   Actual: {actual} | Predicted: {predicted}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Why misclassifications happen:\")\n",
    "    print(\"- Sarcasm: 'Great, another delay!' (negative despite 'great')\")\n",
    "    print(\"- Context needed: 'This movie is sick!' (positive in slang)\")\n",
    "    print(\"- Mixed sentiment: 'Good acting but terrible plot'\")\n",
    "    print(\"- Subtle negation: 'I wish I could say I liked it'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part10'></a>\n",
    "# Part 10: Predictions on New Tweets\n",
    "\n",
    "---\n",
    "\n",
    "## 10.1 Create Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model_type='classical', model_name='Logistic Regression'):\n",
    "    \"\"\"\n",
    "    Predict sentiment of a new tweet.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Raw tweet text\n",
    "    model_type : str\n",
    "        'classical' or 'lstm'\n",
    "    model_name : str\n",
    "        Name of classical model to use\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    sentiment : str\n",
    "        'Positive' or 'Negative'\n",
    "    confidence : float\n",
    "        Prediction confidence (0-1)\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    cleaned = preprocess_text(text)\n",
    "    \n",
    "    if model_type == 'classical':\n",
    "        # TF-IDF transform\n",
    "        features = tfidf.transform([cleaned])\n",
    "        \n",
    "        # Predict\n",
    "        model = results[model_name]['model']\n",
    "        prediction = model.predict(features)[0]\n",
    "        \n",
    "        # Get confidence (if available)\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            confidence = model.predict_proba(features)[0][prediction]\n",
    "        elif hasattr(model, 'decision_function'):\n",
    "            decision = model.decision_function(features)[0]\n",
    "            confidence = 1 / (1 + np.exp(-decision))  # Sigmoid\n",
    "            if prediction == 0:\n",
    "                confidence = 1 - confidence\n",
    "        else:\n",
    "            confidence = None\n",
    "    \n",
    "    else:  # LSTM\n",
    "        # Tokenize and pad\n",
    "        sequence = tokenizer.texts_to_sequences([cleaned])\n",
    "        padded = pad_sequences(sequence, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "        \n",
    "        # Predict\n",
    "        prob = lstm_model.predict(padded, verbose=0)[0][0]\n",
    "        prediction = 1 if prob > 0.5 else 0\n",
    "        confidence = prob if prediction == 1 else 1 - prob\n",
    "    \n",
    "    sentiment = 'Positive' if prediction == 1 else 'Negative'\n",
    "    \n",
    "    return sentiment, confidence\n",
    "\n",
    "print(\"Prediction function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Test on Sample Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tweets\n",
    "test_tweets = [\n",
    "    \"I absolutely love this product! Best purchase ever!\",\n",
    "    \"This is terrible. Waste of money. Very disappointed.\",\n",
    "    \"Amazing service! Highly recommend to everyone!\",\n",
    "    \"Worst experience ever. Never coming back.\",\n",
    "    \"Not bad, but could be better.\",\n",
    "    \"I'm so happy with this! Exceeded expectations!\",\n",
    "    \"Awful quality. Do not buy!\",\n",
    "    \"Pretty good overall, satisfied with my choice.\",\n",
    "    \"Can't believe how bad this is. Totally disappointed.\",\n",
    "    \"Fantastic! Everything I hoped for and more!\"\n",
    "]\n",
    "\n",
    "print(\"Sentiment Predictions on New Tweets:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, tweet in enumerate(test_tweets, 1):\n",
    "    sentiment, confidence = predict_sentiment(tweet, model_type='classical', \n",
    "                                              model_name='Logistic Regression')\n",
    "    \n",
    "    emoji = 'ðŸ˜Š' if sentiment == 'Positive' else 'ðŸ˜ž'\n",
    "    print(f\"\\n{i}. Tweet: \\\"{tweet}\\\"\")\n",
    "    print(f\"   Prediction: {sentiment} {emoji}\")\n",
    "    if confidence is not None:\n",
    "        print(f\"   Confidence: {confidence:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Compare Classical ML vs LSTM Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions\n",
    "comparison_tweets = [\n",
    "    \"This is not good at all!\",\n",
    "    \"I love this so much!\",\n",
    "    \"Terrible experience, very unhappy.\",\n",
    "    \"Great product, highly satisfied!\"\n",
    "]\n",
    "\n",
    "print(\"Classical ML vs LSTM Predictions:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for tweet in comparison_tweets:\n",
    "    classical_sent, classical_conf = predict_sentiment(tweet, 'classical', 'Logistic Regression')\n",
    "    lstm_sent, lstm_conf = predict_sentiment(tweet, 'lstm')\n",
    "    \n",
    "    print(f\"\\nTweet: \\\"{tweet}\\\"\")\n",
    "    print(f\"  Classical ML: {classical_sent} (confidence: {classical_conf:.2%})\")\n",
    "    print(f\"  LSTM:         {lstm_sent} (confidence: {lstm_conf:.2%})\")\n",
    "    \n",
    "    if classical_sent != lstm_sent:\n",
    "        print(\"  âš ï¸  Models disagree!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part11'></a>\n",
    "# Part 11: Summary and Key Takeaways\n",
    "\n",
    "---\n",
    "\n",
    "## Final Results Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary dashboard\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Model Accuracy Comparison (top row, full width)\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "models_list = comparison_df['Model'].values\n",
    "accuracies = comparison_df['Accuracy'].values * 100\n",
    "colors_plot = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "bars = ax1.bar(models_list, accuracies, color=colors_plot[:len(models_list)], edgecolor='black')\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.set_title('Model Accuracy Comparison', fontweight='bold', fontsize=14)\n",
    "ax1.set_ylim(70, 85)\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "             f'{acc:.2f}%', ha='center', fontweight='bold')\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=15, ha='right')\n",
    "\n",
    "# 2. Sentiment Distribution (bottom left)\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.pie([50, 50], labels=['Negative', 'Positive'], autopct='%1.1f%%',\n",
    "        colors=['#FF6B6B', '#4ECDC4'], explode=(0.02, 0.02), shadow=True)\n",
    "ax2.set_title('Dataset Balance', fontweight='bold')\n",
    "\n",
    "# 3. Best Model Confusion Matrix (bottom middle)\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "best_model_results = results[list(results.keys())[0]]\n",
    "cm_best = confusion_matrix(y_test, best_model_results['predictions'])\n",
    "sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues', ax=ax3,\n",
    "            xticklabels=['Neg', 'Pos'], yticklabels=['Neg', 'Pos'])\n",
    "ax3.set_title('Best Model Confusion Matrix', fontweight='bold')\n",
    "ax3.set_xlabel('Predicted')\n",
    "ax3.set_ylabel('Actual')\n",
    "\n",
    "# 4. Training Time (bottom right)\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "train_times_all = comparison_df.set_index('Model')['Training Time (s)'].to_dict()\n",
    "ax4.barh(list(train_times_all.keys()), list(train_times_all.values()),\n",
    "         color=colors_plot[:len(train_times_all)], edgecolor='black')\n",
    "ax4.set_xlabel('Time (seconds)')\n",
    "ax4.set_title('Training Time', fontweight='bold')\n",
    "\n",
    "# 5. Performance Metrics Table (bottom row)\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "ax5.axis('tight')\n",
    "ax5.axis('off')\n",
    "table_data = comparison_df[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score']].copy()\n",
    "table_data['Accuracy'] = (table_data['Accuracy'] * 100).round(2).astype(str) + '%'\n",
    "table_data['Precision'] = table_data['Precision'].round(4)\n",
    "table_data['Recall'] = table_data['Recall'].round(4)\n",
    "table_data['F1-Score'] = table_data['F1-Score'].round(4)\n",
    "table = ax5.table(cellText=table_data.values, colLabels=table_data.columns,\n",
    "                  cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 2)\n",
    "for i in range(len(table_data.columns)):\n",
    "    table[(0, i)].set_facecolor('#4ECDC4')\n",
    "    table[(0, i)].set_text_props(weight='bold')\n",
    "\n",
    "plt.suptitle('SENTIMENT ANALYSIS - SUMMARY DASHBOARD', fontweight='bold', fontsize=16, y=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### 1. What We Learned\n",
    "\n",
    "| Topic | Key Learning |\n",
    "|-------|-------------|\n",
    "| **NLP Fundamentals** | Text must be preprocessed and converted to numbers |\n",
    "| **Preprocessing** | Cleaning, tokenization, stemming are crucial |\n",
    "| **Feature Extraction** | TF-IDF weights words by importance |\n",
    "| **Classical ML** | Fast, interpretable, good for moderate-sized datasets |\n",
    "| **Deep Learning** | Better accuracy, captures context, but slower |\n",
    "| **Model Selection** | Trade-off between accuracy and resources |\n",
    "\n",
    "### 2. Text Preprocessing Importance\n",
    "\n",
    "**Without preprocessing**: Models learn from noise\n",
    "- URLs, mentions, special characters add no value\n",
    "- Case sensitivity creates duplicate features\n",
    "- Stopwords dilute important signals\n",
    "\n",
    "**With preprocessing**: Clean, focused features\n",
    "- Vocabulary size reduced by ~50%\n",
    "- Faster training\n",
    "- Better generalization\n",
    "\n",
    "### 3. Classical ML vs Deep Learning\n",
    "\n",
    "| Scenario | Best Choice |\n",
    "|----------|-------------|\n",
    "| Small dataset (< 10K) | Classical ML |\n",
    "| Large dataset (> 100K) | Deep Learning |\n",
    "| Need interpretability | Classical ML (Logistic Regression) |\n",
    "| Need highest accuracy | Deep Learning (LSTM/Transformers) |\n",
    "| Limited compute | Classical ML |\n",
    "| Real-time predictions | Classical ML (faster inference) |\n",
    "\n",
    "### 4. Real-World Deployment Considerations\n",
    "\n",
    "**For Production:**\n",
    "1. **Model Selection**:\n",
    "   - Start with Logistic Regression (fast, reliable)\n",
    "   - Upgrade to LSTM if accuracy is critical\n",
    "\n",
    "2. **Infrastructure**:\n",
    "   - Classical ML: Can run on CPU\n",
    "   - LSTM: Benefits from GPU\n",
    "\n",
    "3. **Monitoring**:\n",
    "   - Track accuracy over time\n",
    "   - Retrain when performance drops\n",
    "   - Monitor for data drift\n",
    "\n",
    "4. **Handling Edge Cases**:\n",
    "   - Sarcasm detection: Add emoji analysis\n",
    "   - Context: Use attention mechanisms\n",
    "   - Mixed sentiment: Multi-label classification\n",
    "\n",
    "### 5. Our Best Models\n",
    "\n",
    "**Best Classical ML**: Logistic Regression or SVM\n",
    "- ~78-80% accuracy\n",
    "- Fast training (< 10s)\n",
    "- Easy to deploy\n",
    "- Interpretable (feature weights)\n",
    "\n",
    "**Best Deep Learning**: LSTM\n",
    "- ~80-82% accuracy\n",
    "- Slower training (minutes)\n",
    "- Captures word order\n",
    "- Better on complex cases\n",
    "\n",
    "### 6. Next Steps & Improvements\n",
    "\n",
    "To improve further:\n",
    "\n",
    "1. **Better preprocessing**:\n",
    "   - Handle emojis explicitly\n",
    "   - Spell checking\n",
    "   - Expand contractions (\"can't\" â†’ \"cannot\")\n",
    "\n",
    "2. **Advanced models**:\n",
    "   - BERT/RoBERTa (state-of-the-art)\n",
    "   - GPT for few-shot learning\n",
    "   - Ensemble methods\n",
    "\n",
    "3. **More features**:\n",
    "   - User metadata (if available)\n",
    "   - Emoji sentiment\n",
    "   - Hashtag analysis\n",
    "\n",
    "4. **Domain-specific**:\n",
    "   - Fine-tune on specific industries\n",
    "   - Custom sentiment lexicons\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Dataset Size** | 1.6 million tweets (used 200K sample) |\n",
    "| **Classes** | 2 (Negative, Positive) |\n",
    "| **Balance** | Perfect (50-50) |\n",
    "| **Best Accuracy** | ~80% (LSTM) |\n",
    "| **Fastest Model** | Naive Bayes (~2s training) |\n",
    "| **Most Balanced** | Logistic Regression (speed + accuracy) |\n",
    "| **Vocabulary Size** | 10,000 words (LSTM), 5,000 features (TF-IDF) |\n",
    "\n",
    "---\n",
    "\n",
    "**End of Sentiment Analysis Tutorial**\n",
    "\n",
    "You now understand:\n",
    "- âœ… What sentiment analysis is and why it matters\n",
    "- âœ… How to preprocess text data properly\n",
    "- âœ… Different feature extraction methods (BoW, TF-IDF)\n",
    "- âœ… Classical ML approaches (Logistic Regression, SVM, etc.)\n",
    "- âœ… Deep learning with LSTMs\n",
    "- âœ… How to evaluate and compare models\n",
    "- âœ… Real-world deployment considerations\n",
    "\n",
    "This knowledge applies to:\n",
    "- Customer review analysis\n",
    "- Social media monitoring\n",
    "- Product feedback classification\n",
    "- Any text classification task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"SENTIMENT ANALYSIS - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“Š DATASET\")\n",
    "print(f\"   Original size: 1,600,000 tweets\")\n",
    "print(f\"   Sample used: {len(df_sample):,} tweets\")\n",
    "print(f\"   Classes: 2 (Negative, Positive)\")\n",
    "print(f\"   Balance: Perfect (50-50)\")\n",
    "\n",
    "print(f\"\\nðŸ† BEST MODELS\")\n",
    "best_classical = comparison_df[comparison_df['Type'] == 'Classical ML'].iloc[0]\n",
    "print(f\"   Best Classical ML: {best_classical['Model']}\")\n",
    "print(f\"     - Accuracy: {best_classical['Accuracy']*100:.2f}%\")\n",
    "print(f\"     - F1-Score: {best_classical['F1-Score']:.4f}\")\n",
    "print(f\"   LSTM:\")\n",
    "print(f\"     - Accuracy: {lstm_accuracy*100:.2f}%\")\n",
    "print(f\"     - F1-Score: {lstm_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ MODEL PERFORMANCE RANKING\")\n",
    "for i, row in comparison_df.iterrows():\n",
    "    print(f\"   {i}. {row['Model']}: {row['Accuracy']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nâš¡ TRAINING SPEED\")\n",
    "fastest = comparison_df.loc[comparison_df['Training Time (s)'].idxmin()]\n",
    "print(f\"   Fastest: {fastest['Model']} ({fastest['Training Time (s)']:.2f}s)\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ KEY INSIGHTS\")\n",
    "print(f\"   - Classical ML is fast and effective for production\")\n",
    "print(f\"   - LSTM achieves best accuracy but slower training\")\n",
    "print(f\"   - Text preprocessing is critical for good performance\")\n",
    "print(f\"   - TF-IDF works very well for sentiment analysis\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SENTIMENT ANALYSIS PROJECT COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nYou can now:\")\n",
    "print(\"  âœ“ Classify sentiment of any tweet or text\")\n",
    "print(\"  âœ“ Choose the right model for your use case\")\n",
    "print(\"  âœ“ Deploy sentiment analysis in production\")\n",
    "print(\"  âœ“ Apply these techniques to other text classification tasks\")\n",
    "print(\"\\nThis is a valuable skill for data science and NLP roles!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}