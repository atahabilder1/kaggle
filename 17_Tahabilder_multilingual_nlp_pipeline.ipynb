{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 17: Multilanguage NLP Pipeline\n",
    "\n",
    "**Build a system that supports multiple languages in text processing**\n",
    "\n",
    "In this comprehensive tutorial, we'll build a complete multilingual NLP pipeline that can:\n",
    "- Detect languages automatically\n",
    "- Tokenize text in multiple languages\n",
    "- Clean and preprocess multilingual text\n",
    "- Perform POS tagging across languages\n",
    "- Remove language-specific stopwords\n",
    "- Lemmatize words in different languages\n",
    "- Generate multilingual embeddings\n",
    "- Perform cross-lingual text classification\n",
    "\n",
    "**Pipeline Overview:**\n",
    "```\n",
    "Raw Text â†’ Language Detection â†’ Tokenization â†’ Text Cleaning â†’ POS Tagging â†’ \n",
    "Stopword Removal â†’ Lemmatization â†’ Cleaned Text â†’ Embeddings â†’ ML Model\n",
    "```\n",
    "\n",
    "**Languages we'll support:** English, Spanish, French, German, Italian, Portuguese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Installation](#1-setup-and-installation)\n",
    "2. [Language Detection](#2-language-detection)\n",
    "3. [Multilingual Tokenization](#3-multilingual-tokenization)\n",
    "4. [Text Cleaning](#4-text-cleaning)\n",
    "5. [POS Tagging Across Languages](#5-pos-tagging-across-languages)\n",
    "6. [Stopword Removal](#6-stopword-removal)\n",
    "7. [Lemmatization](#7-lemmatization)\n",
    "8. [Complete Pipeline Class](#8-complete-pipeline-class)\n",
    "9. [Multilingual Embeddings](#9-multilingual-embeddings)\n",
    "10. [Cross-Lingual Text Classification](#10-cross-lingual-text-classification)\n",
    "11. [Translation System](#11-translation-system)\n",
    "12. [Summary](#12-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langdetect spacy transformers sentencepiece torch nltk\n",
    "!pip install -q sacremoses  # For translation tokenization\n",
    "\n",
    "# Download spaCy models for multiple languages\n",
    "!python -m spacy download en_core_web_sm  # English\n",
    "!python -m spacy download es_core_news_sm  # Spanish\n",
    "!python -m spacy download fr_core_news_sm  # French\n",
    "!python -m spacy download de_core_news_sm  # German\n",
    "!python -m spacy download it_core_news_sm  # Italian\n",
    "!python -m spacy download pt_core_news_sm  # Portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "import spacy\n",
    "from langdetect import detect, detect_langs\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    MarianMTModel, \n",
    "    MarianTokenizer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy models for different languages\n",
    "# We'll use a dictionary to manage them\n",
    "\n",
    "SPACY_MODELS = {\n",
    "    'en': 'en_core_web_sm',\n",
    "    'es': 'es_core_news_sm',\n",
    "    'fr': 'fr_core_news_sm',\n",
    "    'de': 'de_core_news_sm',\n",
    "    'it': 'it_core_news_sm',\n",
    "    'pt': 'pt_core_news_sm'\n",
    "}\n",
    "\n",
    "LANGUAGE_NAMES = {\n",
    "    'en': 'English',\n",
    "    'es': 'Spanish',\n",
    "    'fr': 'French',\n",
    "    'de': 'German',\n",
    "    'it': 'Italian',\n",
    "    'pt': 'Portuguese'\n",
    "}\n",
    "\n",
    "def load_spacy_model(lang_code):\n",
    "    \"\"\"Load spaCy model for a given language code.\"\"\"\n",
    "    if lang_code in SPACY_MODELS:\n",
    "        try:\n",
    "            return spacy.load(SPACY_MODELS[lang_code])\n",
    "        except OSError:\n",
    "            print(f\"Model for {lang_code} not found. Using English as fallback.\")\n",
    "            return spacy.load('en_core_web_sm')\n",
    "    else:\n",
    "        print(f\"Language {lang_code} not supported. Using English.\")\n",
    "        return spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load English model as default\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "print(\"Default English model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Language Detection\n",
    "\n",
    "The first step in a multilingual pipeline is identifying what language the text is in. This determines which models and resources to use.\n",
    "\n",
    "**Methods:**\n",
    "- **langdetect**: Fast, statistical approach based on character n-grams\n",
    "- **Transformer-based**: More accurate but slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDetector:\n",
    "    \"\"\"\n",
    "    Detect language of text using multiple methods.\n",
    "    \n",
    "    Methods:\n",
    "    - langdetect: Fast, statistical (default)\n",
    "    - transformer: More accurate using XLM-RoBERTa\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='langdetect'):\n",
    "        self.method = method\n",
    "        self.supported_languages = list(LANGUAGE_NAMES.keys())\n",
    "        \n",
    "        if method == 'transformer':\n",
    "            # Load language detection pipeline\n",
    "            self.detector = pipeline(\n",
    "                'text-classification',\n",
    "                model='papluca/xlm-roberta-base-language-detection',\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "    \n",
    "    def detect(self, text):\n",
    "        \"\"\"\n",
    "        Detect the language of given text.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (language_code, confidence)\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) < 3:\n",
    "            return ('unknown', 0.0)\n",
    "        \n",
    "        if self.method == 'langdetect':\n",
    "            return self._detect_langdetect(text)\n",
    "        else:\n",
    "            return self._detect_transformer(text)\n",
    "    \n",
    "    def _detect_langdetect(self, text):\n",
    "        \"\"\"Use langdetect library for detection.\"\"\"\n",
    "        try:\n",
    "            langs = detect_langs(text)\n",
    "            top_lang = langs[0]\n",
    "            return (top_lang.lang, top_lang.prob)\n",
    "        except:\n",
    "            return ('unknown', 0.0)\n",
    "    \n",
    "    def _detect_transformer(self, text):\n",
    "        \"\"\"Use transformer model for detection.\"\"\"\n",
    "        result = self.detector(text[:512])[0]  # Limit text length\n",
    "        return (result['label'], result['score'])\n",
    "    \n",
    "    def detect_batch(self, texts):\n",
    "        \"\"\"Detect languages for multiple texts.\"\"\"\n",
    "        return [self.detect(text) for text in texts]\n",
    "\n",
    "# Initialize detector\n",
    "lang_detector = LanguageDetector(method='langdetect')\n",
    "\n",
    "# Test with sample texts in different languages\n",
    "sample_texts = [\n",
    "    \"Hello, how are you today? This is a test in English.\",\n",
    "    \"Hola, Â¿cÃ³mo estÃ¡s hoy? Esta es una prueba en espaÃ±ol.\",\n",
    "    \"Bonjour, comment allez-vous aujourd'hui? Ceci est un test en franÃ§ais.\",\n",
    "    \"Hallo, wie geht es Ihnen heute? Dies ist ein Test auf Deutsch.\",\n",
    "    \"Ciao, come stai oggi? Questo Ã¨ un test in italiano.\",\n",
    "    \"OlÃ¡, como vocÃª estÃ¡ hoje? Este Ã© um teste em portuguÃªs.\"\n",
    "]\n",
    "\n",
    "print(\"Language Detection Results:\")\n",
    "print(\"=\" * 60)\n",
    "for text in sample_texts:\n",
    "    lang, conf = lang_detector.detect(text)\n",
    "    lang_name = LANGUAGE_NAMES.get(lang, lang)\n",
    "    print(f\"Language: {lang_name:12} | Confidence: {conf:.2%}\")\n",
    "    print(f\"  Text: {text[:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multilingual Tokenization\n",
    "\n",
    "Tokenization varies by language:\n",
    "- **English**: Space-based with punctuation handling\n",
    "- **German**: Compound words need special handling\n",
    "- **Chinese/Japanese**: No spaces between words\n",
    "- **Romance languages**: Similar to English but with contractions\n",
    "\n",
    "We'll use spaCy for tokenization as it has language-specific rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenize text in multiple languages using spaCy.\n",
    "    \n",
    "    Features:\n",
    "    - Language-specific tokenization rules\n",
    "    - Handles punctuation, contractions, compounds\n",
    "    - Returns token objects with metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp_models = {}\n",
    "        self.lang_detector = LanguageDetector()\n",
    "    \n",
    "    def _get_nlp(self, lang_code):\n",
    "        \"\"\"Get or load spaCy model for language.\"\"\"\n",
    "        if lang_code not in self.nlp_models:\n",
    "            self.nlp_models[lang_code] = load_spacy_model(lang_code)\n",
    "        return self.nlp_models[lang_code]\n",
    "    \n",
    "    def tokenize(self, text, lang=None):\n",
    "        \"\"\"\n",
    "        Tokenize text into words.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            lang: Language code (auto-detected if None)\n",
    "            \n",
    "        Returns:\n",
    "            list: List of token strings\n",
    "        \"\"\"\n",
    "        if lang is None:\n",
    "            lang, _ = self.lang_detector.detect(text)\n",
    "        \n",
    "        nlp = self._get_nlp(lang)\n",
    "        doc = nlp(text)\n",
    "        return [token.text for token in doc]\n",
    "    \n",
    "    def tokenize_detailed(self, text, lang=None):\n",
    "        \"\"\"\n",
    "        Tokenize with detailed information.\n",
    "        \n",
    "        Returns:\n",
    "            list: List of dicts with token details\n",
    "        \"\"\"\n",
    "        if lang is None:\n",
    "            lang, _ = self.lang_detector.detect(text)\n",
    "        \n",
    "        nlp = self._get_nlp(lang)\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        return [{\n",
    "            'text': token.text,\n",
    "            'lemma': token.lemma_,\n",
    "            'pos': token.pos_,\n",
    "            'is_punct': token.is_punct,\n",
    "            'is_stop': token.is_stop,\n",
    "            'is_alpha': token.is_alpha\n",
    "        } for token in doc]\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = MultilingualTokenizer()\n",
    "\n",
    "# Test tokenization\n",
    "test_text_en = \"I can't believe it's already 2024! Machine learning is amazing.\"\n",
    "test_text_de = \"Ich kann es nicht glauben! Maschinelles Lernen ist erstaunlich.\"\n",
    "\n",
    "print(\"English Tokenization:\")\n",
    "tokens_en = tokenizer.tokenize(test_text_en, lang='en')\n",
    "print(f\"Text: {test_text_en}\")\n",
    "print(f\"Tokens: {tokens_en}\")\n",
    "print()\n",
    "\n",
    "print(\"German Tokenization:\")\n",
    "tokens_de = tokenizer.tokenize(test_text_de, lang='de')\n",
    "print(f\"Text: {test_text_de}\")\n",
    "print(f\"Tokens: {tokens_de}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed tokenization example\n",
    "print(\"Detailed Token Analysis (English):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "details = tokenizer.tokenize_detailed(test_text_en, lang='en')\n",
    "df_tokens = pd.DataFrame(details)\n",
    "print(df_tokens.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Cleaning\n",
    "\n",
    "Text cleaning removes noise while preserving meaningful content. Multilingual considerations:\n",
    "- **Accented characters**: Keep them! (cafÃ©, rÃ©sumÃ©, naÃ¯ve)\n",
    "- **Unicode normalization**: Standardize character representations\n",
    "- **Language-specific patterns**: Email, URLs, mentions are universal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "class MultilingualTextCleaner:\n",
    "    \"\"\"\n",
    "    Clean text while preserving language-specific characters.\n",
    "    \n",
    "    Features:\n",
    "    - Remove URLs, emails, mentions\n",
    "    - Normalize unicode\n",
    "    - Handle emojis\n",
    "    - Preserve accented characters\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Regex patterns\n",
    "        self.url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        self.email_pattern = re.compile(r'\\S+@\\S+\\.\\S+')\n",
    "        self.mention_pattern = re.compile(r'@\\w+')\n",
    "        self.hashtag_pattern = re.compile(r'#\\w+')\n",
    "        self.number_pattern = re.compile(r'\\d+')\n",
    "        self.extra_space_pattern = re.compile(r'\\s+')\n",
    "        \n",
    "        # Emoji pattern (covers most common emojis)\n",
    "        self.emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            \"]+\",\n",
    "            flags=re.UNICODE\n",
    "        )\n",
    "    \n",
    "    def clean(self, text, \n",
    "              remove_urls=True,\n",
    "              remove_emails=True,\n",
    "              remove_mentions=True,\n",
    "              remove_hashtags=False,\n",
    "              remove_numbers=False,\n",
    "              remove_emojis=True,\n",
    "              lowercase=True,\n",
    "              normalize_unicode=True):\n",
    "        \"\"\"\n",
    "        Clean text with configurable options.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            remove_*: Boolean flags for what to remove\n",
    "            lowercase: Convert to lowercase\n",
    "            normalize_unicode: Apply NFC normalization\n",
    "            \n",
    "        Returns:\n",
    "            str: Cleaned text\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Unicode normalization (NFC keeps accented chars as single chars)\n",
    "        if normalize_unicode:\n",
    "            text = unicodedata.normalize('NFC', text)\n",
    "        \n",
    "        # Remove patterns\n",
    "        if remove_urls:\n",
    "            text = self.url_pattern.sub(' ', text)\n",
    "        if remove_emails:\n",
    "            text = self.email_pattern.sub(' ', text)\n",
    "        if remove_mentions:\n",
    "            text = self.mention_pattern.sub(' ', text)\n",
    "        if remove_hashtags:\n",
    "            text = self.hashtag_pattern.sub(' ', text)\n",
    "        if remove_numbers:\n",
    "            text = self.number_pattern.sub(' ', text)\n",
    "        if remove_emojis:\n",
    "            text = self.emoji_pattern.sub(' ', text)\n",
    "        \n",
    "        # Lowercase\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = self.extra_space_pattern.sub(' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def clean_batch(self, texts, **kwargs):\n",
    "        \"\"\"Clean multiple texts.\"\"\"\n",
    "        return [self.clean(text, **kwargs) for text in texts]\n",
    "\n",
    "# Initialize cleaner\n",
    "cleaner = MultilingualTextCleaner()\n",
    "\n",
    "# Test with noisy multilingual text\n",
    "noisy_texts = [\n",
    "    \"Check out https://example.com! Contact me@email.com #NLP @user ðŸ˜€\",\n",
    "    \"Â¡Hola! Visita https://sitio.es Â¿QuÃ© tal? ðŸŽ‰ #espaÃ±ol\",\n",
    "    \"CafÃ©, rÃ©sumÃ©, naÃ¯ve - keeping accents! Visit http://test.fr\",\n",
    "    \"GrÃ¶ÃŸe und Ã„pfel - German umlauts! ðŸ“§ test@mail.de\"\n",
    "]\n",
    "\n",
    "print(\"Text Cleaning Results:\")\n",
    "print(\"=\" * 70)\n",
    "for text in noisy_texts:\n",
    "    cleaned = cleaner.clean(text)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Cleaned:  {cleaned}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. POS Tagging Across Languages\n",
    "\n",
    "Part-of-Speech (POS) tagging identifies grammatical categories:\n",
    "- **NOUN**: Nouns\n",
    "- **VERB**: Verbs\n",
    "- **ADJ**: Adjectives\n",
    "- **ADV**: Adverbs\n",
    "- **PROPN**: Proper nouns\n",
    "\n",
    "spaCy uses Universal Dependencies tagset - consistent across languages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualPOSTagger:\n",
    "    \"\"\"\n",
    "    POS tagging for multiple languages using spaCy.\n",
    "    \n",
    "    Uses Universal Dependencies tagset for consistency.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Universal POS tags description\n",
    "    POS_DESCRIPTIONS = {\n",
    "        'ADJ': 'Adjective',\n",
    "        'ADP': 'Adposition (preposition)',\n",
    "        'ADV': 'Adverb',\n",
    "        'AUX': 'Auxiliary verb',\n",
    "        'CCONJ': 'Coordinating conjunction',\n",
    "        'DET': 'Determiner',\n",
    "        'INTJ': 'Interjection',\n",
    "        'NOUN': 'Noun',\n",
    "        'NUM': 'Numeral',\n",
    "        'PART': 'Particle',\n",
    "        'PRON': 'Pronoun',\n",
    "        'PROPN': 'Proper noun',\n",
    "        'PUNCT': 'Punctuation',\n",
    "        'SCONJ': 'Subordinating conjunction',\n",
    "        'SYM': 'Symbol',\n",
    "        'VERB': 'Verb',\n",
    "        'X': 'Other'\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp_models = {}\n",
    "        self.lang_detector = LanguageDetector()\n",
    "    \n",
    "    def _get_nlp(self, lang_code):\n",
    "        \"\"\"Get or load spaCy model for language.\"\"\"\n",
    "        if lang_code not in self.nlp_models:\n",
    "            self.nlp_models[lang_code] = load_spacy_model(lang_code)\n",
    "        return self.nlp_models[lang_code]\n",
    "    \n",
    "    def tag(self, text, lang=None):\n",
    "        \"\"\"\n",
    "        Tag text with POS labels.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            lang: Language code (auto-detected if None)\n",
    "            \n",
    "        Returns:\n",
    "            list: List of (token, pos_tag) tuples\n",
    "        \"\"\"\n",
    "        if lang is None:\n",
    "            lang, _ = self.lang_detector.detect(text)\n",
    "        \n",
    "        nlp = self._get_nlp(lang)\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        return [(token.text, token.pos_) for token in doc]\n",
    "    \n",
    "    def tag_detailed(self, text, lang=None):\n",
    "        \"\"\"\n",
    "        Tag with detailed morphological information.\n",
    "        \n",
    "        Returns:\n",
    "            list: List of dicts with detailed POS info\n",
    "        \"\"\"\n",
    "        if lang is None:\n",
    "            lang, _ = self.lang_detector.detect(text)\n",
    "        \n",
    "        nlp = self._get_nlp(lang)\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        return [{\n",
    "            'token': token.text,\n",
    "            'pos': token.pos_,\n",
    "            'pos_desc': self.POS_DESCRIPTIONS.get(token.pos_, 'Unknown'),\n",
    "            'tag': token.tag_,  # Fine-grained tag\n",
    "            'dep': token.dep_,  # Dependency relation\n",
    "            'morph': str(token.morph)  # Morphological features\n",
    "        } for token in doc]\n",
    "    \n",
    "    def get_pos_distribution(self, text, lang=None):\n",
    "        \"\"\"Get distribution of POS tags in text.\"\"\"\n",
    "        tags = self.tag(text, lang)\n",
    "        pos_counts = Counter(tag for _, tag in tags)\n",
    "        return dict(pos_counts)\n",
    "\n",
    "# Initialize POS tagger\n",
    "pos_tagger = MultilingualPOSTagger()\n",
    "\n",
    "# Test on multiple languages\n",
    "test_sentences = {\n",
    "    'en': \"The quick brown fox jumps over the lazy dog.\",\n",
    "    'es': \"El rÃ¡pido zorro marrÃ³n salta sobre el perro perezoso.\",\n",
    "    'fr': \"Le renard brun rapide saute par-dessus le chien paresseux.\",\n",
    "    'de': \"Der schnelle braune Fuchs springt Ã¼ber den faulen Hund.\"\n",
    "}\n",
    "\n",
    "print(\"POS Tagging Across Languages:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for lang, sentence in test_sentences.items():\n",
    "    print(f\"\\n{LANGUAGE_NAMES[lang]}:\")\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    tags = pos_tagger.tag(sentence, lang)\n",
    "    print(\"Tags:\", [(t, p) for t, p in tags if p != 'PUNCT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize POS distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for ax, (lang, sentence) in zip(axes.flatten(), test_sentences.items()):\n",
    "    dist = pos_tagger.get_pos_distribution(sentence, lang)\n",
    "    # Remove punctuation from visualization\n",
    "    dist.pop('PUNCT', None)\n",
    "    dist.pop('SPACE', None)\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(dist)))\n",
    "    ax.bar(dist.keys(), dist.values(), color=colors)\n",
    "    ax.set_title(f'{LANGUAGE_NAMES[lang]} POS Distribution')\n",
    "    ax.set_xlabel('POS Tag')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stopword Removal\n",
    "\n",
    "Stopwords are common words that carry little semantic meaning:\n",
    "- **English**: the, is, at, which, on\n",
    "- **Spanish**: el, la, de, en, y\n",
    "- **French**: le, la, de, et, est\n",
    "\n",
    "We'll combine stopwords from NLTK and spaCy for comprehensive coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualStopwordRemover:\n",
    "    \"\"\"\n",
    "    Remove stopwords from text in multiple languages.\n",
    "    \n",
    "    Combines stopwords from NLTK and spaCy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # NLTK language codes mapping\n",
    "    NLTK_LANG_MAP = {\n",
    "        'en': 'english',\n",
    "        'es': 'spanish',\n",
    "        'fr': 'french',\n",
    "        'de': 'german',\n",
    "        'it': 'italian',\n",
    "        'pt': 'portuguese'\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stopwords_cache = {}\n",
    "        self.nlp_models = {}\n",
    "        self.lang_detector = LanguageDetector()\n",
    "    \n",
    "    def _get_stopwords(self, lang_code):\n",
    "        \"\"\"Get combined stopwords for language.\"\"\"\n",
    "        if lang_code in self.stopwords_cache:\n",
    "            return self.stopwords_cache[lang_code]\n",
    "        \n",
    "        combined_stopwords = set()\n",
    "        \n",
    "        # NLTK stopwords\n",
    "        if lang_code in self.NLTK_LANG_MAP:\n",
    "            try:\n",
    "                nltk_stops = set(stopwords.words(self.NLTK_LANG_MAP[lang_code]))\n",
    "                combined_stopwords.update(nltk_stops)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # spaCy stopwords\n",
    "        if lang_code not in self.nlp_models:\n",
    "            self.nlp_models[lang_code] = load_spacy_model(lang_code)\n",
    "        \n",
    "        nlp = self.nlp_models[lang_code]\n",
    "        spacy_stops = nlp.Defaults.stop_words\n",
    "        combined_stopwords.update(spacy_stops)\n",
    "        \n",
    "        self.stopwords_cache[lang_code] = combined_stopwords\n",
    "        return combined_stopwords\n",
    "    \n",
    "    def remove_stopwords(self, text, lang=None, return_removed=False):\n",
    "        \"\"\"\n",
    "        Remove stopwords from text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            lang: Language code (auto-detected if None)\n",
    "            return_removed: If True, also return removed words\n",
    "            \n",
    "        Returns:\n",
    "            str or tuple: Cleaned text (and removed words if requested)\n",
    "        \"\"\"\n",
    "        if lang is None:\n",
    "            lang, _ = self.lang_detector.detect(text)\n",
    "        \n",
    "        stops = self._get_stopwords(lang)\n",
    "        \n",
    "        # Tokenize and filter\n",
    "        if lang not in self.nlp_models:\n",
    "            self.nlp_models[lang] = load_spacy_model(lang)\n",
    "        \n",
    "        nlp = self.nlp_models[lang]\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        kept_tokens = []\n",
    "        removed_tokens = []\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.text.lower() in stops or token.is_stop:\n",
    "                removed_tokens.append(token.text)\n",
    "            else:\n",
    "                kept_tokens.append(token.text)\n",
    "        \n",
    "        result = ' '.join(kept_tokens)\n",
    "        \n",
    "        if return_removed:\n",
    "            return result, removed_tokens\n",
    "        return result\n",
    "    \n",
    "    def get_stopword_count(self, lang_code):\n",
    "        \"\"\"Get number of stopwords for a language.\"\"\"\n",
    "        return len(self._get_stopwords(lang_code))\n",
    "\n",
    "# Initialize stopword remover\n",
    "stopword_remover = MultilingualStopwordRemover()\n",
    "\n",
    "# Show stopword counts per language\n",
    "print(\"Stopword Counts by Language:\")\n",
    "print(\"=\" * 40)\n",
    "for lang in LANGUAGE_NAMES.keys():\n",
    "    count = stopword_remover.get_stopword_count(lang)\n",
    "    print(f\"{LANGUAGE_NAMES[lang]:12}: {count:4} stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test stopword removal\n",
    "test_texts = {\n",
    "    'en': \"The quick brown fox jumps over the lazy dog in the park.\",\n",
    "    'es': \"El rÃ¡pido zorro marrÃ³n salta sobre el perro perezoso en el parque.\",\n",
    "    'fr': \"Le renard brun rapide saute par-dessus le chien paresseux dans le parc.\",\n",
    "    'de': \"Der schnelle braune Fuchs springt Ã¼ber den faulen Hund im Park.\"\n",
    "}\n",
    "\n",
    "print(\"Stopword Removal Results:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for lang, text in test_texts.items():\n",
    "    cleaned, removed = stopword_remover.remove_stopwords(text, lang, return_removed=True)\n",
    "    print(f\"\\n{LANGUAGE_NAMES[lang]}:\")\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Cleaned:  {cleaned}\")\n",
    "    print(f\"Removed:  {removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lemmatization\n",
    "\n",
    "Lemmatization reduces words to their base/dictionary form:\n",
    "- **running â†’ run**\n",
    "- **better â†’ good**\n",
    "- **corriendo â†’ correr** (Spanish)\n",
    "\n",
    "Unlike stemming, lemmatization uses vocabulary and morphological analysis for accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualLemmatizer:\n",
    "    \"\"\"\n",
    "    Lemmatize text in multiple languages using spaCy.\n",
    "    \n",
    "    Lemmatization reduces words to their dictionary form\n",
    "    while considering context and part of speech.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp_models = {}\n",
    "        self.lang_detector = LanguageDetector()\n",
    "    \n",
    "    def _get_nlp(self, lang_code):\n",
    "        \"\"\"Get or load spaCy model for language.\"\"\"\n",
    "        if lang_code not in self.nlp_models:\n",
    "            self.nlp_models[lang_code] = load_spacy_model(lang_code)\n",
    "        return self.nlp_models[lang_code]\n",
    "    \n",
    "    def lemmatize(self, text, lang=None):\n",
    "        \"\"\"\n",
    "        Lemmatize text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            lang: Language code (auto-detected if None)\n",
    "            \n",
    "        Returns:\n",
    "            str: Lemmatized text\n",
    "        \"\"\"\n",
    "        if lang is None:\n",
    "            lang, _ = self.lang_detector.detect(text)\n",
    "        \n",
    "        nlp = self._get_nlp(lang)\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        return ' '.join([token.lemma_ for token in doc])\n",
    "    \n",
    "    def lemmatize_detailed(self, text, lang=None):\n",
    "        \"\"\"\n",
    "        Lemmatize with word-by-word comparison.\n",
    "        \n",
    "        Returns:\n",
    "            list: List of (original, lemma) tuples\n",
    "        \"\"\"\n",
    "        if lang is None:\n",
    "            lang, _ = self.lang_detector.detect(text)\n",
    "        \n",
    "        nlp = self._get_nlp(lang)\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        return [(token.text, token.lemma_) for token in doc]\n",
    "    \n",
    "    def get_lemma_mapping(self, text, lang=None):\n",
    "        \"\"\"\n",
    "        Get mapping of changed words only.\n",
    "        \n",
    "        Returns:\n",
    "            dict: {original: lemma} for changed words\n",
    "        \"\"\"\n",
    "        pairs = self.lemmatize_detailed(text, lang)\n",
    "        return {orig: lemma for orig, lemma in pairs \n",
    "                if orig.lower() != lemma.lower()}\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = MultilingualLemmatizer()\n",
    "\n",
    "# Test lemmatization across languages\n",
    "test_sentences = {\n",
    "    'en': \"The cats were running and jumping over the fallen leaves.\",\n",
    "    'es': \"Los gatos estaban corriendo y saltando sobre las hojas caÃ­das.\",\n",
    "    'fr': \"Les chats couraient et sautaient par-dessus les feuilles tombÃ©es.\",\n",
    "    'de': \"Die Katzen rannten und sprangen Ã¼ber die gefallenen BlÃ¤tter.\"\n",
    "}\n",
    "\n",
    "print(\"Lemmatization Results:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for lang, sentence in test_sentences.items():\n",
    "    lemmatized = lemmatizer.lemmatize(sentence, lang)\n",
    "    changes = lemmatizer.get_lemma_mapping(sentence, lang)\n",
    "    \n",
    "    print(f\"\\n{LANGUAGE_NAMES[lang]}:\")\n",
    "    print(f\"Original:   {sentence}\")\n",
    "    print(f\"Lemmatized: {lemmatized}\")\n",
    "    print(f\"Changes:    {changes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete Pipeline Class\n",
    "\n",
    "Now let's combine all components into a unified multilingual NLP pipeline that follows the flow:\n",
    "\n",
    "```\n",
    "Raw Text â†’ Language Detection â†’ Tokenization â†’ Text Cleaning â†’ \n",
    "POS Tagging â†’ Stopword Removal â†’ Lemmatization â†’ Cleaned Text â†’ ML Model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualNLPPipeline:\n",
    "    \"\"\"\n",
    "    Complete multilingual NLP preprocessing pipeline.\n",
    "    \n",
    "    Pipeline stages:\n",
    "    1. Language Detection\n",
    "    2. Tokenization\n",
    "    3. Text Cleaning\n",
    "    4. POS Tagging\n",
    "    5. Stopword Removal\n",
    "    6. Lemmatization\n",
    "    \n",
    "    Output ready for embeddings or ML models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lang_detector = LanguageDetector()\n",
    "        self.tokenizer = MultilingualTokenizer()\n",
    "        self.cleaner = MultilingualTextCleaner()\n",
    "        self.pos_tagger = MultilingualPOSTagger()\n",
    "        self.stopword_remover = MultilingualStopwordRemover()\n",
    "        self.lemmatizer = MultilingualLemmatizer()\n",
    "        \n",
    "        self.supported_languages = list(LANGUAGE_NAMES.keys())\n",
    "    \n",
    "    def process(self, text, \n",
    "                lang=None,\n",
    "                clean=True,\n",
    "                remove_stopwords=True,\n",
    "                lemmatize=True,\n",
    "                lowercase=True,\n",
    "                remove_punctuation=True,\n",
    "                return_details=False):\n",
    "        \"\"\"\n",
    "        Process text through the complete pipeline.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            lang: Language code (auto-detected if None)\n",
    "            clean: Apply text cleaning\n",
    "            remove_stopwords: Remove stopwords\n",
    "            lemmatize: Apply lemmatization\n",
    "            lowercase: Convert to lowercase\n",
    "            remove_punctuation: Remove punctuation tokens\n",
    "            return_details: Return detailed processing info\n",
    "            \n",
    "        Returns:\n",
    "            str or dict: Processed text (or detailed results)\n",
    "        \"\"\"\n",
    "        details = {\n",
    "            'original': text,\n",
    "            'steps': []\n",
    "        }\n",
    "        \n",
    "        # Step 1: Language Detection\n",
    "        if lang is None:\n",
    "            lang, conf = self.lang_detector.detect(text)\n",
    "            details['detected_language'] = lang\n",
    "            details['language_confidence'] = conf\n",
    "        else:\n",
    "            details['detected_language'] = lang\n",
    "            details['language_confidence'] = 1.0\n",
    "        \n",
    "        details['steps'].append(('language_detection', lang))\n",
    "        \n",
    "        # Step 2: Text Cleaning\n",
    "        if clean:\n",
    "            text = self.cleaner.clean(text, lowercase=lowercase)\n",
    "            details['steps'].append(('cleaning', text))\n",
    "        \n",
    "        # Step 3: Tokenization & POS Tagging\n",
    "        pos_tags = self.pos_tagger.tag(text, lang)\n",
    "        details['pos_tags'] = pos_tags\n",
    "        details['steps'].append(('pos_tagging', pos_tags))\n",
    "        \n",
    "        # Step 4: Stopword Removal\n",
    "        if remove_stopwords:\n",
    "            text, removed = self.stopword_remover.remove_stopwords(\n",
    "                text, lang, return_removed=True\n",
    "            )\n",
    "            details['removed_stopwords'] = removed\n",
    "            details['steps'].append(('stopword_removal', text))\n",
    "        \n",
    "        # Step 5: Lemmatization\n",
    "        if lemmatize:\n",
    "            text = self.lemmatizer.lemmatize(text, lang)\n",
    "            details['steps'].append(('lemmatization', text))\n",
    "        \n",
    "        # Step 6: Remove Punctuation (if requested)\n",
    "        if remove_punctuation:\n",
    "            # Keep only alphanumeric tokens\n",
    "            tokens = text.split()\n",
    "            tokens = [t for t in tokens if any(c.isalnum() for c in t)]\n",
    "            text = ' '.join(tokens)\n",
    "            details['steps'].append(('punctuation_removal', text))\n",
    "        \n",
    "        details['processed'] = text\n",
    "        \n",
    "        if return_details:\n",
    "            return details\n",
    "        return text\n",
    "    \n",
    "    def process_batch(self, texts, **kwargs):\n",
    "        \"\"\"\n",
    "        Process multiple texts.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of input texts\n",
    "            **kwargs: Arguments passed to process()\n",
    "            \n",
    "        Returns:\n",
    "            list: List of processed texts\n",
    "        \"\"\"\n",
    "        return [self.process(text, **kwargs) for text in texts]\n",
    "    \n",
    "    def get_pipeline_info(self):\n",
    "        \"\"\"Get information about the pipeline.\"\"\"\n",
    "        return {\n",
    "            'supported_languages': self.supported_languages,\n",
    "            'language_names': LANGUAGE_NAMES,\n",
    "            'pipeline_stages': [\n",
    "                'Language Detection',\n",
    "                'Text Cleaning',\n",
    "                'Tokenization',\n",
    "                'POS Tagging',\n",
    "                'Stopword Removal',\n",
    "                'Lemmatization',\n",
    "                'Punctuation Removal'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = MultilingualNLPPipeline()\n",
    "\n",
    "print(\"Pipeline Info:\")\n",
    "print(\"=\" * 50)\n",
    "info = pipeline.get_pipeline_info()\n",
    "print(f\"Supported Languages: {list(info['language_names'].values())}\")\n",
    "print(f\"\\nPipeline Stages:\")\n",
    "for i, stage in enumerate(info['pipeline_stages'], 1):\n",
    "    print(f\"  {i}. {stage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete pipeline\n",
    "test_texts = [\n",
    "    \"I'm loving these amazing new features! Check https://example.com @user #NLP ðŸ˜Š\",\n",
    "    \"Â¡Me encantan estas nuevas caracterÃ­sticas increÃ­bles! Visita https://sitio.es ðŸŽ‰\",\n",
    "    \"J'adore ces nouvelles fonctionnalitÃ©s incroyables! Visitez https://site.fr ðŸ‡«ðŸ‡·\",\n",
    "    \"Ich liebe diese erstaunlichen neuen Funktionen! Besuchen Sie https://seite.de ðŸ‡©ðŸ‡ª\"\n",
    "]\n",
    "\n",
    "print(\"Complete Pipeline Processing:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for text in test_texts:\n",
    "    result = pipeline.process(text, return_details=True)\n",
    "    \n",
    "    print(f\"\\nLanguage: {LANGUAGE_NAMES.get(result['detected_language'], 'Unknown')}\")\n",
    "    print(f\"Confidence: {result['language_confidence']:.2%}\")\n",
    "    print(f\"\\nOriginal:  {result['original']}\")\n",
    "    print(f\"Processed: {result['processed']}\")\n",
    "    print(f\"\\nStopwords Removed: {result.get('removed_stopwords', [])[:5]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Multilingual Embeddings\n",
    "\n",
    "Multilingual embeddings map text from different languages into a shared vector space.\n",
    "\n",
    "**Key Models:**\n",
    "- **mBERT**: Multilingual BERT (104 languages)\n",
    "- **XLM-RoBERTa**: Cross-lingual Language Model (100 languages)\n",
    "- **LaBSE**: Language-agnostic BERT Sentence Embeddings\n",
    "\n",
    "These models enable:\n",
    "- Cross-lingual similarity search\n",
    "- Zero-shot cross-lingual transfer\n",
    "- Multilingual classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualEmbedder:\n",
    "    \"\"\"\n",
    "    Generate multilingual embeddings using transformer models.\n",
    "    \n",
    "    Models:\n",
    "    - mbert: Multilingual BERT\n",
    "    - xlm-roberta: XLM-RoBERTa\n",
    "    - labse: Language-agnostic BERT Sentence Embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    MODEL_CONFIGS = {\n",
    "        'mbert': 'bert-base-multilingual-cased',\n",
    "        'xlm-roberta': 'xlm-roberta-base',\n",
    "        'labse': 'sentence-transformers/LaBSE'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model_name='xlm-roberta'):\n",
    "        \"\"\"\n",
    "        Initialize embedder with specified model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: One of 'mbert', 'xlm-roberta', 'labse'\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model_path = self.MODEL_CONFIGS.get(model_name, model_name)\n",
    "        \n",
    "        print(f\"Loading {model_name} model...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self.model = AutoModel.from_pretrained(self.model_path)\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"Model loaded on {self.device}\")\n",
    "    \n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"Apply mean pooling to get sentence embeddings.\"\"\"\n",
    "        token_embeddings = model_output[0]  # First element is token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def embed(self, text):\n",
    "        \"\"\"\n",
    "        Generate embedding for text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text (any language)\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Embedding vector\n",
    "        \"\"\"\n",
    "        # Tokenize\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoded = {k: v.to(self.device) for k, v in encoded.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**encoded)\n",
    "        \n",
    "        # Mean pooling\n",
    "        embedding = self._mean_pooling(output, encoded['attention_mask'])\n",
    "        \n",
    "        # Normalize\n",
    "        embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)\n",
    "        \n",
    "        return embedding.cpu().numpy()[0]\n",
    "    \n",
    "    def embed_batch(self, texts, batch_size=32):\n",
    "        \"\"\"\n",
    "        Generate embeddings for multiple texts.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of input texts\n",
    "            batch_size: Processing batch size\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Matrix of embeddings (n_texts x embedding_dim)\n",
    "        \"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            \n",
    "            encoded = self.tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            encoded = {k: v.to(self.device) for k, v in encoded.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = self.model(**encoded)\n",
    "            \n",
    "            embeddings = self._mean_pooling(output, encoded['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def compute_similarity(self, text1, text2):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two texts.\n",
    "        \n",
    "        Works across languages!\n",
    "        \"\"\"\n",
    "        emb1 = self.embed(text1)\n",
    "        emb2 = self.embed(text2)\n",
    "        return np.dot(emb1, emb2)\n",
    "\n",
    "# Initialize embedder (using XLM-RoBERTa for balance of speed and quality)\n",
    "print(\"Initializing Multilingual Embedder...\")\n",
    "embedder = MultilingualEmbedder('xlm-roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cross-lingual similarity\n",
    "# Same meaning in different languages should have high similarity\n",
    "\n",
    "parallel_sentences = [\n",
    "    (\"I love machine learning.\", \"Me encanta el aprendizaje automÃ¡tico.\"),  # EN-ES\n",
    "    (\"The weather is beautiful today.\", \"Le temps est magnifique aujourd'hui.\"),  # EN-FR\n",
    "    (\"I am learning to code.\", \"Ich lerne programmieren.\"),  # EN-DE\n",
    "]\n",
    "\n",
    "unrelated_sentences = [\n",
    "    (\"I love machine learning.\", \"Le temps est magnifique aujourd'hui.\"),  # Different topics\n",
    "    (\"The cat is sleeping.\", \"Ich fahre mit dem Auto.\"),  # Cat vs Car\n",
    "]\n",
    "\n",
    "print(\"Cross-Lingual Similarity (Same Meaning):\")\n",
    "print(\"=\" * 60)\n",
    "for en, other in parallel_sentences:\n",
    "    sim = embedder.compute_similarity(en, other)\n",
    "    print(f\"Similarity: {sim:.4f}\")\n",
    "    print(f\"  EN: {en}\")\n",
    "    print(f\"  --: {other}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nCross-Lingual Similarity (Different Meaning):\")\n",
    "print(\"=\" * 60)\n",
    "for text1, text2 in unrelated_sentences:\n",
    "    sim = embedder.compute_similarity(text1, text2)\n",
    "    print(f\"Similarity: {sim:.4f}\")\n",
    "    print(f\"  1: {text1}\")\n",
    "    print(f\"  2: {text2}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings in 2D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sentences about similar topics in different languages\n",
    "sentences = [\n",
    "    # Technology (cluster 1)\n",
    "    \"Artificial intelligence is transforming the world.\",\n",
    "    \"La inteligencia artificial estÃ¡ transformando el mundo.\",\n",
    "    \"L'intelligence artificielle transforme le monde.\",\n",
    "    \"KÃ¼nstliche Intelligenz verÃ¤ndert die Welt.\",\n",
    "    \n",
    "    # Nature (cluster 2)\n",
    "    \"The forest is full of beautiful trees.\",\n",
    "    \"El bosque estÃ¡ lleno de Ã¡rboles hermosos.\",\n",
    "    \"La forÃªt est pleine de beaux arbres.\",\n",
    "    \"Der Wald ist voller schÃ¶ner BÃ¤ume.\",\n",
    "    \n",
    "    # Food (cluster 3)\n",
    "    \"I enjoy eating pizza and pasta.\",\n",
    "    \"Me gusta comer pizza y pasta.\",\n",
    "    \"J'aime manger de la pizza et des pÃ¢tes.\",\n",
    "    \"Ich esse gerne Pizza und Pasta.\"\n",
    "]\n",
    "\n",
    "labels = ['EN-Tech', 'ES-Tech', 'FR-Tech', 'DE-Tech',\n",
    "          'EN-Nature', 'ES-Nature', 'FR-Nature', 'DE-Nature',\n",
    "          'EN-Food', 'ES-Food', 'FR-Food', 'DE-Food']\n",
    "\n",
    "topics = ['Technology'] * 4 + ['Nature'] * 4 + ['Food'] * 4\n",
    "languages = ['English', 'Spanish', 'French', 'German'] * 3\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedder.embed_batch(sentences)\n",
    "\n",
    "# Reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "colors = {'Technology': 'red', 'Nature': 'green', 'Food': 'blue'}\n",
    "markers = {'English': 'o', 'Spanish': 's', 'French': '^', 'German': 'D'}\n",
    "\n",
    "for i, (x, y) in enumerate(embeddings_2d):\n",
    "    ax.scatter(x, y, \n",
    "               c=colors[topics[i]], \n",
    "               marker=markers[languages[i]],\n",
    "               s=150,\n",
    "               alpha=0.7)\n",
    "    ax.annotate(labels[i], (x, y), fontsize=9, \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "# Legend\n",
    "from matplotlib.lines import Line2D\n",
    "topic_handles = [Line2D([0], [0], marker='o', color='w', \n",
    "                        markerfacecolor=c, markersize=10, label=t)\n",
    "                 for t, c in colors.items()]\n",
    "lang_handles = [Line2D([0], [0], marker=m, color='gray', \n",
    "                       markersize=10, label=l, linestyle='None')\n",
    "                for l, m in markers.items()]\n",
    "\n",
    "ax.legend(handles=topic_handles + lang_handles, \n",
    "          loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "\n",
    "ax.set_title('Multilingual Embeddings: Topics Cluster Across Languages')\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: Sentences about the same topic cluster together,\")\n",
    "print(\"regardless of language! This is the power of multilingual embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cross-Lingual Text Classification\n",
    "\n",
    "One powerful application of multilingual embeddings is **zero-shot cross-lingual transfer**:\n",
    "- Train a classifier in ONE language (e.g., English)\n",
    "- Apply it to OTHER languages without any training data in those languages!\n",
    "\n",
    "This is incredibly useful when you have labeled data in only one language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "class CrossLingualClassifier:\n",
    "    \"\"\"\n",
    "    Train on one language, predict on any language.\n",
    "    \n",
    "    Uses multilingual embeddings for zero-shot cross-lingual transfer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedder):\n",
    "        self.embedder = embedder\n",
    "        self.classifier = LogisticRegression(max_iter=1000)\n",
    "        self.is_trained = False\n",
    "        self.classes = None\n",
    "    \n",
    "    def train(self, texts, labels):\n",
    "        \"\"\"\n",
    "        Train classifier on labeled texts.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of training texts\n",
    "            labels: List of labels\n",
    "        \"\"\"\n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.embedder.embed_batch(texts)\n",
    "        \n",
    "        print(\"Training classifier...\")\n",
    "        self.classifier.fit(embeddings, labels)\n",
    "        self.classes = self.classifier.classes_\n",
    "        self.is_trained = True\n",
    "        print(\"Training complete!\")\n",
    "    \n",
    "    def predict(self, texts):\n",
    "        \"\"\"\n",
    "        Predict labels for texts (any language!).\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts to classify\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted labels\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Classifier not trained. Call train() first.\")\n",
    "        \n",
    "        embeddings = self.embedder.embed_batch(texts)\n",
    "        return self.classifier.predict(embeddings)\n",
    "    \n",
    "    def predict_proba(self, texts):\n",
    "        \"\"\"Get prediction probabilities.\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Classifier not trained. Call train() first.\")\n",
    "        \n",
    "        embeddings = self.embedder.embed_batch(texts)\n",
    "        return self.classifier.predict_proba(embeddings)\n",
    "    \n",
    "    def evaluate(self, texts, labels):\n",
    "        \"\"\"\n",
    "        Evaluate classifier on test data.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Evaluation metrics\n",
    "        \"\"\"\n",
    "        predictions = self.predict(texts)\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': predictions,\n",
    "            'report': classification_report(labels, predictions)\n",
    "        }\n",
    "\n",
    "# Create training data (English only)\n",
    "train_texts_en = [\n",
    "    # Positive sentiment\n",
    "    \"I love this product, it's amazing!\",\n",
    "    \"This is the best thing I've ever bought.\",\n",
    "    \"Absolutely fantastic experience!\",\n",
    "    \"Great quality and fast delivery.\",\n",
    "    \"I'm so happy with this purchase.\",\n",
    "    \"Excellent service, highly recommend!\",\n",
    "    \"This exceeded all my expectations.\",\n",
    "    \"Perfect, exactly what I needed.\",\n",
    "    \n",
    "    # Negative sentiment\n",
    "    \"Terrible product, waste of money.\",\n",
    "    \"I'm very disappointed with this.\",\n",
    "    \"The worst purchase I've ever made.\",\n",
    "    \"Poor quality, broke after one day.\",\n",
    "    \"Don't buy this, it's awful.\",\n",
    "    \"Horrible experience, never again.\",\n",
    "    \"Complete garbage, avoid at all costs.\",\n",
    "    \"This product is defective and useless.\"\n",
    "]\n",
    "\n",
    "train_labels = ['positive'] * 8 + ['negative'] * 8\n",
    "\n",
    "# Initialize and train\n",
    "classifier = CrossLingualClassifier(embedder)\n",
    "classifier.train(train_texts_en, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on multiple languages (zero-shot transfer!)\n",
    "test_data = {\n",
    "    'English': [\n",
    "        (\"This is wonderful, I love it!\", 'positive'),\n",
    "        (\"Terrible quality, very bad.\", 'negative'),\n",
    "    ],\n",
    "    'Spanish': [\n",
    "        (\"Â¡Esto es maravilloso, me encanta!\", 'positive'),\n",
    "        (\"Calidad terrible, muy malo.\", 'negative'),\n",
    "    ],\n",
    "    'French': [\n",
    "        (\"C'est merveilleux, j'adore!\", 'positive'),\n",
    "        (\"QualitÃ© terrible, trÃ¨s mauvais.\", 'negative'),\n",
    "    ],\n",
    "    'German': [\n",
    "        (\"Das ist wunderbar, ich liebe es!\", 'positive'),\n",
    "        (\"Schreckliche QualitÃ¤t, sehr schlecht.\", 'negative'),\n",
    "    ],\n",
    "    'Italian': [\n",
    "        (\"Questo Ã¨ meraviglioso, lo adoro!\", 'positive'),\n",
    "        (\"QualitÃ  terribile, molto cattivo.\", 'negative'),\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Zero-Shot Cross-Lingual Classification Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"(Trained ONLY on English data)\\n\")\n",
    "\n",
    "results = {}\n",
    "for lang, examples in test_data.items():\n",
    "    texts = [t for t, _ in examples]\n",
    "    true_labels = [l for _, l in examples]\n",
    "    \n",
    "    predictions = classifier.predict(texts)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    results[lang] = accuracy\n",
    "    \n",
    "    print(f\"{lang}:\")\n",
    "    for text, true, pred in zip(texts, true_labels, predictions):\n",
    "        status = \"âœ“\" if true == pred else \"âœ—\"\n",
    "        print(f\"  {status} '{text[:40]}...'\")\n",
    "        print(f\"    True: {true}, Pred: {pred}\")\n",
    "    print(f\"  Accuracy: {accuracy:.0%}\\n\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary - Accuracy by Language:\")\n",
    "print(\"-\" * 30)\n",
    "for lang, acc in results.items():\n",
    "    bar = \"â–ˆ\" * int(acc * 20)\n",
    "    print(f\"{lang:10} {bar} {acc:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Translation System\n",
    "\n",
    "Machine translation is another key component of multilingual NLP. We'll use MarianMT models from Helsinki-NLP, which are compact and effective.\n",
    "\n",
    "**Model naming convention:** `Helsinki-NLP/opus-mt-{src}-{tgt}`\n",
    "- `en-es`: English to Spanish\n",
    "- `es-en`: Spanish to English\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualTranslator:\n",
    "    \"\"\"\n",
    "    Translate between multiple languages using MarianMT.\n",
    "    \n",
    "    Supports: English, Spanish, French, German, Italian, Portuguese\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.tokenizers = {}\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Available translation pairs\n",
    "        self.supported_pairs = [\n",
    "            ('en', 'es'), ('es', 'en'),\n",
    "            ('en', 'fr'), ('fr', 'en'),\n",
    "            ('en', 'de'), ('de', 'en'),\n",
    "            ('en', 'it'), ('it', 'en'),\n",
    "            ('en', 'pt'), ('pt', 'en'),\n",
    "        ]\n",
    "    \n",
    "    def _get_model_name(self, src, tgt):\n",
    "        \"\"\"Get HuggingFace model name for language pair.\"\"\"\n",
    "        return f\"Helsinki-NLP/opus-mt-{src}-{tgt}\"\n",
    "    \n",
    "    def _load_model(self, src, tgt):\n",
    "        \"\"\"Load translation model for language pair.\"\"\"\n",
    "        pair = (src, tgt)\n",
    "        if pair not in self.models:\n",
    "            model_name = self._get_model_name(src, tgt)\n",
    "            print(f\"Loading translation model: {src} â†’ {tgt}...\")\n",
    "            \n",
    "            self.tokenizers[pair] = MarianTokenizer.from_pretrained(model_name)\n",
    "            self.models[pair] = MarianMTModel.from_pretrained(model_name)\n",
    "            self.models[pair].to(self.device)\n",
    "            self.models[pair].eval()\n",
    "        \n",
    "        return self.models[pair], self.tokenizers[pair]\n",
    "    \n",
    "    def translate(self, text, src_lang, tgt_lang):\n",
    "        \"\"\"\n",
    "        Translate text from source to target language.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            src_lang: Source language code\n",
    "            tgt_lang: Target language code\n",
    "            \n",
    "        Returns:\n",
    "            str: Translated text\n",
    "        \"\"\"\n",
    "        if (src_lang, tgt_lang) not in self.supported_pairs:\n",
    "            raise ValueError(f\"Translation pair {src_lang} â†’ {tgt_lang} not supported\")\n",
    "        \n",
    "        model, tokenizer = self._load_model(src_lang, tgt_lang)\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate translation\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_length=512)\n",
    "        \n",
    "        # Decode\n",
    "        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return translation\n",
    "    \n",
    "    def translate_batch(self, texts, src_lang, tgt_lang):\n",
    "        \"\"\"Translate multiple texts.\"\"\"\n",
    "        return [self.translate(text, src_lang, tgt_lang) for text in texts]\n",
    "    \n",
    "    def translate_to_english(self, text, src_lang):\n",
    "        \"\"\"Convenience method to translate any language to English.\"\"\"\n",
    "        if src_lang == 'en':\n",
    "            return text\n",
    "        return self.translate(text, src_lang, 'en')\n",
    "    \n",
    "    def get_supported_pairs(self):\n",
    "        \"\"\"Get list of supported translation pairs.\"\"\"\n",
    "        return self.supported_pairs\n",
    "\n",
    "# Initialize translator\n",
    "translator = MultilingualTranslator()\n",
    "\n",
    "print(\"Supported Translation Pairs:\")\n",
    "for src, tgt in translator.get_supported_pairs():\n",
    "    print(f\"  {LANGUAGE_NAMES[src]} â†’ {LANGUAGE_NAMES[tgt]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test translations\n",
    "test_text = \"Machine learning is revolutionizing how we solve complex problems.\"\n",
    "\n",
    "print(f\"Original (English): {test_text}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "target_languages = ['es', 'fr', 'de', 'it', 'pt']\n",
    "\n",
    "translations = {}\n",
    "for tgt in target_languages:\n",
    "    translation = translator.translate(test_text, 'en', tgt)\n",
    "    translations[tgt] = translation\n",
    "    print(f\"{LANGUAGE_NAMES[tgt]:12}: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round-trip translation test (translate and back)\n",
    "print(\"Round-Trip Translation Test:\")\n",
    "print(\"(English â†’ Target â†’ English)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "original = \"The quick brown fox jumps over the lazy dog.\"\n",
    "print(f\"\\nOriginal: {original}\\n\")\n",
    "\n",
    "for lang in ['es', 'fr', 'de']:\n",
    "    # Translate to target language\n",
    "    translated = translator.translate(original, 'en', lang)\n",
    "    # Translate back to English\n",
    "    back_translated = translator.translate(translated, lang, 'en')\n",
    "    \n",
    "    print(f\"{LANGUAGE_NAMES[lang]}:\")\n",
    "    print(f\"  â†’ {translated}\")\n",
    "    print(f\"  â† {back_translated}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "We've built a complete **Multilingual NLP Pipeline** covering:\n",
    "\n",
    "### Core Pipeline Components\n",
    "\n",
    "| Component | Purpose | Key Library |\n",
    "|-----------|---------|-------------|\n",
    "| Language Detection | Identify text language | langdetect |\n",
    "| Tokenization | Split text into tokens | spaCy |\n",
    "| Text Cleaning | Remove noise, normalize | regex, unicodedata |\n",
    "| POS Tagging | Identify parts of speech | spaCy |\n",
    "| Stopword Removal | Remove common words | NLTK + spaCy |\n",
    "| Lemmatization | Reduce to base form | spaCy |\n",
    "\n",
    "### Advanced Features\n",
    "\n",
    "| Feature | Purpose | Model |\n",
    "|---------|---------|-------|\n",
    "| Multilingual Embeddings | Shared vector space | XLM-RoBERTa |\n",
    "| Cross-lingual Classification | Zero-shot transfer | Embeddings + LogReg |\n",
    "| Machine Translation | Text translation | MarianMT |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Language-specific resources matter**: Different languages need different tokenization, stopwords, and lemmatization rules\n",
    "\n",
    "2. **Universal tagsets help**: POS tags using Universal Dependencies work across languages\n",
    "\n",
    "3. **Multilingual embeddings are powerful**: They map semantically similar content close together, regardless of language\n",
    "\n",
    "4. **Zero-shot transfer works**: Train in one language, apply to many others\n",
    "\n",
    "5. **Translation enables more**: When you need language-specific processing, translate first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: Complete workflow\n",
    "print(\"=\"*70)\n",
    "print(\"COMPLETE MULTILINGUAL NLP WORKFLOW DEMONSTRATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Input: Mixed language texts\n",
    "input_texts = [\n",
    "    \"I absolutely love this new AI technology! It's revolutionary.\",\n",
    "    \"Â¡Esta tecnologÃ­a de IA es increÃ­ble! CambiarÃ¡ todo.\",\n",
    "    \"Cette technologie d'IA est incroyable! Elle va tout changer.\",\n",
    "    \"Diese KI-Technologie ist unglaublich! Sie wird alles verÃ¤ndern.\"\n",
    "]\n",
    "\n",
    "print(\"\\n1. INPUT TEXTS (Multiple Languages):\")\n",
    "print(\"-\" * 50)\n",
    "for i, text in enumerate(input_texts, 1):\n",
    "    print(f\"   {i}. {text}\")\n",
    "\n",
    "print(\"\\n2. LANGUAGE DETECTION:\")\n",
    "print(\"-\" * 50)\n",
    "detected_langs = []\n",
    "for text in input_texts:\n",
    "    lang, conf = lang_detector.detect(text)\n",
    "    detected_langs.append(lang)\n",
    "    print(f\"   {LANGUAGE_NAMES.get(lang, lang):12} ({conf:.0%}): {text[:40]}...\")\n",
    "\n",
    "print(\"\\n3. PREPROCESSING (Pipeline):\")\n",
    "print(\"-\" * 50)\n",
    "processed_texts = []\n",
    "for text, lang in zip(input_texts, detected_langs):\n",
    "    processed = pipeline.process(text, lang=lang)\n",
    "    processed_texts.append(processed)\n",
    "    print(f\"   {LANGUAGE_NAMES.get(lang, lang):12}: {processed}\")\n",
    "\n",
    "print(\"\\n4. EMBEDDINGS & SIMILARITY:\")\n",
    "print(\"-\" * 50)\n",
    "embeddings = embedder.embed_batch(input_texts)\n",
    "print(f\"   Generated {embeddings.shape[0]} embeddings of dimension {embeddings.shape[1]}\")\n",
    "\n",
    "# Compute pairwise similarity\n",
    "print(\"   \\n   Cross-lingual similarity matrix:\")\n",
    "langs_short = ['EN', 'ES', 'FR', 'DE']\n",
    "print(f\"        {' '.join([f'{l:6}' for l in langs_short])}\")\n",
    "for i, lang in enumerate(langs_short):\n",
    "    sims = [np.dot(embeddings[i], embeddings[j]) for j in range(len(embeddings))]\n",
    "    sim_str = ' '.join([f'{s:.3f} ' for s in sims])\n",
    "    print(f\"   {lang}   {sim_str}\")\n",
    "\n",
    "print(\"\\n5. SENTIMENT CLASSIFICATION (Zero-shot):\")\n",
    "print(\"-\" * 50)\n",
    "predictions = classifier.predict(input_texts)\n",
    "for text, lang, pred in zip(input_texts, detected_langs, predictions):\n",
    "    print(f\"   {LANGUAGE_NAMES.get(lang, lang):12}: {pred:8} | {text[:35]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Pipeline complete! All texts processed through the multilingual NLP system.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pipeline components for reuse\n",
    "print(\"\\nPipeline Components Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "Classes implemented:\n",
    "--------------------\n",
    "1. LanguageDetector       - Detect language of text\n",
    "2. MultilingualTokenizer  - Language-aware tokenization\n",
    "3. MultilingualTextCleaner - Clean text preserving unicode\n",
    "4. MultilingualPOSTagger  - Universal POS tagging\n",
    "5. MultilingualStopwordRemover - Language-specific stopwords\n",
    "6. MultilingualLemmatizer - Reduce words to base forms\n",
    "7. MultilingualNLPPipeline - Complete preprocessing pipeline\n",
    "8. MultilingualEmbedder   - Generate cross-lingual embeddings\n",
    "9. CrossLingualClassifier - Train once, classify any language\n",
    "10. MultilingualTranslator - Translate between languages\n",
    "\n",
    "Supported Languages:\n",
    "-------------------\n",
    "- English (en)\n",
    "- Spanish (es)\n",
    "- French (fr)\n",
    "- German (de)\n",
    "- Italian (it)\n",
    "- Portuguese (pt)\n",
    "\n",
    "Key Models Used:\n",
    "---------------\n",
    "- spaCy multilingual models (for NLP processing)\n",
    "- XLM-RoBERTa (for embeddings)\n",
    "- MarianMT (for translation)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
