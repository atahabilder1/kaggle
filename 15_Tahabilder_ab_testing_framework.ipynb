{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing Framework - Complete Tutorial\n",
    "\n",
    "**Author:** Anik Tahabilder  \n",
    "**Project:** 15 of 22 - Kaggle ML Portfolio  \n",
    "**Difficulty:** 7/10 | **Learning Value:** 9/10\n",
    "\n",
    "---\n",
    "\n",
    "## What Will You Learn?\n",
    "\n",
    "This tutorial covers **statistical experiment design** for data-driven decisions:\n",
    "\n",
    "| Topic | What You'll Understand |\n",
    "|-------|------------------------|\n",
    "| **Hypothesis Testing** | Null/Alternative hypotheses, p-values |\n",
    "| **Statistical Tests** | t-test, z-test, chi-square, Mann-Whitney |\n",
    "| **Sample Size** | Power analysis, minimum detectable effect |\n",
    "| **Effect Size** | Cohen's d, practical significance |\n",
    "| **Confidence Intervals** | Uncertainty quantification |\n",
    "| **Common Pitfalls** | Multiple testing, peeking, Simpson's paradox |\n",
    "| **Bayesian A/B Testing** | Alternative to frequentist approach |\n",
    "| **Multi-Armed Bandits** | Adaptive experimentation |\n",
    "\n",
    "---\n",
    "\n",
    "## A/B Testing Flow\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                         A/B TESTING FRAMEWORK                            │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│                         ┌─────────────┐                                 │\n",
    "│                         │   USERS     │                                 │\n",
    "│                         │  (Traffic)  │                                 │\n",
    "│                         └──────┬──────┘                                 │\n",
    "│                                │                                        │\n",
    "│                    ┌───────────┴───────────┐                            │\n",
    "│                    │     RANDOM SPLIT      │                            │\n",
    "│                    │       50/50           │                            │\n",
    "│                    └───────────┬───────────┘                            │\n",
    "│                                │                                        │\n",
    "│              ┌─────────────────┴─────────────────┐                      │\n",
    "│              │                                   │                      │\n",
    "│              ▼                                   ▼                      │\n",
    "│     ┌─────────────────┐                ┌─────────────────┐             │\n",
    "│     │   CONTROL (A)   │                │  TREATMENT (B)  │             │\n",
    "│     │                 │                │                 │             │\n",
    "│     │  Current Design │                │   New Design    │             │\n",
    "│     │  (Baseline)     │                │   (Variant)     │             │\n",
    "│     └────────┬────────┘                └────────┬────────┘             │\n",
    "│              │                                   │                      │\n",
    "│              ▼                                   ▼                      │\n",
    "│     ┌─────────────────┐                ┌─────────────────┐             │\n",
    "│     │ Measure Metric  │                │ Measure Metric  │             │\n",
    "│     │ (Conversion,    │                │ (Conversion,    │             │\n",
    "│     │  Revenue, etc.) │                │  Revenue, etc.) │             │\n",
    "│     └────────┬────────┘                └────────┬────────┘             │\n",
    "│              │                                   │                      │\n",
    "│              └───────────────┬───────────────────┘                      │\n",
    "│                              │                                          │\n",
    "│                              ▼                                          │\n",
    "│                    ┌─────────────────────┐                              │\n",
    "│                    │ STATISTICAL TEST    │                              │\n",
    "│                    │                     │                              │\n",
    "│                    │ Is B significantly  │                              │\n",
    "│                    │ better than A?      │                              │\n",
    "│                    └─────────────────────┘                              │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: A/B Testing Fundamentals](#part1)\n",
    "2. [Part 2: Hypothesis Testing](#part2)\n",
    "3. [Part 3: Statistical Tests](#part3)\n",
    "4. [Part 4: Sample Size & Power Analysis](#part4)\n",
    "5. [Part 5: Effect Size & Practical Significance](#part5)\n",
    "6. [Part 6: Common Pitfalls](#part6)\n",
    "7. [Part 7: Bayesian A/B Testing](#part7)\n",
    "8. [Part 8: Multi-Armed Bandits](#part8)\n",
    "9. [Part 9: Complete Framework](#part9)\n",
    "10. [Part 10: Summary](#part10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part1'></a>\n",
    "# Part 1: A/B Testing Fundamentals\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 What is A/B Testing?\n",
    "\n",
    "A/B testing is a **randomized controlled experiment** to compare two versions:\n",
    "\n",
    "| Term | Definition |\n",
    "|------|------------|\n",
    "| **Control (A)** | Current/baseline version |\n",
    "| **Treatment (B)** | New variant being tested |\n",
    "| **Metric** | What we measure (conversion, revenue, CTR) |\n",
    "| **Randomization** | Users randomly assigned to A or B |\n",
    "\n",
    "## 1.2 When to Use A/B Testing\n",
    "\n",
    "| Use Case | Example |\n",
    "|----------|--------|\n",
    "| **Product Changes** | New checkout flow, button color |\n",
    "| **ML Models** | New recommendation algorithm |\n",
    "| **Pricing** | Different price points |\n",
    "| **Marketing** | Email subject lines, ad copy |\n",
    "| **UI/UX** | Layout changes, new features |\n",
    "\n",
    "## 1.3 Key Metrics\n",
    "\n",
    "| Metric Type | Examples | Test Type |\n",
    "|-------------|----------|----------|\n",
    "| **Binary** | Conversion (yes/no), Click (yes/no) | Chi-square, Z-test |\n",
    "| **Continuous** | Revenue, Time on site | t-test |\n",
    "| **Count** | Page views, Purchases | Poisson test |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP AND IMPORTS\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, t, chi2_contingency, mannwhitneyu\n",
    "from scipy.stats import ttest_ind, fisher_exact\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For power analysis\n",
    "from statsmodels.stats.power import TTestIndPower, NormalIndPower\n",
    "from statsmodels.stats.proportion import proportions_ztest, proportion_confint\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"A/B TESTING FRAMEWORK - TUTORIAL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(\"\\nAll libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# LOAD A/B TEST DATA\n# ============================================================\nprint(\"=\"*70)\nprint(\"LOADING A/B TEST DATA\")\nprint(\"=\"*70)\n\n# ============================================================\n# KAGGLE DATASET CONFIGURATION\n# ============================================================\n# Dataset: https://www.kaggle.com/datasets/adarsh0806/ab-testing-practice\n# This is a real A/B testing dataset for practice\n\nimport os\n\nUSE_KAGGLE = os.path.exists('/kaggle/input')\n\ndf = None\n\n# Try loading from Kaggle\nif USE_KAGGLE:\n    # Try different possible paths\n    possible_paths = [\n        '/kaggle/input/ab-testing-practice/ab_testing_data.csv',\n        '/kaggle/input/ab-testing-practice/AB_Testing.csv',\n        '/kaggle/input/ab-testing-practice'\n    ]\n    \n    for path in possible_paths:\n        if os.path.exists(path):\n            if os.path.isdir(path):\n                # List files in directory\n                files = os.listdir(path)\n                csv_files = [f for f in files if f.endswith('.csv')]\n                if csv_files:\n                    df = pd.read_csv(os.path.join(path, csv_files[0]))\n                    print(f\"✓ Loaded from: {os.path.join(path, csv_files[0])}\")\n            else:\n                df = pd.read_csv(path)\n                print(f\"✓ Loaded from: {path}\")\n            break\n    \n    if df is None:\n        print(\"Dataset not found. Add 'ab-testing-practice' dataset in Kaggle.\")\n\n# Try kagglehub if available and df is None\nif df is None:\n    try:\n        import kagglehub\n        from kagglehub import KaggleDatasetAdapter\n        \n        df = kagglehub.load_dataset(\n            KaggleDatasetAdapter.PANDAS,\n            \"adarsh0806/ab-testing-practice\",\n            \"\",\n        )\n        print(\"✓ Loaded via kagglehub\")\n    except Exception as e:\n        print(f\"kagglehub not available: {e}\")\n\n# Fallback: Generate synthetic data\nif df is None:\n    print(\"\\nGenerating synthetic A/B test data...\")\n    print(\"(Add 'ab-testing-practice' dataset in Kaggle for real data)\")\n    \n    def generate_ab_data(n_control=5000, n_treatment=5000, \n                         control_rate=0.10, treatment_rate=0.12,\n                         seed=42):\n        \"\"\"Generate synthetic A/B test data.\"\"\"\n        np.random.seed(seed)\n        \n        control_conversions = np.random.binomial(1, control_rate, n_control)\n        treatment_conversions = np.random.binomial(1, treatment_rate, n_treatment)\n        \n        df = pd.DataFrame({\n            'user_id': range(n_control + n_treatment),\n            'group': ['control'] * n_control + ['treatment'] * n_treatment,\n            'converted': np.concatenate([control_conversions, treatment_conversions])\n        })\n        \n        df['time_on_site'] = np.where(\n            df['group'] == 'control',\n            np.random.exponential(120, len(df)),\n            np.random.exponential(135, len(df))\n        )\n        \n        df['revenue'] = np.where(\n            df['converted'] == 1,\n            np.random.lognormal(3.5, 0.8, len(df)),\n            0\n        )\n        \n        return df\n    \n    df = generate_ab_data()\n\n# ============================================================\n# STANDARDIZE COLUMN NAMES\n# ============================================================\nprint(f\"\\nOriginal columns: {list(df.columns)}\")\n\n# Common column name mappings\ncolumn_mappings = {\n    'variant': 'group',\n    'Variant': 'group',\n    'test_group': 'group',\n    'experiment_group': 'group',\n    'ab_group': 'group',\n    'conversion': 'converted',\n    'Conversion': 'converted',\n    'convert': 'converted',\n    'purchased': 'converted',\n    'clicked': 'converted',\n    'user': 'user_id',\n    'User': 'user_id',\n    'userid': 'user_id',\n    'id': 'user_id'\n}\n\n# Rename columns if needed\nfor old_name, new_name in column_mappings.items():\n    if old_name in df.columns and new_name not in df.columns:\n        df = df.rename(columns={old_name: new_name})\n\n# Standardize group values\nif 'group' in df.columns:\n    # Convert to lowercase\n    df['group'] = df['group'].astype(str).str.lower().str.strip()\n    \n    # Map common variations\n    group_mappings = {\n        'a': 'control',\n        'b': 'treatment',\n        'control': 'control',\n        'treatment': 'treatment',\n        'test': 'treatment',\n        'variant': 'treatment',\n        'experiment': 'treatment',\n        '0': 'control',\n        '1': 'treatment'\n    }\n    df['group'] = df['group'].map(lambda x: group_mappings.get(x, x))\n\n# Add time_on_site if not present\nif 'time_on_site' not in df.columns:\n    df['time_on_site'] = np.where(\n        df['group'] == 'control',\n        np.random.exponential(120, len(df)),\n        np.random.exponential(135, len(df))\n    )\n\n# Add revenue if not present\nif 'revenue' not in df.columns:\n    if 'converted' in df.columns:\n        df['revenue'] = np.where(\n            df['converted'] == 1,\n            np.random.lognormal(3.5, 0.8, len(df)),\n            0\n        )\n    else:\n        df['revenue'] = 0\n\nprint(f\"Standardized columns: {list(df.columns)}\")\n\n# ============================================================\n# DATA SUMMARY\n# ============================================================\nprint(f\"\\n\" + \"=\"*50)\nprint(\"DATASET SUMMARY\")\nprint(\"=\"*50)\nprint(f\"Shape: {df.shape}\")\nprint(f\"\\nGroup distribution:\")\nprint(df['group'].value_counts())\n\nprint(f\"\\nSample data:\")\nprint(df.head(10))\n\nif 'converted' in df.columns:\n    print(f\"\\nConversion rates by group:\")\n    print(df.groupby('group')['converted'].agg(['sum', 'count', 'mean']))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "print(\"=\"*70)\n",
    "print(\"DATA VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Conversion rates by group\n",
    "ax1 = axes[0]\n",
    "conv_rates = df.groupby('group')['converted'].mean()\n",
    "colors = ['steelblue', 'coral']\n",
    "bars = ax1.bar(conv_rates.index, conv_rates.values, color=colors, edgecolor='black')\n",
    "ax1.set_ylabel('Conversion Rate')\n",
    "ax1.set_title('Conversion Rate by Group', fontweight='bold')\n",
    "for bar, rate in zip(bars, conv_rates.values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "             f'{rate:.2%}', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Revenue distribution\n",
    "ax2 = axes[1]\n",
    "for group, color in zip(['control', 'treatment'], colors):\n",
    "    data = df[(df['group'] == group) & (df['revenue'] > 0)]['revenue']\n",
    "    ax2.hist(data, bins=30, alpha=0.6, label=group.capitalize(), color=color, edgecolor='black')\n",
    "ax2.set_xlabel('Revenue ($)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Revenue Distribution (Converters)', fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Time on site\n",
    "ax3 = axes[2]\n",
    "df.boxplot(column='time_on_site', by='group', ax=ax3)\n",
    "ax3.set_xlabel('Group')\n",
    "ax3.set_ylabel('Time on Site (seconds)')\n",
    "ax3.set_title('Time on Site by Group', fontweight='bold')\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part2'></a>\n",
    "# Part 2: Hypothesis Testing\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 The Hypothesis Testing Framework\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                    HYPOTHESIS TESTING                                │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│  Step 1: Define Hypotheses                                         │\n",
    "│  ┌───────────────────────────────────────────────────────────────┐ │\n",
    "│  │ H₀ (Null):        No difference between A and B               │ │\n",
    "│  │ H₁ (Alternative): There IS a difference (B ≠ A, B > A, B < A) │ │\n",
    "│  └───────────────────────────────────────────────────────────────┘ │\n",
    "│                                                                     │\n",
    "│  Step 2: Choose Significance Level (α)                             │\n",
    "│  ┌───────────────────────────────────────────────────────────────┐ │\n",
    "│  │ α = 0.05 (5%) is standard                                     │ │\n",
    "│  │ This is the probability of false positive (Type I error)     │ │\n",
    "│  └───────────────────────────────────────────────────────────────┘ │\n",
    "│                                                                     │\n",
    "│  Step 3: Collect Data & Calculate Test Statistic                   │\n",
    "│                                                                     │\n",
    "│  Step 4: Calculate p-value                                         │\n",
    "│  ┌───────────────────────────────────────────────────────────────┐ │\n",
    "│  │ p-value = P(observing data | H₀ is true)                      │ │\n",
    "│  │ Small p-value → Evidence against H₀                           │ │\n",
    "│  └───────────────────────────────────────────────────────────────┘ │\n",
    "│                                                                     │\n",
    "│  Step 5: Make Decision                                             │\n",
    "│  ┌───────────────────────────────────────────────────────────────┐ │\n",
    "│  │ if p-value < α: Reject H₀ (statistically significant)        │ │\n",
    "│  │ if p-value ≥ α: Fail to reject H₀ (not significant)          │ │\n",
    "│  └───────────────────────────────────────────────────────────────┘ │\n",
    "│                                                                     │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## 2.2 Types of Errors\n",
    "\n",
    "| | H₀ True (No Effect) | H₀ False (Real Effect) |\n",
    "|---|---|---|\n",
    "| **Reject H₀** | Type I Error (α) - False Positive | Correct! (Power = 1-β) |\n",
    "| **Fail to Reject H₀** | Correct! | Type II Error (β) - False Negative |\n",
    "\n",
    "## 2.3 One-Tailed vs Two-Tailed Tests\n",
    "\n",
    "| Test Type | Hypothesis | Use When |\n",
    "|-----------|------------|----------|\n",
    "| **Two-tailed** | H₁: μ_B ≠ μ_A | Don't know if B is better or worse |\n",
    "| **One-tailed (right)** | H₁: μ_B > μ_A | Only care if B is better |\n",
    "| **One-tailed (left)** | H₁: μ_B < μ_A | Only care if B is worse |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HYPOTHESIS TESTING EXAMPLE\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS TESTING FOR A/B TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract data\n",
    "control = df[df['group'] == 'control']\n",
    "treatment = df[df['group'] == 'treatment']\n",
    "\n",
    "# Calculate statistics\n",
    "n_control = len(control)\n",
    "n_treatment = len(treatment)\n",
    "conversions_control = control['converted'].sum()\n",
    "conversions_treatment = treatment['converted'].sum()\n",
    "rate_control = conversions_control / n_control\n",
    "rate_treatment = conversions_treatment / n_treatment\n",
    "\n",
    "print(f\"\"\"\n",
    "STEP 1: Define Hypotheses\n",
    "{'='*50}\n",
    "H₀ (Null):        p_treatment = p_control\n",
    "                  (No difference in conversion rates)\n",
    "\n",
    "H₁ (Alternative): p_treatment ≠ p_control\n",
    "                  (There is a difference)\n",
    "\n",
    "STEP 2: Choose Significance Level\n",
    "{'='*50}\n",
    "α = 0.05 (5%)\n",
    "\n",
    "STEP 3: Collect Data\n",
    "{'='*50}\n",
    "Control (A):   {conversions_control:,} / {n_control:,} = {rate_control:.4f} ({rate_control:.2%})\n",
    "Treatment (B): {conversions_treatment:,} / {n_treatment:,} = {rate_treatment:.4f} ({rate_treatment:.2%})\n",
    "\n",
    "Observed Difference: {rate_treatment - rate_control:.4f} ({(rate_treatment - rate_control):.2%})\n",
    "Relative Lift: {(rate_treatment - rate_control) / rate_control * 100:.1f}%\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Z-test for proportions\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 4 & 5: STATISTICAL TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Z-test for two proportions\n",
    "count = np.array([conversions_treatment, conversions_control])\n",
    "nobs = np.array([n_treatment, n_control])\n",
    "\n",
    "z_stat, p_value = proportions_ztest(count, nobs, alternative='two-sided')\n",
    "\n",
    "# Calculate confidence interval for the difference\n",
    "pooled_rate = (conversions_control + conversions_treatment) / (n_control + n_treatment)\n",
    "se = np.sqrt(pooled_rate * (1 - pooled_rate) * (1/n_control + 1/n_treatment))\n",
    "diff = rate_treatment - rate_control\n",
    "ci_low = diff - 1.96 * se\n",
    "ci_high = diff + 1.96 * se\n",
    "\n",
    "print(f\"\"\"\n",
    "Z-TEST FOR PROPORTIONS\n",
    "{'='*50}\n",
    "Test Statistic (Z): {z_stat:.4f}\n",
    "P-value:            {p_value:.6f}\n",
    "\n",
    "95% Confidence Interval for Difference:\n",
    "  [{ci_low:.4f}, {ci_high:.4f}]\n",
    "  [{ci_low:.2%}, {ci_high:.2%}]\n",
    "\n",
    "DECISION (α = 0.05)\n",
    "{'='*50}\n",
    "\"\"\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"p-value ({p_value:.6f}) < 0.05\")\n",
    "    print(\"→ REJECT H₀\")\n",
    "    print(\"→ The difference IS statistically significant!\")\n",
    "    print(f\"→ Treatment conversion rate is significantly {'higher' if diff > 0 else 'lower'}\")\n",
    "else:\n",
    "    print(f\"p-value ({p_value:.6f}) ≥ 0.05\")\n",
    "    print(\"→ FAIL TO REJECT H₀\")\n",
    "    print(\"→ The difference is NOT statistically significant\")\n",
    "    print(\"→ Cannot conclude treatment is different from control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the hypothesis test\n",
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS TEST VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Normal distribution with test statistic\n",
    "ax1 = axes[0]\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "y = norm.pdf(x)\n",
    "ax1.plot(x, y, 'b-', linewidth=2, label='Null Distribution')\n",
    "ax1.fill_between(x, y, where=(x < -1.96) | (x > 1.96), alpha=0.3, color='red', label='Rejection Region (α=0.05)')\n",
    "ax1.axvline(x=z_stat, color='green', linestyle='--', linewidth=2, label=f'Observed Z = {z_stat:.2f}')\n",
    "ax1.axvline(x=-1.96, color='red', linestyle=':', alpha=0.7)\n",
    "ax1.axvline(x=1.96, color='red', linestyle=':', alpha=0.7)\n",
    "ax1.set_xlabel('Z-score')\n",
    "ax1.set_ylabel('Probability Density')\n",
    "ax1.set_title('Hypothesis Test Visualization', fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Confidence interval\n",
    "ax2 = axes[1]\n",
    "ax2.errorbar(1, diff, yerr=[[diff - ci_low], [ci_high - diff]], \n",
    "             fmt='o', markersize=10, capsize=10, capthick=2, color='steelblue')\n",
    "ax2.axhline(y=0, color='red', linestyle='--', label='No Effect')\n",
    "ax2.set_xlim(0.5, 1.5)\n",
    "ax2.set_xticks([1])\n",
    "ax2.set_xticklabels(['Treatment - Control'])\n",
    "ax2.set_ylabel('Difference in Conversion Rate')\n",
    "ax2.set_title('95% Confidence Interval', fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "# Add text annotation\n",
    "ax2.text(1.1, diff, f'{diff:.4f}\\n({diff:.2%})', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - If CI doesn't include 0: Statistically significant\")\n",
    "print(\"  - If CI includes 0: Not statistically significant\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# P-VALUE EXPLAINED VISUALLY\n# ============================================================\nprint(\"=\"*70)\nprint(\"UNDERSTANDING P-VALUE\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nWHAT IS A P-VALUE?\n==================\nThe p-value is the probability of observing data as extreme (or more extreme)\nthan what we actually observed, ASSUMING the null hypothesis is true.\n\nSmall p-value → Our observed data is very unlikely under H₀\n             → Evidence AGAINST the null hypothesis\n             → The difference is probably REAL\n\nExample interpretation:\n- p-value = 0.03 means: \"If there were truly NO difference between A and B,\n  we'd see results this extreme only 3% of the time by random chance.\"\n\"\"\")\n\n# Create detailed p-value visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 1. P-value explanation with bell curve\nax1 = axes[0]\nx = np.linspace(-4, 4, 1000)\ny = norm.pdf(x)\n\n# Plot the distribution\nax1.plot(x, y, 'b-', linewidth=2.5, label='Null Distribution\\n(If H₀ were true)')\nax1.fill_between(x, y, alpha=0.1, color='blue')\n\n# Mark the center as \"More likely observations\"\nax1.annotate('More likely\\nobservations', xy=(0, 0.4), fontsize=10, ha='center',\n            fontweight='bold')\n\n# Mark the tails as \"Very unlikely observations\"\nax1.annotate('Very unlikely\\nobservations', xy=(-2.8, 0.05), fontsize=9, ha='center',\n            color='red')\nax1.annotate('Very unlikely\\nobservations', xy=(2.8, 0.05), fontsize=9, ha='center',\n            color='red')\n\n# Show our observed data point (z_stat from earlier)\nobserved_z = abs(z_stat)\nax1.axvline(x=observed_z, color='green', linestyle='-', linewidth=2.5)\nax1.axvline(x=-observed_z, color='green', linestyle='-', linewidth=2.5)\n\n# Shade the p-value region (both tails for two-tailed test)\nx_right = x[x >= observed_z]\nx_left = x[x <= -observed_z]\nax1.fill_between(x_right, norm.pdf(x_right), alpha=0.5, color='green', label='P-value\\n(shaded area)')\nax1.fill_between(x_left, norm.pdf(x_left), alpha=0.5, color='green')\n\n# Add annotation for observed data point\nax1.annotate(f'Observed\\ndata point\\n(Z={observed_z:.2f})', \n            xy=(observed_z, norm.pdf(observed_z)), \n            xytext=(observed_z + 0.8, 0.2),\n            fontsize=10, fontweight='bold', color='green',\n            arrowprops=dict(arrowstyle='->', color='green', lw=2))\n\nax1.set_xlabel('Possible Results (Z-score)', fontsize=11)\nax1.set_ylabel('Probability Density', fontsize=11)\nax1.set_title('P-Value: Probability of Extreme Results Under H₀', fontweight='bold', fontsize=12)\nax1.legend(loc='upper right')\nax1.set_xlim(-4, 4)\n\n# Add text box with p-value definition\ntextstr = f'P-value = {p_value:.4f}\\n\\nIf H₀ were true, we\\'d see\\nresults this extreme only\\n{p_value:.1%} of the time.'\nprops = dict(boxstyle='round', facecolor='lightyellow', alpha=0.8)\nax1.text(0.02, 0.98, textstr, transform=ax1.transAxes, fontsize=10,\n        verticalalignment='top', bbox=props)\n\n# 2. Business impact visualization (like the 228% chart)\nax2 = axes[1]\n\n# Simulate cumulative performance over time\nmonths = ['Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nnp.random.seed(42)\n\n# Without A/B testing (random decisions)\nbaseline_growth = [100]\nfor i in range(6):\n    # Random growth with some noise, sometimes negative\n    growth = baseline_growth[-1] * (1 + np.random.uniform(-0.02, 0.05))\n    baseline_growth.append(growth)\n\n# With A/B testing (data-driven decisions)  \nab_growth = [100]\nfor i in range(6):\n    # Consistent positive growth from making good decisions\n    growth = ab_growth[-1] * (1 + np.random.uniform(0.08, 0.15))\n    ab_growth.append(growth)\n\nax2.plot(months, baseline_growth, 'r-', linewidth=2.5, marker='o', markersize=8, \n         label='Without A/B Testing')\nax2.plot(months, ab_growth, 'b-', linewidth=2.5, marker='o', markersize=8,\n         label='With A/B Testing')\n\n# Fill between to show the gap\nax2.fill_between(months, baseline_growth, ab_growth, alpha=0.2, color='green')\n\n# Annotate the improvement\nimprovement = (ab_growth[-1] - baseline_growth[-1]) / baseline_growth[-1] * 100\nax2.annotate(f'+{improvement:.0f}%', xy=(6, (ab_growth[-1] + baseline_growth[-1])/2),\n            fontsize=16, fontweight='bold', color='green',\n            ha='center')\n\nax2.set_xlabel('Month', fontsize=11)\nax2.set_ylabel('Performance Index', fontsize=11)\nax2.set_title('Business Impact of A/B Testing', fontweight='bold', fontsize=12)\nax2.legend(loc='upper left')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nKEY TAKEAWAY:\")\nprint(f\"  Our observed Z-score of {z_stat:.2f} falls in the tail of the distribution.\")\nprint(f\"  The p-value ({p_value:.4f}) tells us this would happen only {p_value:.1%} of the time\")\nprint(f\"  if there were truly no difference between Control and Treatment.\")\nprint(f\"  Since {p_value:.4f} < 0.05, we reject H₀ - the difference is REAL!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part3'></a>\n",
    "# Part 3: Statistical Tests\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Choosing the Right Test\n",
    "\n",
    "| Data Type | Test | When to Use |\n",
    "|-----------|------|-------------|\n",
    "| **Binary (proportions)** | Z-test, Chi-square | Conversion rate, CTR |\n",
    "| **Continuous (normal)** | t-test | Revenue, time on site |\n",
    "| **Continuous (non-normal)** | Mann-Whitney U | Skewed distributions |\n",
    "| **Count data** | Poisson test | Page views, events |\n",
    "| **Small samples** | Fisher's exact | n < 30 per group |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STATISTICAL TESTS COMPARISON\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"STATISTICAL TESTS FOR A/B TESTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class ABTestSuite:\n",
    "    \"\"\"\n",
    "    Comprehensive A/B testing suite with multiple statistical tests.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def z_test_proportions(conversions_a, n_a, conversions_b, n_b, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Z-test for comparing two proportions.\n",
    "        \n",
    "        Best for: Binary outcomes (conversion, click, etc.)\n",
    "        Assumption: Large sample sizes (n > 30)\n",
    "        \"\"\"\n",
    "        p_a = conversions_a / n_a\n",
    "        p_b = conversions_b / n_b\n",
    "        \n",
    "        # Pooled proportion\n",
    "        p_pool = (conversions_a + conversions_b) / (n_a + n_b)\n",
    "        \n",
    "        # Standard error\n",
    "        se = np.sqrt(p_pool * (1 - p_pool) * (1/n_a + 1/n_b))\n",
    "        \n",
    "        # Z statistic\n",
    "        z = (p_b - p_a) / se\n",
    "        \n",
    "        # P-value (two-tailed)\n",
    "        p_value = 2 * (1 - norm.cdf(abs(z)))\n",
    "        \n",
    "        # Confidence interval\n",
    "        se_diff = np.sqrt(p_a*(1-p_a)/n_a + p_b*(1-p_b)/n_b)\n",
    "        ci = (p_b - p_a - 1.96*se_diff, p_b - p_a + 1.96*se_diff)\n",
    "        \n",
    "        return {\n",
    "            'test': 'Z-test for Proportions',\n",
    "            'statistic': z,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < alpha,\n",
    "            'ci_95': ci,\n",
    "            'effect': p_b - p_a,\n",
    "            'relative_effect': (p_b - p_a) / p_a * 100\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def chi_square_test(conversions_a, n_a, conversions_b, n_b, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Chi-square test for independence.\n",
    "        \n",
    "        Best for: Binary outcomes\n",
    "        Alternative to Z-test, gives same result for 2x2 tables\n",
    "        \"\"\"\n",
    "        # Create contingency table\n",
    "        table = np.array([\n",
    "            [conversions_a, n_a - conversions_a],\n",
    "            [conversions_b, n_b - conversions_b]\n",
    "        ])\n",
    "        \n",
    "        chi2, p_value, dof, expected = chi2_contingency(table)\n",
    "        \n",
    "        return {\n",
    "            'test': 'Chi-square Test',\n",
    "            'statistic': chi2,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < alpha,\n",
    "            'dof': dof\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def t_test(data_a, data_b, alpha=0.05, equal_var=False):\n",
    "        \"\"\"\n",
    "        Independent samples t-test.\n",
    "        \n",
    "        Best for: Continuous outcomes (revenue, time)\n",
    "        Assumption: Approximately normal distribution\n",
    "        Use equal_var=False for Welch's t-test (recommended)\n",
    "        \"\"\"\n",
    "        t_stat, p_value = ttest_ind(data_a, data_b, equal_var=equal_var)\n",
    "        \n",
    "        mean_a = np.mean(data_a)\n",
    "        mean_b = np.mean(data_b)\n",
    "        \n",
    "        # Cohen's d effect size\n",
    "        pooled_std = np.sqrt((np.var(data_a) + np.var(data_b)) / 2)\n",
    "        cohens_d = (mean_b - mean_a) / pooled_std\n",
    "        \n",
    "        return {\n",
    "            'test': \"Welch's t-test\" if not equal_var else \"Student's t-test\",\n",
    "            'statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < alpha,\n",
    "            'mean_a': mean_a,\n",
    "            'mean_b': mean_b,\n",
    "            'effect': mean_b - mean_a,\n",
    "            'cohens_d': cohens_d\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def mann_whitney_test(data_a, data_b, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Mann-Whitney U test (non-parametric).\n",
    "        \n",
    "        Best for: Non-normal distributions, ordinal data\n",
    "        More robust than t-test for skewed data\n",
    "        \"\"\"\n",
    "        u_stat, p_value = mannwhitneyu(data_a, data_b, alternative='two-sided')\n",
    "        \n",
    "        return {\n",
    "            'test': 'Mann-Whitney U Test',\n",
    "            'statistic': u_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < alpha,\n",
    "            'median_a': np.median(data_a),\n",
    "            'median_b': np.median(data_b)\n",
    "        }\n",
    "\n",
    "print(\"ABTestSuite class created with methods:\")\n",
    "print(\"  - z_test_proportions(): For binary outcomes\")\n",
    "print(\"  - chi_square_test(): Alternative for binary outcomes\")\n",
    "print(\"  - t_test(): For continuous outcomes\")\n",
    "print(\"  - mann_whitney_test(): For non-normal distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all tests on our data\n",
    "print(\"=\"*70)\n",
    "print(\"RUNNING MULTIPLE STATISTICAL TESTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Z-test for conversion rate\n",
    "print(\"\\n1. Z-TEST FOR CONVERSION RATE\")\n",
    "print(\"-\" * 50)\n",
    "z_result = ABTestSuite.z_test_proportions(\n",
    "    conversions_control, n_control,\n",
    "    conversions_treatment, n_treatment\n",
    ")\n",
    "for k, v in z_result.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# 2. Chi-square test\n",
    "print(\"\\n2. CHI-SQUARE TEST\")\n",
    "print(\"-\" * 50)\n",
    "chi_result = ABTestSuite.chi_square_test(\n",
    "    conversions_control, n_control,\n",
    "    conversions_treatment, n_treatment\n",
    ")\n",
    "for k, v in chi_result.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# 3. T-test for time on site\n",
    "print(\"\\n3. T-TEST FOR TIME ON SITE\")\n",
    "print(\"-\" * 50)\n",
    "t_result = ABTestSuite.t_test(\n",
    "    control['time_on_site'].values,\n",
    "    treatment['time_on_site'].values\n",
    ")\n",
    "for k, v in t_result.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# 4. Mann-Whitney for revenue (skewed distribution)\n",
    "print(\"\\n4. MANN-WHITNEY TEST FOR REVENUE\")\n",
    "print(\"-\" * 50)\n",
    "mw_result = ABTestSuite.mann_whitney_test(\n",
    "    control[control['revenue'] > 0]['revenue'].values,\n",
    "    treatment[treatment['revenue'] > 0]['revenue'].values\n",
    ")\n",
    "for k, v in mw_result.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part4'></a>\n",
    "# Part 4: Sample Size & Power Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 Why Sample Size Matters\n",
    "\n",
    "| Sample Size | Issue |\n",
    "|-------------|-------|\n",
    "| **Too Small** | High chance of missing real effects (low power) |\n",
    "| **Too Large** | Wasted resources, may detect trivial effects |\n",
    "| **Just Right** | Detect meaningful effects with high confidence |\n",
    "\n",
    "## 4.2 Power Analysis Components\n",
    "\n",
    "| Parameter | Symbol | Description | Typical Value |\n",
    "|-----------|--------|-------------|---------------|\n",
    "| **Significance Level** | α | P(Type I error) | 0.05 |\n",
    "| **Power** | 1-β | P(detecting real effect) | 0.80 |\n",
    "| **Effect Size** | δ | Minimum detectable effect | Depends on business |\n",
    "| **Sample Size** | n | Number per group | Calculated |\n",
    "\n",
    "## 4.3 The Power Formula\n",
    "\n",
    "For proportions:\n",
    "```\n",
    "n = 2 × (Z_{α/2} + Z_β)² × p̄(1-p̄) / (p₁ - p₂)²\n",
    "\n",
    "Where:\n",
    "- p̄ = (p₁ + p₂) / 2\n",
    "- Z_{α/2} = 1.96 for α=0.05\n",
    "- Z_β = 0.84 for power=0.80\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAMPLE SIZE CALCULATOR\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"SAMPLE SIZE CALCULATOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def calculate_sample_size_proportions(p1, p2, alpha=0.05, power=0.80):\n",
    "    \"\"\"\n",
    "    Calculate required sample size for comparing two proportions.\n",
    "    \n",
    "    Parameters:\n",
    "    - p1: Baseline conversion rate (control)\n",
    "    - p2: Expected conversion rate (treatment)\n",
    "    - alpha: Significance level (default 0.05)\n",
    "    - power: Statistical power (default 0.80)\n",
    "    \n",
    "    Returns:\n",
    "    - n: Sample size per group\n",
    "    \"\"\"\n",
    "    # Z-scores\n",
    "    z_alpha = norm.ppf(1 - alpha/2)  # Two-tailed\n",
    "    z_beta = norm.ppf(power)\n",
    "    \n",
    "    # Pooled proportion\n",
    "    p_bar = (p1 + p2) / 2\n",
    "    \n",
    "    # Sample size formula\n",
    "    n = 2 * ((z_alpha + z_beta)**2) * p_bar * (1 - p_bar) / ((p2 - p1)**2)\n",
    "    \n",
    "    return int(np.ceil(n))\n",
    "\n",
    "def calculate_sample_size_continuous(effect_size, alpha=0.05, power=0.80):\n",
    "    \"\"\"\n",
    "    Calculate required sample size for continuous outcomes.\n",
    "    \n",
    "    Parameters:\n",
    "    - effect_size: Cohen's d (standardized effect size)\n",
    "    - alpha: Significance level\n",
    "    - power: Statistical power\n",
    "    \"\"\"\n",
    "    analysis = TTestIndPower()\n",
    "    n = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power)\n",
    "    return int(np.ceil(n))\n",
    "\n",
    "# Example calculations\n",
    "print(\"\\nSCENARIO 1: Conversion Rate\")\n",
    "print(\"-\" * 50)\n",
    "baseline = 0.10  # 10% current conversion\n",
    "expected = 0.12  # 12% expected with new design (20% lift)\n",
    "\n",
    "n_required = calculate_sample_size_proportions(baseline, expected)\n",
    "print(f\"Baseline rate: {baseline:.1%}\")\n",
    "print(f\"Expected rate: {expected:.1%}\")\n",
    "print(f\"Minimum Detectable Effect: {expected - baseline:.1%} ({(expected-baseline)/baseline*100:.0f}% relative)\")\n",
    "print(f\"\\nRequired sample size per group: {n_required:,}\")\n",
    "print(f\"Total users needed: {n_required * 2:,}\")\n",
    "\n",
    "# Vary the minimum detectable effect\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE SIZE vs MINIMUM DETECTABLE EFFECT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n{'MDE':>10} {'Relative Lift':>15} {'Sample Size (per group)':>25}\")\n",
    "print(\"-\" * 55)\n",
    "for lift in [0.05, 0.10, 0.15, 0.20, 0.25, 0.30]:\n",
    "    expected = baseline * (1 + lift)\n",
    "    n = calculate_sample_size_proportions(baseline, expected)\n",
    "    print(f\"{expected - baseline:>10.2%} {lift:>14.0%} {n:>20,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample size requirements\n",
    "print(\"=\"*70)\n",
    "print(\"SAMPLE SIZE VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Sample size vs MDE\n",
    "ax1 = axes[0]\n",
    "lifts = np.linspace(0.05, 0.50, 50)\n",
    "sample_sizes = [calculate_sample_size_proportions(0.10, 0.10*(1+l)) for l in lifts]\n",
    "ax1.plot(lifts * 100, sample_sizes, 'b-', linewidth=2)\n",
    "ax1.fill_between(lifts * 100, sample_sizes, alpha=0.3)\n",
    "ax1.set_xlabel('Relative Lift (%)')\n",
    "ax1.set_ylabel('Sample Size per Group')\n",
    "ax1.set_title('Sample Size vs Minimum Detectable Effect', fontweight='bold')\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "for lift_pct, label in [(10, '10% lift'), (20, '20% lift'), (30, '30% lift')]:\n",
    "    n = calculate_sample_size_proportions(0.10, 0.10*(1+lift_pct/100))\n",
    "    ax1.annotate(f'{n:,}', xy=(lift_pct, n), xytext=(lift_pct+5, n*1.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='red'),\n",
    "                fontsize=9, color='red')\n",
    "\n",
    "# 2. Sample size vs Power\n",
    "ax2 = axes[1]\n",
    "powers = np.linspace(0.5, 0.99, 50)\n",
    "sample_sizes_power = [calculate_sample_size_proportions(0.10, 0.12, power=p) for p in powers]\n",
    "ax2.plot(powers * 100, sample_sizes_power, 'g-', linewidth=2)\n",
    "ax2.fill_between(powers * 100, sample_sizes_power, alpha=0.3, color='green')\n",
    "ax2.axvline(x=80, color='red', linestyle='--', label='80% power (standard)')\n",
    "ax2.set_xlabel('Statistical Power (%)')\n",
    "ax2.set_ylabel('Sample Size per Group')\n",
    "ax2.set_title('Sample Size vs Power (20% relative lift)', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"  - Smaller effects require exponentially more samples\")\n",
    "print(\"  - Higher power requires more samples (but 80% is standard)\")\n",
    "print(\"  - Always calculate sample size BEFORE running the test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part5'></a>\n",
    "# Part 5: Effect Size & Practical Significance\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 Statistical vs Practical Significance\n",
    "\n",
    "| Type | Question | Example |\n",
    "|------|----------|---------|\n",
    "| **Statistical Significance** | Is the effect real (not due to chance)? | p < 0.05 |\n",
    "| **Practical Significance** | Is the effect large enough to matter? | +$100K revenue |\n",
    "\n",
    "**Critical Insight:** A result can be statistically significant but practically meaningless!\n",
    "\n",
    "## 5.2 Cohen's d (Effect Size for Continuous)\n",
    "\n",
    "| Cohen's d | Interpretation |\n",
    "|-----------|----------------|\n",
    "| 0.2 | Small effect |\n",
    "| 0.5 | Medium effect |\n",
    "| 0.8 | Large effect |\n",
    "\n",
    "## 5.3 Effect Size for Proportions\n",
    "\n",
    "| Metric | Formula |\n",
    "|--------|--------|\n",
    "| **Absolute Difference** | p_B - p_A |\n",
    "| **Relative Lift** | (p_B - p_A) / p_A × 100% |\n",
    "| **Odds Ratio** | (p_B/(1-p_B)) / (p_A/(1-p_A)) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EFFECT SIZE AND PRACTICAL SIGNIFICANCE\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"EFFECT SIZE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def calculate_effect_sizes(p_a, p_b, n_a, n_b):\n",
    "    \"\"\"\n",
    "    Calculate various effect size metrics.\n",
    "    \"\"\"\n",
    "    # Absolute difference\n",
    "    abs_diff = p_b - p_a\n",
    "    \n",
    "    # Relative lift\n",
    "    rel_lift = (p_b - p_a) / p_a * 100\n",
    "    \n",
    "    # Odds ratio\n",
    "    odds_a = p_a / (1 - p_a)\n",
    "    odds_b = p_b / (1 - p_b)\n",
    "    odds_ratio = odds_b / odds_a\n",
    "    \n",
    "    # Risk ratio (relative risk)\n",
    "    risk_ratio = p_b / p_a\n",
    "    \n",
    "    # Number Needed to Treat (NNT)\n",
    "    nnt = 1 / abs(abs_diff) if abs_diff != 0 else float('inf')\n",
    "    \n",
    "    # Cohen's h (effect size for proportions)\n",
    "    h = 2 * (np.arcsin(np.sqrt(p_b)) - np.arcsin(np.sqrt(p_a)))\n",
    "    \n",
    "    return {\n",
    "        'absolute_difference': abs_diff,\n",
    "        'relative_lift': rel_lift,\n",
    "        'odds_ratio': odds_ratio,\n",
    "        'risk_ratio': risk_ratio,\n",
    "        'nnt': nnt,\n",
    "        'cohens_h': h\n",
    "    }\n",
    "\n",
    "# Calculate for our test\n",
    "effects = calculate_effect_sizes(rate_control, rate_treatment, n_control, n_treatment)\n",
    "\n",
    "print(f\"\"\"\n",
    "EFFECT SIZE METRICS\n",
    "{'='*50}\n",
    "Control Rate:      {rate_control:.4f} ({rate_control:.2%})\n",
    "Treatment Rate:    {rate_treatment:.4f} ({rate_treatment:.2%})\n",
    "\n",
    "Absolute Difference: {effects['absolute_difference']:.4f} ({effects['absolute_difference']:.2%})\n",
    "Relative Lift:       {effects['relative_lift']:.1f}%\n",
    "Odds Ratio:          {effects['odds_ratio']:.3f}\n",
    "Risk Ratio:          {effects['risk_ratio']:.3f}\n",
    "Number Needed to Treat: {effects['nnt']:.0f}\n",
    "Cohen's h:           {effects['cohens_h']:.3f}\n",
    "\n",
    "INTERPRETATION\n",
    "{'='*50}\n",
    "\"\"\")\n",
    "\n",
    "# Interpret Cohen's h\n",
    "h = abs(effects['cohens_h'])\n",
    "if h < 0.2:\n",
    "    interpretation = \"Small effect (h < 0.2)\"\n",
    "elif h < 0.5:\n",
    "    interpretation = \"Small to Medium effect (0.2 ≤ h < 0.5)\"\n",
    "elif h < 0.8:\n",
    "    interpretation = \"Medium to Large effect (0.5 ≤ h < 0.8)\"\n",
    "else:\n",
    "    interpretation = \"Large effect (h ≥ 0.8)\"\n",
    "\n",
    "print(f\"Cohen's h interpretation: {interpretation}\")\n",
    "print(f\"\\nPractical interpretation:\")\n",
    "print(f\"  - For every {effects['nnt']:.0f} users exposed to treatment,\")\n",
    "print(f\"    we get 1 additional conversion compared to control.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business impact calculation\n",
    "print(\"=\"*70)\n",
    "print(\"BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Hypothetical business metrics\n",
    "monthly_visitors = 1_000_000\n",
    "avg_order_value = 50  # dollars\n",
    "\n",
    "# Current (control) metrics\n",
    "current_conversions = monthly_visitors * rate_control\n",
    "current_revenue = current_conversions * avg_order_value\n",
    "\n",
    "# Expected (treatment) metrics\n",
    "expected_conversions = monthly_visitors * rate_treatment\n",
    "expected_revenue = expected_conversions * avg_order_value\n",
    "\n",
    "# Impact\n",
    "additional_conversions = expected_conversions - current_conversions\n",
    "additional_revenue = expected_revenue - current_revenue\n",
    "\n",
    "print(f\"\"\"\n",
    "PROJECTED MONTHLY IMPACT\n",
    "{'='*50}\n",
    "Monthly Visitors: {monthly_visitors:,}\n",
    "Average Order Value: ${avg_order_value}\n",
    "\n",
    "                    Control        Treatment       Difference\n",
    "Conversion Rate     {rate_control:.2%}          {rate_treatment:.2%}           +{rate_treatment-rate_control:.2%}\n",
    "Conversions         {current_conversions:,.0f}        {expected_conversions:,.0f}         +{additional_conversions:,.0f}\n",
    "Revenue             ${current_revenue:,.0f}      ${expected_revenue:,.0f}       +${additional_revenue:,.0f}\n",
    "\n",
    "ANNUAL PROJECTED IMPACT: +${additional_revenue * 12:,.0f}\n",
    "\"\"\")\n",
    "\n",
    "# Visualize business impact\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Monthly revenue comparison\n",
    "ax1 = axes[0]\n",
    "categories = ['Control', 'Treatment']\n",
    "revenues = [current_revenue / 1e6, expected_revenue / 1e6]\n",
    "colors = ['steelblue', 'coral']\n",
    "bars = ax1.bar(categories, revenues, color=colors, edgecolor='black')\n",
    "ax1.set_ylabel('Monthly Revenue (Millions $)')\n",
    "ax1.set_title('Revenue Comparison', fontweight='bold')\n",
    "for bar, rev in zip(bars, revenues):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "             f'${rev:.2f}M', ha='center', fontweight='bold')\n",
    "\n",
    "# Additional revenue highlight\n",
    "ax2 = axes[1]\n",
    "months = list(range(1, 13))\n",
    "cumulative_impact = [additional_revenue * m / 1e6 for m in months]\n",
    "ax2.bar(months, cumulative_impact, color='green', edgecolor='black', alpha=0.7)\n",
    "ax2.plot(months, cumulative_impact, 'go-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Month')\n",
    "ax2.set_ylabel('Cumulative Additional Revenue (Millions $)')\n",
    "ax2.set_title('Projected Cumulative Impact', fontweight='bold')\n",
    "ax2.set_xticks(months)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part6'></a>\n",
    "# Part 6: Common Pitfalls\n",
    "\n",
    "---\n",
    "\n",
    "## 6.1 The Big Mistakes in A/B Testing\n",
    "\n",
    "| Pitfall | Problem | Solution |\n",
    "|---------|---------|----------|\n",
    "| **Peeking** | Checking results too early | Pre-define sample size, don't peek |\n",
    "| **Multiple Testing** | Testing many variants | Bonferroni/FDR correction |\n",
    "| **Simpson's Paradox** | Aggregation hides truth | Segment analysis |\n",
    "| **Novelty Effect** | Initial boost fades | Run test longer |\n",
    "| **Selection Bias** | Non-random assignment | Proper randomization |\n",
    "| **Survivorship Bias** | Only measure survivors | Intent-to-treat analysis |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PITFALL 1: PEEKING PROBLEM\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"PITFALL 1: THE PEEKING PROBLEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "THE PROBLEM:\n",
    "If you check p-values repeatedly as data comes in,\n",
    "you WILL find \"significance\" by chance!\n",
    "\n",
    "With α=0.05 and continuous monitoring:\n",
    "- After 100 peeks: ~40% chance of false positive\n",
    "- After 1000 peeks: ~80% chance of false positive\n",
    "\"\"\")\n",
    "\n",
    "# Simulate the peeking problem\n",
    "def simulate_peeking(n_simulations=1000, max_samples=5000, peek_interval=100):\n",
    "    \"\"\"\n",
    "    Simulate the false positive rate when peeking at results.\n",
    "    Both groups have SAME conversion rate (no real effect).\n",
    "    \"\"\"\n",
    "    true_rate = 0.10  # Same for both groups\n",
    "    false_positives = 0\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # Generate data incrementally\n",
    "        control = []\n",
    "        treatment = []\n",
    "        \n",
    "        found_significant = False\n",
    "        \n",
    "        for n in range(peek_interval, max_samples + 1, peek_interval):\n",
    "            # Add new data\n",
    "            control.extend(np.random.binomial(1, true_rate, peek_interval))\n",
    "            treatment.extend(np.random.binomial(1, true_rate, peek_interval))\n",
    "            \n",
    "            # Calculate p-value\n",
    "            count = np.array([sum(treatment), sum(control)])\n",
    "            nobs = np.array([len(treatment), len(control)])\n",
    "            _, p_value = proportions_ztest(count, nobs)\n",
    "            \n",
    "            if p_value < 0.05:\n",
    "                found_significant = True\n",
    "                break\n",
    "        \n",
    "        if found_significant:\n",
    "            false_positives += 1\n",
    "    \n",
    "    return false_positives / n_simulations\n",
    "\n",
    "# Run simulation\n",
    "print(\"\\nSimulating 1000 experiments with peeking (NO real effect)...\")\n",
    "false_positive_rate = simulate_peeking(n_simulations=500)\n",
    "print(f\"\\nFalse Positive Rate with peeking: {false_positive_rate:.1%}\")\n",
    "print(f\"Expected without peeking: 5.0%\")\n",
    "print(f\"\\nPeeking inflated false positives by {false_positive_rate/0.05:.1f}x!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PITFALL 2: MULTIPLE TESTING\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"PITFALL 2: MULTIPLE TESTING PROBLEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "THE PROBLEM:\n",
    "Testing multiple hypotheses increases false positive rate.\n",
    "\n",
    "P(at least 1 false positive) = 1 - (1 - α)^n\n",
    "\n",
    "With α=0.05:\n",
    "- 1 test:  5% chance of false positive\n",
    "- 10 tests: 40% chance of at least 1 false positive\n",
    "- 20 tests: 64% chance of at least 1 false positive\n",
    "\"\"\")\n",
    "\n",
    "# Demonstrate\n",
    "alpha = 0.05\n",
    "n_tests = np.arange(1, 51)\n",
    "familywise_error = 1 - (1 - alpha) ** n_tests\n",
    "\n",
    "# Bonferroni correction\n",
    "def bonferroni_correction(p_values, alpha=0.05):\n",
    "    \"\"\"Apply Bonferroni correction.\"\"\"\n",
    "    n = len(p_values)\n",
    "    adjusted_alpha = alpha / n\n",
    "    return [p < adjusted_alpha for p in p_values], adjusted_alpha\n",
    "\n",
    "# Benjamini-Hochberg (FDR) correction\n",
    "def benjamini_hochberg(p_values, alpha=0.05):\n",
    "    \"\"\"Apply Benjamini-Hochberg FDR correction.\"\"\"\n",
    "    n = len(p_values)\n",
    "    sorted_indices = np.argsort(p_values)\n",
    "    sorted_p = np.array(p_values)[sorted_indices]\n",
    "    \n",
    "    # Calculate BH threshold\n",
    "    thresholds = alpha * (np.arange(1, n + 1) / n)\n",
    "    \n",
    "    # Find significant tests\n",
    "    significant = sorted_p <= thresholds\n",
    "    \n",
    "    # Return in original order\n",
    "    result = np.zeros(n, dtype=bool)\n",
    "    result[sorted_indices] = significant\n",
    "    \n",
    "    return list(result)\n",
    "\n",
    "# Example with multiple tests\n",
    "np.random.seed(42)\n",
    "# Simulate 20 p-values (assume 3 are truly significant)\n",
    "p_values = list(np.random.uniform(0.01, 0.99, 17)) + [0.01, 0.02, 0.03]\n",
    "np.random.shuffle(p_values)\n",
    "\n",
    "# Apply corrections\n",
    "uncorrected = [p < 0.05 for p in p_values]\n",
    "bonf_sig, bonf_alpha = bonferroni_correction(p_values)\n",
    "bh_sig = benjamini_hochberg(p_values)\n",
    "\n",
    "print(f\"\\nEXAMPLE: 20 hypothesis tests\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'Test':<6} {'P-value':<12} {'Uncorrected':<14} {'Bonferroni':<14} {'BH (FDR)'}\")\n",
    "print(f\"{'-'*60}\")\n",
    "for i, (p, unc, bonf, bh) in enumerate(zip(p_values, uncorrected, bonf_sig, bh_sig)):\n",
    "    print(f\"{i+1:<6} {p:<12.4f} {'SIG' if unc else '':<14} {'SIG' if bonf else '':<14} {'SIG' if bh else ''}\")\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Uncorrected significant: {sum(uncorrected)}\")\n",
    "print(f\"  Bonferroni significant: {sum(bonf_sig)} (α = {bonf_alpha:.4f})\")\n",
    "print(f\"  BH (FDR) significant: {sum(bh_sig)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multiple testing\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Family-wise error rate\n",
    "ax1 = axes[0]\n",
    "ax1.plot(n_tests, familywise_error * 100, 'r-', linewidth=2)\n",
    "ax1.axhline(y=5, color='green', linestyle='--', label='Desired α=5%')\n",
    "ax1.fill_between(n_tests, familywise_error * 100, alpha=0.3, color='red')\n",
    "ax1.set_xlabel('Number of Tests')\n",
    "ax1.set_ylabel('P(At Least 1 False Positive) %')\n",
    "ax1.set_title('Family-wise Error Rate', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Correction comparison\n",
    "ax2 = axes[1]\n",
    "methods = ['Uncorrected', 'Bonferroni', 'BH (FDR)']\n",
    "counts = [sum(uncorrected), sum(bonf_sig), sum(bh_sig)]\n",
    "colors = ['red', 'steelblue', 'green']\n",
    "bars = ax2.bar(methods, counts, color=colors, edgecolor='black')\n",
    "ax2.set_ylabel('Number of Significant Results')\n",
    "ax2.set_title('Multiple Testing Corrections (20 tests)', fontweight='bold')\n",
    "ax2.axhline(y=3, color='orange', linestyle='--', label='True positives (3)')\n",
    "ax2.legend()\n",
    "\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n",
    "             str(count), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part7'></a>\n",
    "# Part 7: Bayesian A/B Testing\n",
    "\n",
    "---\n",
    "\n",
    "## 7.1 Frequentist vs Bayesian\n",
    "\n",
    "| Aspect | Frequentist | Bayesian |\n",
    "|--------|------------|----------|\n",
    "| **Question** | What's P(data \\| H₀)? | What's P(B better \\| data)? |\n",
    "| **Output** | p-value, CI | Posterior probability |\n",
    "| **Prior info** | Not used | Can incorporate |\n",
    "| **Interpretation** | \"Reject/Fail to reject\" | \"95% chance B is better\" |\n",
    "| **Sample size** | Fixed in advance | Can stop anytime |\n",
    "\n",
    "## 7.2 Bayesian Approach\n",
    "\n",
    "```\n",
    "Prior × Likelihood = Posterior\n",
    "\n",
    "P(θ|data) ∝ P(data|θ) × P(θ)\n",
    "\n",
    "For conversion rates, use Beta distribution:\n",
    "- Prior: Beta(α₀, β₀)\n",
    "- Posterior: Beta(α₀ + conversions, β₀ + non-conversions)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BAYESIAN A/B TESTING\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"BAYESIAN A/B TESTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from scipy.stats import beta\n",
    "\n",
    "class BayesianABTest:\n",
    "    \"\"\"\n",
    "    Bayesian A/B testing using Beta-Binomial model.\n",
    "    \n",
    "    Prior: Beta(α, β)\n",
    "    - α = 1, β = 1 gives uniform prior (no prior knowledge)\n",
    "    - Can use historical data to set informative prior\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, prior_alpha=1, prior_beta=1):\n",
    "        self.prior_alpha = prior_alpha\n",
    "        self.prior_beta = prior_beta\n",
    "    \n",
    "    def update(self, conversions, n):\n",
    "        \"\"\"\n",
    "        Update posterior with observed data.\n",
    "        \n",
    "        Posterior: Beta(α + conversions, β + non-conversions)\n",
    "        \"\"\"\n",
    "        post_alpha = self.prior_alpha + conversions\n",
    "        post_beta = self.prior_beta + (n - conversions)\n",
    "        return post_alpha, post_beta\n",
    "    \n",
    "    def prob_b_better(self, conv_a, n_a, conv_b, n_b, n_samples=100000):\n",
    "        \"\"\"\n",
    "        Calculate P(B > A) using Monte Carlo simulation.\n",
    "        \n",
    "        Sample from both posteriors and count how often B > A.\n",
    "        \"\"\"\n",
    "        # Get posteriors\n",
    "        alpha_a, beta_a = self.update(conv_a, n_a)\n",
    "        alpha_b, beta_b = self.update(conv_b, n_b)\n",
    "        \n",
    "        # Sample from posteriors\n",
    "        samples_a = beta.rvs(alpha_a, beta_a, size=n_samples)\n",
    "        samples_b = beta.rvs(alpha_b, beta_b, size=n_samples)\n",
    "        \n",
    "        # Calculate probability B > A\n",
    "        prob = np.mean(samples_b > samples_a)\n",
    "        \n",
    "        return prob, samples_a, samples_b\n",
    "    \n",
    "    def expected_loss(self, conv_a, n_a, conv_b, n_b, n_samples=100000):\n",
    "        \"\"\"\n",
    "        Calculate expected loss of choosing B over A.\n",
    "        \n",
    "        Loss = max(0, θ_A - θ_B) if we choose B\n",
    "        \"\"\"\n",
    "        # Get posteriors\n",
    "        alpha_a, beta_a = self.update(conv_a, n_a)\n",
    "        alpha_b, beta_b = self.update(conv_b, n_b)\n",
    "        \n",
    "        # Sample\n",
    "        samples_a = beta.rvs(alpha_a, beta_a, size=n_samples)\n",
    "        samples_b = beta.rvs(alpha_b, beta_b, size=n_samples)\n",
    "        \n",
    "        # Expected loss of choosing B\n",
    "        loss_b = np.mean(np.maximum(0, samples_a - samples_b))\n",
    "        \n",
    "        # Expected loss of choosing A\n",
    "        loss_a = np.mean(np.maximum(0, samples_b - samples_a))\n",
    "        \n",
    "        return loss_a, loss_b\n",
    "\n",
    "# Run Bayesian analysis\n",
    "bayes = BayesianABTest(prior_alpha=1, prior_beta=1)  # Uniform prior\n",
    "\n",
    "prob_b_better, samples_a, samples_b = bayes.prob_b_better(\n",
    "    conversions_control, n_control,\n",
    "    conversions_treatment, n_treatment\n",
    ")\n",
    "\n",
    "loss_a, loss_b = bayes.expected_loss(\n",
    "    conversions_control, n_control,\n",
    "    conversions_treatment, n_treatment\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "BAYESIAN A/B TEST RESULTS\n",
    "{'='*50}\n",
    "Prior: Beta(1, 1) - Uniform (no prior knowledge)\n",
    "\n",
    "Control (A):\n",
    "  Conversions: {conversions_control:,} / {n_control:,}\n",
    "  Posterior: Beta({1 + conversions_control}, {1 + n_control - conversions_control})\n",
    "  \n",
    "Treatment (B):\n",
    "  Conversions: {conversions_treatment:,} / {n_treatment:,}\n",
    "  Posterior: Beta({1 + conversions_treatment}, {1 + n_treatment - conversions_treatment})\n",
    "\n",
    "RESULTS:\n",
    "  P(Treatment > Control) = {prob_b_better:.2%}\n",
    "  P(Control > Treatment) = {1-prob_b_better:.2%}\n",
    "  \n",
    "  Expected Loss (choosing A): {loss_a:.4f} ({loss_a:.2%})\n",
    "  Expected Loss (choosing B): {loss_b:.4f} ({loss_b:.2%})\n",
    "\n",
    "RECOMMENDATION:\n",
    "\"\"\")\n",
    "\n",
    "if prob_b_better > 0.95:\n",
    "    print(f\"  Strong evidence that Treatment is better ({prob_b_better:.1%} probability)\")\n",
    "    print(f\"  → Recommend implementing Treatment\")\n",
    "elif prob_b_better > 0.75:\n",
    "    print(f\"  Moderate evidence that Treatment is better ({prob_b_better:.1%} probability)\")\n",
    "    print(f\"  → Consider running test longer for more certainty\")\n",
    "else:\n",
    "    print(f\"  Insufficient evidence ({prob_b_better:.1%} probability)\")\n",
    "    print(f\"  → Continue the test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Bayesian posteriors\n",
    "print(\"=\"*70)\n",
    "print(\"BAYESIAN POSTERIOR VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Posterior distributions\n",
    "ax1 = axes[0]\n",
    "ax1.hist(samples_a, bins=100, density=True, alpha=0.6, label='Control (A)', color='steelblue')\n",
    "ax1.hist(samples_b, bins=100, density=True, alpha=0.6, label='Treatment (B)', color='coral')\n",
    "ax1.axvline(np.mean(samples_a), color='steelblue', linestyle='--', linewidth=2)\n",
    "ax1.axvline(np.mean(samples_b), color='coral', linestyle='--', linewidth=2)\n",
    "ax1.set_xlabel('Conversion Rate')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Posterior Distributions', fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Difference distribution\n",
    "ax2 = axes[1]\n",
    "diff_samples = samples_b - samples_a\n",
    "ax2.hist(diff_samples, bins=100, density=True, alpha=0.7, color='green', edgecolor='black')\n",
    "ax2.axvline(0, color='red', linestyle='--', linewidth=2, label='No Difference')\n",
    "ax2.axvline(np.mean(diff_samples), color='blue', linestyle='-', linewidth=2, \n",
    "            label=f'Mean Diff: {np.mean(diff_samples):.4f}')\n",
    "\n",
    "# Shade probability of B > A\n",
    "x_fill = diff_samples[diff_samples > 0]\n",
    "ax2.fill_between([0, np.max(diff_samples)], [0, 0], [ax2.get_ylim()[1], ax2.get_ylim()[1]], \n",
    "                 alpha=0.3, color='green', label=f'P(B>A) = {prob_b_better:.1%}')\n",
    "\n",
    "ax2.set_xlabel('Difference (Treatment - Control)')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Posterior of Difference', fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Credible interval\n",
    "ci_low, ci_high = np.percentile(diff_samples, [2.5, 97.5])\n",
    "print(f\"\\n95% Credible Interval for Difference: [{ci_low:.4f}, {ci_high:.4f}]\")\n",
    "print(f\"  In plain English: We're 95% confident the true difference\")\n",
    "print(f\"  is between {ci_low:.2%} and {ci_high:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part8'></a>\n",
    "# Part 8: Multi-Armed Bandits\n",
    "\n",
    "---\n",
    "\n",
    "## 8.1 Exploration vs Exploitation\n",
    "\n",
    "| Approach | Trade-off |\n",
    "|----------|----------|\n",
    "| **A/B Test** | Pure exploration, then exploitation |\n",
    "| **Bandit** | Balance exploration & exploitation |\n",
    "\n",
    "## 8.2 When to Use Bandits\n",
    "\n",
    "| Use Bandits When | Use A/B Tests When |\n",
    "|------------------|--------------------|\n",
    "| High opportunity cost | Need statistical rigor |\n",
    "| Many variants | Few variants |\n",
    "| Quick decisions needed | Can wait for results |\n",
    "| Personalization | One-size-fits-all |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MULTI-ARMED BANDIT ALGORITHMS\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"MULTI-ARMED BANDITS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class EpsilonGreedy:\n",
    "    \"\"\"\n",
    "    Epsilon-Greedy bandit algorithm.\n",
    "    \n",
    "    - With probability ε: Explore (random arm)\n",
    "    - With probability 1-ε: Exploit (best arm so far)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms, epsilon=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "    \n",
    "    def select_arm(self):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_arms)  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.values)  # Exploit\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        self.values[arm] = ((n - 1) * self.values[arm] + reward) / n\n",
    "\n",
    "\n",
    "class ThompsonSampling:\n",
    "    \"\"\"\n",
    "    Thompson Sampling bandit algorithm.\n",
    "    \n",
    "    - Maintain Beta posterior for each arm\n",
    "    - Sample from each posterior\n",
    "    - Choose arm with highest sample\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.alphas = np.ones(n_arms)  # Successes + 1\n",
    "        self.betas = np.ones(n_arms)   # Failures + 1\n",
    "    \n",
    "    def select_arm(self):\n",
    "        samples = [beta.rvs(self.alphas[i], self.betas[i]) for i in range(self.n_arms)]\n",
    "        return np.argmax(samples)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        if reward == 1:\n",
    "            self.alphas[arm] += 1\n",
    "        else:\n",
    "            self.betas[arm] += 1\n",
    "\n",
    "\n",
    "class UCB1:\n",
    "    \"\"\"\n",
    "    Upper Confidence Bound (UCB1) algorithm.\n",
    "    \n",
    "    - Choose arm with highest: mean + exploration_bonus\n",
    "    - Exploration bonus decreases as arm is pulled more\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "        self.total = 0\n",
    "    \n",
    "    def select_arm(self):\n",
    "        # First, try each arm once\n",
    "        for arm in range(self.n_arms):\n",
    "            if self.counts[arm] == 0:\n",
    "                return arm\n",
    "        \n",
    "        # UCB formula\n",
    "        ucb_values = self.values + np.sqrt(2 * np.log(self.total) / self.counts)\n",
    "        return np.argmax(ucb_values)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        self.total += 1\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        self.values[arm] = ((n - 1) * self.values[arm] + reward) / n\n",
    "\n",
    "print(\"Bandit algorithms created:\")\n",
    "print(\"  1. Epsilon-Greedy: Simple exploration with probability ε\")\n",
    "print(\"  2. Thompson Sampling: Bayesian approach, samples from posteriors\")\n",
    "print(\"  3. UCB1: Optimism in face of uncertainty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate bandits\n",
    "print(\"=\"*70)\n",
    "print(\"BANDIT SIMULATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def simulate_bandit(bandit, true_rates, n_rounds=10000):\n",
    "    \"\"\"\n",
    "    Simulate a bandit algorithm.\n",
    "    \n",
    "    Returns:\n",
    "    - rewards: Total reward over time\n",
    "    - arm_pulls: Number of times each arm was pulled\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    arm_selections = []\n",
    "    cumulative = 0\n",
    "    \n",
    "    for _ in range(n_rounds):\n",
    "        arm = bandit.select_arm()\n",
    "        reward = np.random.binomial(1, true_rates[arm])\n",
    "        bandit.update(arm, reward)\n",
    "        \n",
    "        cumulative += reward\n",
    "        rewards.append(cumulative)\n",
    "        arm_selections.append(arm)\n",
    "    \n",
    "    return rewards, arm_selections\n",
    "\n",
    "# True conversion rates (arm 2 is best)\n",
    "true_rates = [0.10, 0.12, 0.15]  # Control, Treatment1, Treatment2\n",
    "n_rounds = 10000\n",
    "\n",
    "# Run simulations\n",
    "np.random.seed(42)\n",
    "\n",
    "eg_bandit = EpsilonGreedy(n_arms=3, epsilon=0.1)\n",
    "ts_bandit = ThompsonSampling(n_arms=3)\n",
    "ucb_bandit = UCB1(n_arms=3)\n",
    "\n",
    "eg_rewards, eg_arms = simulate_bandit(eg_bandit, true_rates, n_rounds)\n",
    "\n",
    "np.random.seed(42)\n",
    "ts_bandit = ThompsonSampling(n_arms=3)\n",
    "ts_rewards, ts_arms = simulate_bandit(ts_bandit, true_rates, n_rounds)\n",
    "\n",
    "np.random.seed(42)\n",
    "ucb_bandit = UCB1(n_arms=3)\n",
    "ucb_rewards, ucb_arms = simulate_bandit(ucb_bandit, true_rates, n_rounds)\n",
    "\n",
    "# Optimal (always best arm)\n",
    "optimal_rewards = np.cumsum(np.random.binomial(1, max(true_rates), n_rounds))\n",
    "\n",
    "print(f\"\\nTrue conversion rates: {true_rates}\")\n",
    "print(f\"Best arm: 2 (rate = {max(true_rates)})\")\n",
    "print(f\"\\nTotal rewards after {n_rounds:,} rounds:\")\n",
    "print(f\"  Epsilon-Greedy: {eg_rewards[-1]:,}\")\n",
    "print(f\"  Thompson Sampling: {ts_rewards[-1]:,}\")\n",
    "print(f\"  UCB1: {ucb_rewards[-1]:,}\")\n",
    "print(f\"  Optimal: {optimal_rewards[-1]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bandit performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cumulative rewards\n",
    "ax1 = axes[0]\n",
    "rounds = range(n_rounds)\n",
    "ax1.plot(rounds, eg_rewards, label='Epsilon-Greedy', alpha=0.8)\n",
    "ax1.plot(rounds, ts_rewards, label='Thompson Sampling', alpha=0.8)\n",
    "ax1.plot(rounds, ucb_rewards, label='UCB1', alpha=0.8)\n",
    "ax1.plot(rounds, optimal_rewards, 'k--', label='Optimal', alpha=0.5)\n",
    "ax1.set_xlabel('Round')\n",
    "ax1.set_ylabel('Cumulative Reward')\n",
    "ax1.set_title('Bandit Algorithm Comparison', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Arm selection distribution\n",
    "ax2 = axes[1]\n",
    "x = np.arange(3)\n",
    "width = 0.25\n",
    "\n",
    "eg_counts = [eg_arms.count(i) for i in range(3)]\n",
    "ts_counts = [ts_arms.count(i) for i in range(3)]\n",
    "ucb_counts = [ucb_arms.count(i) for i in range(3)]\n",
    "\n",
    "ax2.bar(x - width, eg_counts, width, label='Epsilon-Greedy')\n",
    "ax2.bar(x, ts_counts, width, label='Thompson Sampling')\n",
    "ax2.bar(x + width, ucb_counts, width, label='UCB1')\n",
    "\n",
    "ax2.set_xlabel('Arm')\n",
    "ax2.set_ylabel('Times Selected')\n",
    "ax2.set_title('Arm Selection Distribution', fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(['Arm 0\\n(10%)', 'Arm 1\\n(12%)', 'Arm 2\\n(15%)'])\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: Thompson Sampling typically focuses on the best arm\")\n",
    "print(\"while still exploring enough to find it quickly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part9'></a>\n",
    "# Part 9: Complete A/B Testing Framework\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPLETE A/B TESTING FRAMEWORK\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"COMPLETE A/B TESTING FRAMEWORK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class ABTestFramework:\n",
    "    \"\"\"\n",
    "    Complete A/B Testing Framework.\n",
    "    \n",
    "    Features:\n",
    "    - Sample size calculation\n",
    "    - Multiple statistical tests\n",
    "    - Bayesian analysis\n",
    "    - Effect size calculation\n",
    "    - Business impact estimation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.05, power=0.80):\n",
    "        self.alpha = alpha\n",
    "        self.power = power\n",
    "        self.results = {}\n",
    "        \n",
    "    def calculate_sample_size(self, baseline_rate, mde_relative):\n",
    "        \"\"\"\n",
    "        Calculate required sample size.\n",
    "        \n",
    "        Args:\n",
    "            baseline_rate: Current conversion rate\n",
    "            mde_relative: Minimum detectable effect (relative, e.g., 0.10 for 10%)\n",
    "        \"\"\"\n",
    "        expected_rate = baseline_rate * (1 + mde_relative)\n",
    "        \n",
    "        z_alpha = norm.ppf(1 - self.alpha/2)\n",
    "        z_beta = norm.ppf(self.power)\n",
    "        p_bar = (baseline_rate + expected_rate) / 2\n",
    "        \n",
    "        n = 2 * ((z_alpha + z_beta)**2) * p_bar * (1 - p_bar) / ((expected_rate - baseline_rate)**2)\n",
    "        \n",
    "        return int(np.ceil(n))\n",
    "    \n",
    "    def run_test(self, control_data, treatment_data, metric_type='binary'):\n",
    "        \"\"\"\n",
    "        Run complete A/B test analysis.\n",
    "        \n",
    "        Args:\n",
    "            control_data: Array of control group outcomes\n",
    "            treatment_data: Array of treatment group outcomes\n",
    "            metric_type: 'binary' or 'continuous'\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Basic statistics\n",
    "        n_control = len(control_data)\n",
    "        n_treatment = len(treatment_data)\n",
    "        \n",
    "        if metric_type == 'binary':\n",
    "            # Conversion rates\n",
    "            conv_control = sum(control_data)\n",
    "            conv_treatment = sum(treatment_data)\n",
    "            rate_control = conv_control / n_control\n",
    "            rate_treatment = conv_treatment / n_treatment\n",
    "            \n",
    "            # Z-test\n",
    "            z_result = ABTestSuite.z_test_proportions(\n",
    "                conv_control, n_control, conv_treatment, n_treatment, self.alpha\n",
    "            )\n",
    "            \n",
    "            # Bayesian\n",
    "            bayes = BayesianABTest()\n",
    "            prob_b_better, _, _ = bayes.prob_b_better(\n",
    "                conv_control, n_control, conv_treatment, n_treatment\n",
    "            )\n",
    "            \n",
    "            results['frequentist'] = z_result\n",
    "            results['bayesian'] = {'prob_b_better': prob_b_better}\n",
    "            results['effect'] = {\n",
    "                'absolute': rate_treatment - rate_control,\n",
    "                'relative': (rate_treatment - rate_control) / rate_control * 100\n",
    "            }\n",
    "            \n",
    "        else:  # continuous\n",
    "            # T-test\n",
    "            t_result = ABTestSuite.t_test(control_data, treatment_data, self.alpha)\n",
    "            results['frequentist'] = t_result\n",
    "        \n",
    "        results['sample_sizes'] = {'control': n_control, 'treatment': n_treatment}\n",
    "        \n",
    "        self.results = results\n",
    "        return results\n",
    "    \n",
    "    def get_recommendation(self):\n",
    "        \"\"\"\n",
    "        Get recommendation based on test results.\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            return \"No test results available\"\n",
    "        \n",
    "        freq = self.results.get('frequentist', {})\n",
    "        bayes = self.results.get('bayesian', {})\n",
    "        effect = self.results.get('effect', {})\n",
    "        \n",
    "        # Decision logic\n",
    "        is_significant = freq.get('significant', False)\n",
    "        prob_better = bayes.get('prob_b_better', 0.5)\n",
    "        relative_lift = effect.get('relative', 0)\n",
    "        \n",
    "        if is_significant and prob_better > 0.95 and relative_lift > 0:\n",
    "            return \"STRONG RECOMMENDATION: Implement Treatment\"\n",
    "        elif is_significant and prob_better > 0.80 and relative_lift > 0:\n",
    "            return \"RECOMMENDATION: Implement Treatment (with monitoring)\"\n",
    "        elif prob_better > 0.75:\n",
    "            return \"CAUTIOUS: Treatment looks promising, consider extending test\"\n",
    "        elif prob_better < 0.25:\n",
    "            return \"NOT RECOMMENDED: Treatment performs worse than Control\"\n",
    "        else:\n",
    "            return \"INCONCLUSIVE: Continue testing or accept no difference\"\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"\n",
    "        Generate comprehensive test report.\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            return \"No results to report\"\n",
    "        \n",
    "        freq = self.results.get('frequentist', {})\n",
    "        bayes = self.results.get('bayesian', {})\n",
    "        effect = self.results.get('effect', {})\n",
    "        sizes = self.results.get('sample_sizes', {})\n",
    "        \n",
    "        report = f\"\"\"\n",
    "{'='*60}\n",
    "A/B TEST REPORT\n",
    "{'='*60}\n",
    "\n",
    "SAMPLE SIZES\n",
    "{'-'*40}\n",
    "Control:   {sizes.get('control', 'N/A'):,}\n",
    "Treatment: {sizes.get('treatment', 'N/A'):,}\n",
    "\n",
    "FREQUENTIST ANALYSIS\n",
    "{'-'*40}\n",
    "Test: {freq.get('test', 'N/A')}\n",
    "Test Statistic: {freq.get('statistic', 'N/A'):.4f}\n",
    "P-value: {freq.get('p_value', 'N/A'):.6f}\n",
    "Significant (α={self.alpha}): {'YES' if freq.get('significant') else 'NO'}\n",
    "95% CI: {freq.get('ci_95', 'N/A')}\n",
    "\n",
    "BAYESIAN ANALYSIS\n",
    "{'-'*40}\n",
    "P(Treatment > Control): {bayes.get('prob_b_better', 'N/A'):.2%}\n",
    "\n",
    "EFFECT SIZE\n",
    "{'-'*40}\n",
    "Absolute Difference: {effect.get('absolute', 0):.4f} ({effect.get('absolute', 0):.2%})\n",
    "Relative Lift: {effect.get('relative', 0):.1f}%\n",
    "\n",
    "RECOMMENDATION\n",
    "{'-'*40}\n",
    "{self.get_recommendation()}\n",
    "\n",
    "{'='*60}\n",
    "\"\"\"\n",
    "        return report\n",
    "\n",
    "# Use the framework\n",
    "framework = ABTestFramework(alpha=0.05, power=0.80)\n",
    "\n",
    "# Run test\n",
    "results = framework.run_test(\n",
    "    control['converted'].values,\n",
    "    treatment['converted'].values,\n",
    "    metric_type='binary'\n",
    ")\n",
    "\n",
    "# Generate report\n",
    "print(framework.generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part10'></a>\n",
    "# Part 10: Summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"A/B TESTING FRAMEWORK - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "WHAT WE LEARNED:\n",
    "================\n",
    "\n",
    "1. HYPOTHESIS TESTING FRAMEWORK:\n",
    "   ┌─────────────────────────────────────────────┐\n",
    "   │ H₀: No difference (p_A = p_B)              │\n",
    "   │ H₁: There is a difference (p_A ≠ p_B)      │\n",
    "   │                                             │\n",
    "   │ If p-value < α: Reject H₀ (significant)    │\n",
    "   │ If p-value ≥ α: Fail to reject H₀          │\n",
    "   └─────────────────────────────────────────────┘\n",
    "\n",
    "2. CHOOSING THE RIGHT TEST:\n",
    "   ┌──────────────────┬─────────────────────────┐\n",
    "   │ Data Type        │ Test                    │\n",
    "   ├──────────────────┼─────────────────────────┤\n",
    "   │ Binary           │ Z-test, Chi-square      │\n",
    "   │ Continuous       │ t-test                  │\n",
    "   │ Non-normal       │ Mann-Whitney U          │\n",
    "   │ Small samples    │ Fisher's exact          │\n",
    "   └──────────────────┴─────────────────────────┘\n",
    "\n",
    "3. SAMPLE SIZE FORMULA (Proportions):\n",
    "   n = 2 × (Z_{α/2} + Z_β)² × p̄(1-p̄) / (p₁ - p₂)²\n",
    "\n",
    "4. KEY METRICS:\n",
    "   - Statistical Significance: p-value < α\n",
    "   - Practical Significance: Effect size, business impact\n",
    "   - Confidence Interval: Range of plausible values\n",
    "\n",
    "5. COMMON PITFALLS:\n",
    "   ┌─────────────────────────────────────────────┐\n",
    "   │ ❌ Peeking at results early                 │\n",
    "   │ ❌ Multiple testing without correction      │\n",
    "   │ ❌ Stopping early when significant          │\n",
    "   │ ❌ Ignoring practical significance          │\n",
    "   │ ❌ Selection bias in assignment             │\n",
    "   └─────────────────────────────────────────────┘\n",
    "\n",
    "6. BAYESIAN A/B TESTING:\n",
    "   - Output: P(B > A) directly\n",
    "   - More intuitive interpretation\n",
    "   - Can incorporate prior knowledge\n",
    "   - Flexible stopping rules\n",
    "\n",
    "7. MULTI-ARMED BANDITS:\n",
    "   - Balance exploration vs exploitation\n",
    "   - Minimize opportunity cost\n",
    "   - Thompson Sampling often best choice\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nA/B TESTING CHECKLIST:\")\n",
    "print(\"  [1] Define hypothesis and success metric\")\n",
    "print(\"  [2] Calculate required sample size\")\n",
    "print(\"  [3] Randomize users properly\")\n",
    "print(\"  [4] Run test for full duration (no peeking!)\")\n",
    "print(\"  [5] Analyze with appropriate statistical test\")\n",
    "print(\"  [6] Consider both statistical AND practical significance\")\n",
    "print(\"  [7] Document and communicate results\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm & Method Taxonomy\n",
    "\n",
    "### Statistical Tests\n",
    "\n",
    "| Test | Data Type | Assumption | Use Case |\n",
    "|------|-----------|------------|----------|\n",
    "| **Z-test** | Binary | Large n | Conversion rates |\n",
    "| **Chi-square** | Binary | Large n | Independence test |\n",
    "| **t-test** | Continuous | ~Normal | Revenue, time |\n",
    "| **Welch's t-test** | Continuous | Unequal variance | Most cases |\n",
    "| **Mann-Whitney U** | Continuous | Non-parametric | Skewed data |\n",
    "| **Fisher's exact** | Binary | Small n | Small samples |\n",
    "\n",
    "### Effect Size Measures\n",
    "\n",
    "| Measure | Formula | Interpretation |\n",
    "|---------|---------|----------------|\n",
    "| **Absolute Diff** | p_B - p_A | Direct difference |\n",
    "| **Relative Lift** | (p_B - p_A) / p_A | % improvement |\n",
    "| **Odds Ratio** | (p_B/(1-p_B)) / (p_A/(1-p_A)) | Odds comparison |\n",
    "| **Cohen's d** | (μ_B - μ_A) / σ_pooled | Standardized (continuous) |\n",
    "| **Cohen's h** | 2(arcsin√p_B - arcsin√p_A) | Standardized (binary) |\n",
    "\n",
    "### Multiple Testing Corrections\n",
    "\n",
    "| Method | Controls | When to Use |\n",
    "|--------|----------|-------------|\n",
    "| **Bonferroni** | FWER | Few tests, need strict control |\n",
    "| **Benjamini-Hochberg** | FDR | Many tests, can tolerate some FP |\n",
    "| **Holm** | FWER | Stepwise, more powerful than Bonferroni |\n",
    "\n",
    "### Bandit Algorithms\n",
    "\n",
    "| Algorithm | Strategy | Pros | Cons |\n",
    "|-----------|----------|------|------|\n",
    "| **Epsilon-Greedy** | Random exploration | Simple | Suboptimal |\n",
    "| **UCB1** | Optimistic exploration | No parameters | Can over-explore |\n",
    "| **Thompson Sampling** | Probability matching | Often best | More complex |\n",
    "\n",
    "---\n",
    "\n",
    "## Checklist\n",
    "\n",
    "- [x] Understand null and alternative hypotheses\n",
    "- [x] Know when to use which statistical test\n",
    "- [x] Can calculate required sample size\n",
    "- [x] Understand statistical vs practical significance\n",
    "- [x] Know common pitfalls (peeking, multiple testing)\n",
    "- [x] Can apply multiple testing corrections\n",
    "- [x] Understand Bayesian A/B testing\n",
    "- [x] Know when to use multi-armed bandits\n",
    "\n",
    "---\n",
    "\n",
    "**End of A/B Testing Framework Tutorial**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}