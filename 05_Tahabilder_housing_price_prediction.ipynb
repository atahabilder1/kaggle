{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California Housing Price Prediction\n",
    "\n",
    "**Author:** Anik Tahabilder  \n",
    "**Project:** 5 of 22 - Kaggle ML Portfolio  \n",
    "**Dataset:** California Housing  \n",
    "**Difficulty:** 4/10 | **Learning Value:** 8/10\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "Predict **median house values** in California districts using census data. This is a classic **regression problem** - predicting a continuous value.\n",
    "\n",
    "### What You'll Learn:\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| **Data Exploration** | Understand geographic and demographic features |\n",
    "| **Handling Missing Values** | Imputation strategies |\n",
    "| **Feature Engineering** | Create meaningful derived features |\n",
    "| **Multiple Regression Models** | Compare Linear, Ridge, Lasso, Random Forest, etc. |\n",
    "| **Evaluation Metrics** | MSE, RMSE, MAE, R² for regression |\n",
    "\n",
    "### Dataset Features:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| longitude/latitude | Geographic coordinates |\n",
    "| housing_median_age | Median age of houses in block |\n",
    "| total_rooms | Total rooms in block |\n",
    "| total_bedrooms | Total bedrooms in block |\n",
    "| population | Population in block |\n",
    "| households | Number of households |\n",
    "| median_income | Median income (scaled) |\n",
    "| ocean_proximity | Location relative to ocean |\n",
    "| **median_house_value** | **TARGET** - Median house value |\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: Setup and Data Loading](#part1)\n",
    "2. [Part 2: Exploratory Data Analysis](#part2)\n",
    "3. [Part 3: Data Preprocessing](#part3)\n",
    "4. [Part 4: Feature Engineering](#part4)\n",
    "5. [Part 5: Model Training](#part5)\n",
    "6. [Part 6: Model Evaluation](#part6)\n",
    "7. [Part 7: Feature Importance](#part7)\n",
    "8. [Part 8: Summary and Conclusions](#part8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part1'></a>\n",
    "# Part 1: Setup and Data Loading\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Regression models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"scikit-learn version: {__import__('sklearn').__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('housing.csv')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CALIFORNIA HOUSING DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nShape: {df.shape[0]:,} samples, {df.shape[1]} features\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA INFO\")\n",
    "print(\"=\" * 60)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part2'></a>\n",
    "# Part 2: Exploratory Data Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Missing': missing, 'Percent': missing_pct})\n",
    "print(missing_df[missing_df['Missing'] > 0])\n",
    "\n",
    "print(f\"\\nTotal missing: {missing.sum()} ({missing.sum()/len(df)*100:.2f}%)\")\n",
    "print(\"\\nOnly total_bedrooms has missing values - will impute with median.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable: median_house_value\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df['median_house_value'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].axvline(df['median_house_value'].mean(), color='red', linestyle='--', label=f\"Mean: ${df['median_house_value'].mean():,.0f}\")\n",
    "axes[0].axvline(df['median_house_value'].median(), color='green', linestyle='--', label=f\"Median: ${df['median_house_value'].median():,.0f}\")\n",
    "axes[0].set_xlabel('Median House Value ($)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of House Values', fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(df['median_house_value'], vert=True)\n",
    "axes[1].set_ylabel('Median House Value ($)')\n",
    "axes[1].set_title('Box Plot of House Values', fontweight='bold')\n",
    "\n",
    "# Log-transformed\n",
    "axes[2].hist(np.log1p(df['median_house_value']), bins=50, edgecolor='black', alpha=0.7, color='darkorange')\n",
    "axes[2].set_xlabel('Log(Median House Value)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Log-Transformed Distribution', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Right-skewed distribution\")\n",
    "print(\"- Cap at $500,001 (data censored)\")\n",
    "print(f\"- Range: ${df['median_house_value'].min():,.0f} - ${df['median_house_value'].max():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Geographic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Population density\n",
    "scatter1 = axes[0].scatter(df['longitude'], df['latitude'], \n",
    "                           c=df['population'], cmap='viridis',\n",
    "                           alpha=0.4, s=df['population']/100)\n",
    "axes[0].set_xlabel('Longitude')\n",
    "axes[0].set_ylabel('Latitude')\n",
    "axes[0].set_title('California: Population Density', fontweight='bold')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Population')\n",
    "\n",
    "# House values\n",
    "scatter2 = axes[1].scatter(df['longitude'], df['latitude'], \n",
    "                           c=df['median_house_value'], cmap='jet',\n",
    "                           alpha=0.4, s=df['population']/100)\n",
    "axes[1].set_xlabel('Longitude')\n",
    "axes[1].set_ylabel('Latitude')\n",
    "axes[1].set_title('California: House Values', fontweight='bold')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Median House Value ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGeographic Insights:\")\n",
    "print(\"- Coastal areas (especially Bay Area, LA) have highest values\")\n",
    "print(\"- Population concentrated along coast\")\n",
    "print(\"- Interior areas generally cheaper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Ocean Proximity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ocean proximity analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"OCEAN PROXIMITY CATEGORIES\")\n",
    "print(\"=\" * 60)\n",
    "print(df['ocean_proximity'].value_counts())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count\n",
    "ocean_counts = df['ocean_proximity'].value_counts()\n",
    "axes[0].bar(ocean_counts.index, ocean_counts.values, color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Ocean Proximity')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Ocean Proximity', fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# House values by ocean proximity\n",
    "df.boxplot(column='median_house_value', by='ocean_proximity', ax=axes[1])\n",
    "axes[1].set_xlabel('Ocean Proximity')\n",
    "axes[1].set_ylabel('Median House Value ($)')\n",
    "axes[1].set_title('House Values by Ocean Proximity', fontweight='bold')\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mean values by ocean proximity\n",
    "print(\"\\nMean House Value by Ocean Proximity:\")\n",
    "print(df.groupby('ocean_proximity')['median_house_value'].mean().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical features\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_cols.remove('median_house_value')  # Remove target\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    axes[i].hist(df[col].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_title(f'Distribution of {col}', fontweight='bold')\n",
    "\n",
    "# Hide empty subplot\n",
    "if len(numerical_cols) < 9:\n",
    "    axes[-1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "corr_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0,\n",
    "            fmt='.2f', linewidths=0.5, mask=mask,\n",
    "            annot_kws={'size': 10})\n",
    "plt.title('Feature Correlation Matrix', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlations with target\n",
    "print(\"\\nCorrelations with median_house_value:\")\n",
    "target_corr = corr_matrix['median_house_value'].sort_values(ascending=False)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots of top correlated features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "top_features = ['median_income', 'total_rooms', 'housing_median_age', 'latitude']\n",
    "\n",
    "for i, feat in enumerate(top_features):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    ax.scatter(df[feat], df['median_house_value'], alpha=0.3, s=5)\n",
    "    ax.set_xlabel(feat)\n",
    "    ax.set_ylabel('Median House Value ($)')\n",
    "    ax.set_title(f'{feat} vs House Value (r={corr_matrix.loc[feat, \"median_house_value\"]:.2f})', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: Median income has the STRONGEST correlation with house value!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part3'></a>\n",
    "# Part 3: Data Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean copy\n",
    "data = df.copy()\n",
    "\n",
    "# Impute total_bedrooms with median\n",
    "print(\"Handling missing values...\")\n",
    "print(f\"  total_bedrooms missing before: {data['total_bedrooms'].isnull().sum()}\")\n",
    "\n",
    "data['total_bedrooms'] = data['total_bedrooms'].fillna(data['total_bedrooms'].median())\n",
    "\n",
    "print(f\"  total_bedrooms missing after: {data['total_bedrooms'].isnull().sum()}\")\n",
    "print(\"\\nMissing values handled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for capped values in target\n",
    "print(\"=\" * 60)\n",
    "print(\"CAPPED VALUES CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "capped = (data['median_house_value'] >= 500000).sum()\n",
    "print(f\"\\nHouses at $500,001 cap: {capped} ({capped/len(data)*100:.1f}%)\")\n",
    "print(\"\\nNote: These are censored values - actual prices could be higher.\")\n",
    "print(\"For this tutorial, we'll keep them as-is.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part4'></a>\n",
    "# Part 4: Feature Engineering\n",
    "\n",
    "---\n",
    "\n",
    "Create new features that might be more predictive than raw values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Rooms per household\n",
    "data['rooms_per_household'] = data['total_rooms'] / data['households']\n",
    "print(\"\\n1. Created 'rooms_per_household' = total_rooms / households\")\n",
    "\n",
    "# 2. Bedrooms per room\n",
    "data['bedrooms_per_room'] = data['total_bedrooms'] / data['total_rooms']\n",
    "print(\"2. Created 'bedrooms_per_room' = total_bedrooms / total_rooms\")\n",
    "\n",
    "# 3. Population per household\n",
    "data['population_per_household'] = data['population'] / data['households']\n",
    "print(\"3. Created 'population_per_household' = population / households\")\n",
    "\n",
    "# 4. Bedrooms per household\n",
    "data['bedrooms_per_household'] = data['total_bedrooms'] / data['households']\n",
    "print(\"4. Created 'bedrooms_per_household' = total_bedrooms / households\")\n",
    "\n",
    "# 5. Income category\n",
    "data['income_category'] = pd.cut(data['median_income'],\n",
    "                                  bins=[0, 1.5, 3, 4.5, 6, np.inf],\n",
    "                                  labels=[1, 2, 3, 4, 5])\n",
    "data['income_category'] = data['income_category'].astype(int)\n",
    "print(\"5. Created 'income_category' (1-5 buckets)\")\n",
    "\n",
    "print(f\"\\nTotal features now: {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize engineered features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "engineered = ['rooms_per_household', 'bedrooms_per_room', 'population_per_household', 'income_category']\n",
    "\n",
    "for i, feat in enumerate(engineered):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    ax.scatter(data[feat], data['median_house_value'], alpha=0.3, s=5)\n",
    "    ax.set_xlabel(feat)\n",
    "    ax.set_ylabel('Median House Value ($)')\n",
    "    corr = data[feat].corr(data['median_house_value'])\n",
    "    ax.set_title(f'{feat} vs House Value (r={corr:.2f})', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEngineered Feature Correlations with Target:\")\n",
    "for feat in engineered:\n",
    "    corr = data[feat].corr(data['median_house_value'])\n",
    "    print(f\"  {feat}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Select features (drop target and original columns that we engineered from)\n",
    "feature_cols = [\n",
    "    'longitude', 'latitude', 'housing_median_age',\n",
    "    'total_rooms', 'total_bedrooms', 'population', 'households',\n",
    "    'median_income', 'ocean_proximity',\n",
    "    'rooms_per_household', 'bedrooms_per_room', \n",
    "    'population_per_household', 'bedrooms_per_household', 'income_category'\n",
    "]\n",
    "\n",
    "X = data[feature_cols].copy()\n",
    "y = data['median_house_value'].copy()\n",
    "\n",
    "print(f\"\\nFeatures: {len(feature_cols)}\")\n",
    "print(f\"  Numerical: {len(X.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"  Categorical: {len(X.select_dtypes(include=['object']).columns)}\")\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining set: {len(X_train):,} samples ({len(X_train)/len(X)*100:.0f}%)\")\n",
    "print(f\"Testing set: {len(X_test):,} samples ({len(X_test)/len(X)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify column types\n",
    "numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nX_train shape after preprocessing: {X_train_processed.shape}\")\n",
    "print(f\"X_test shape after preprocessing: {X_test_processed.shape}\")\n",
    "\n",
    "# Get feature names after one-hot encoding\n",
    "cat_encoder = preprocessor.named_transformers_['cat']\n",
    "cat_feature_names = cat_encoder.get_feature_names_out(categorical_features).tolist()\n",
    "all_feature_names = numerical_features + cat_feature_names\n",
    "print(f\"\\nTotal features after encoding: {len(all_feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part5'></a>\n",
    "# Part 5: Model Training\n",
    "\n",
    "---\n",
    "\n",
    "## Regression Algorithms\n",
    "\n",
    "| Algorithm | Type | Characteristics |\n",
    "|-----------|------|----------------|\n",
    "| **Linear Regression** | Linear | Simple, interpretable |\n",
    "| **Ridge** | Linear + L2 | Handles multicollinearity |\n",
    "| **Lasso** | Linear + L1 | Feature selection |\n",
    "| **Decision Tree** | Tree-based | Non-linear, interpretable |\n",
    "| **Random Forest** | Ensemble | Robust, handles overfitting |\n",
    "| **Gradient Boosting** | Ensemble | High accuracy |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso Regression': Lasso(alpha=1.0, random_state=42),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"Models to train:\")\n",
    "for i, name in enumerate(models.keys(), 1):\n",
    "    print(f\"  {i}. {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "results = {}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_train = model.predict(X_train_processed)\n",
    "    y_pred_test = model.predict(X_test_processed)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred_test,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'test_r2': test_r2\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Train RMSE: ${train_rmse:,.0f}\")\n",
    "    print(f\"  Test RMSE:  ${test_rmse:,.0f}\")\n",
    "    print(f\"  Test MAE:   ${test_mae:,.0f}\")\n",
    "    print(f\"  Test R²:    {test_r2:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part6'></a>\n",
    "# Part 6: Model Evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## 6.1 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Train RMSE': [r['train_rmse'] for r in results.values()],\n",
    "    'Test RMSE': [r['test_rmse'] for r in results.values()],\n",
    "    'Test MAE': [r['test_mae'] for r in results.values()],\n",
    "    'R²': [r['test_r2'] for r in results.values()]\n",
    "}).sort_values('Test RMSE').reset_index(drop=True)\n",
    "\n",
    "comparison['Rank'] = range(1, len(comparison) + 1)\n",
    "comparison = comparison[['Rank', 'Model', 'Train RMSE', 'Test RMSE', 'Test MAE', 'R²']]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL COMPARISON (sorted by Test RMSE)\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sort by test RMSE\n",
    "sorted_results = dict(sorted(results.items(), key=lambda x: x[1]['test_rmse']))\n",
    "names = list(sorted_results.keys())\n",
    "rmses = [sorted_results[n]['test_rmse'] for n in names]\n",
    "r2s = [sorted_results[n]['test_r2'] for n in names]\n",
    "\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.9, 0.3, len(names)))  # Green = better (lower RMSE)\n",
    "\n",
    "# RMSE\n",
    "bars1 = axes[0].barh(names[::-1], rmses[::-1], color=colors[::-1], edgecolor='black')\n",
    "axes[0].set_xlabel('Test RMSE ($)')\n",
    "axes[0].set_title('Model Comparison: RMSE (Lower is Better)', fontweight='bold')\n",
    "for bar, rmse in zip(bars1, rmses[::-1]):\n",
    "    axes[0].text(bar.get_width() + 1000, bar.get_y() + bar.get_height()/2,\n",
    "                 f'${rmse:,.0f}', va='center', fontweight='bold')\n",
    "\n",
    "# R²\n",
    "colors_r2 = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(names)))  # Green = better (higher R²)\n",
    "bars2 = axes[1].barh(names[::-1], r2s[::-1], color=colors_r2[::-1], edgecolor='black')\n",
    "axes[1].set_xlabel('R² Score')\n",
    "axes[1].set_xlim(0, 1)\n",
    "axes[1].set_title('Model Comparison: R² (Higher is Better)', fontweight='bold')\n",
    "for bar, r2 in zip(bars2, r2s[::-1]):\n",
    "    axes[1].text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                 f'{r2:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_model = names[0]\n",
    "print(f\"\\nBest Model: {best_model}\")\n",
    "print(f\"  Test RMSE: ${sorted_results[best_model]['test_rmse']:,.0f}\")\n",
    "print(f\"  Test R²: {sorted_results[best_model]['test_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Fold Cross-Validation\n",
    "print(\"=\" * 70)\n",
    "print(\"5-FOLD CROSS-VALIDATION (using negative RMSE)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Cross-validation with negative MSE (sklearn convention)\n",
    "    scores = cross_val_score(model, X_train_processed, y_train, \n",
    "                             cv=5, scoring='neg_root_mean_squared_error')\n",
    "    rmse_scores = -scores  # Convert to positive RMSE\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'mean': rmse_scores.mean(),\n",
    "        'std': rmse_scores.std(),\n",
    "        'scores': rmse_scores\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  RMSE scores: {rmse_scores.round(0)}\")\n",
    "    print(f\"  Mean RMSE: ${rmse_scores.mean():,.0f} (+/- ${rmse_scores.std()*2:,.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV results\n",
    "cv_sorted = dict(sorted(cv_results.items(), key=lambda x: x[1]['mean']))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "names_cv = list(cv_sorted.keys())\n",
    "means = [cv_sorted[n]['mean'] for n in names_cv]\n",
    "stds = [cv_sorted[n]['std'] for n in names_cv]\n",
    "\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.9, 0.3, len(names_cv)))\n",
    "bars = ax.barh(names_cv[::-1], means[::-1], xerr=stds[::-1],\n",
    "               color=colors[::-1], edgecolor='black', capsize=5)\n",
    "\n",
    "ax.set_xlabel('Cross-Validation RMSE ($)')\n",
    "ax.set_title('5-Fold Cross-Validation Results', fontweight='bold', fontsize=14)\n",
    "\n",
    "for bar, mean, std in zip(bars, means[::-1], stds[::-1]):\n",
    "    ax.text(bar.get_width() + std + 1000, bar.get_y() + bar.get_height()/2,\n",
    "            f'${mean:,.0f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Actual vs Predicted Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted for top models\n",
    "top_models = list(sorted_results.keys())[:4]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, name in enumerate(top_models):\n",
    "    y_pred = results[name]['predictions']\n",
    "    \n",
    "    axes[i].scatter(y_test, y_pred, alpha=0.3, s=10)\n",
    "    axes[i].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[i].set_xlabel('Actual House Value ($)')\n",
    "    axes[i].set_ylabel('Predicted House Value ($)')\n",
    "    axes[i].set_title(f'{name}\\nR² = {results[name][\"test_r2\"]:.4f}', fontweight='bold')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Points closer to the red diagonal line = better predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots for best model\n",
    "best_model_name = list(sorted_results.keys())[0]\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "residuals = y_test - best_predictions\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[0].scatter(best_predictions, residuals, alpha=0.3, s=10)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Predicted House Value ($)')\n",
    "axes[0].set_ylabel('Residuals ($)')\n",
    "axes[0].set_title('Residuals vs Predicted', fontweight='bold')\n",
    "\n",
    "# Residual distribution\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residuals ($)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Residual Distribution', fontweight='bold')\n",
    "\n",
    "# Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title('Q-Q Plot (Normality Check)', fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'Residual Analysis: {best_model_name}', fontweight='bold', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResidual Statistics for {best_model_name}:\")\n",
    "print(f\"  Mean: ${residuals.mean():,.0f}\")\n",
    "print(f\"  Std: ${residuals.std():,.0f}\")\n",
    "print(f\"  Min: ${residuals.min():,.0f}\")\n",
    "print(f\"  Max: ${residuals.max():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part7'></a>\n",
    "# Part 7: Feature Importance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "rf_model = results['Random Forest']['model']\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE IMPORTANCE (Random Forest)\")\n",
    "print(\"=\" * 60)\n",
    "print(feature_importance.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 15 features\n",
    "top_n = 15\n",
    "top_features = feature_importance.head(top_n)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, top_n))\n",
    "bars = ax.barh(top_features['Feature'][::-1], \n",
    "               top_features['Importance'][::-1] * 100,\n",
    "               color=colors[::-1], edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Importance (%)', fontsize=12)\n",
    "ax.set_title('Top 15 Features for House Price Prediction', fontweight='bold', fontsize=14)\n",
    "\n",
    "for bar, imp in zip(bars, top_features['Importance'][::-1]):\n",
    "    ax.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height()/2,\n",
    "            f'{imp*100:.1f}%', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: Median income is by far the most important predictor!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression coefficients\n",
    "lr_model = results['Linear Regression']['model']\n",
    "\n",
    "lr_importance = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Coefficient': lr_model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "# Top 15\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "top_lr = lr_importance.head(15)\n",
    "colors = ['green' if c > 0 else 'red' for c in top_lr['Coefficient']]\n",
    "\n",
    "ax.barh(top_lr['Feature'][::-1], top_lr['Coefficient'][::-1], \n",
    "        color=colors[::-1], edgecolor='black', alpha=0.7)\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_xlabel('Coefficient Value')\n",
    "ax.set_title('Linear Regression Coefficients (Standardized)', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Green = Positive effect on price\")\n",
    "print(\"Red = Negative effect on price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part8'></a>\n",
    "# Part 8: Summary and Conclusions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary dashboard\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Model Comparison\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "models_sorted = list(sorted_results.keys())\n",
    "r2_sorted = [sorted_results[m]['test_r2'] for m in models_sorted]\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(models_sorted)))\n",
    "bars = ax1.bar(models_sorted, r2_sorted, color=colors, edgecolor='black')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_ylabel('R² Score')\n",
    "ax1.set_title('Model R² Comparison', fontweight='bold', fontsize=14)\n",
    "for bar, r2 in zip(bars, r2_sorted):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{r2:.3f}', ha='center', fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=15)\n",
    "\n",
    "# 2. Feature Importance\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "top_feat = feature_importance.head(8)\n",
    "ax2.barh(top_feat['Feature'][::-1], \n",
    "         top_feat['Importance'][::-1] * 100,\n",
    "         color=plt.cm.viridis(np.linspace(0.3, 0.9, 8))[::-1], edgecolor='black')\n",
    "ax2.set_xlabel('Importance (%)')\n",
    "ax2.set_title('Top Features', fontweight='bold')\n",
    "\n",
    "# 3. Best Model Predictions\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax3.scatter(y_test, best_predictions, alpha=0.3, s=10)\n",
    "ax3.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
    "ax3.set_xlabel('Actual ($)')\n",
    "ax3.set_ylabel('Predicted ($)')\n",
    "ax3.set_title(f'Best Model: {best_model_name}', fontweight='bold')\n",
    "\n",
    "# 4. Target Distribution\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "ax4.hist(y, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax4.axvline(y.mean(), color='red', linestyle='--', label=f'Mean: ${y.mean():,.0f}')\n",
    "ax4.set_xlabel('House Value ($)')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.set_title('Target Distribution', fontweight='bold')\n",
    "ax4.legend()\n",
    "\n",
    "plt.suptitle('CALIFORNIA HOUSING PREDICTION - SUMMARY DASHBOARD', \n",
    "             fontweight='bold', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### 1. Model Performance\n",
    "- **Best model**: Random Forest / Gradient Boosting\n",
    "- **R² scores**: 0.80-0.82 (explains ~80% of variance)\n",
    "- **RMSE**: ~$50,000-60,000 prediction error\n",
    "\n",
    "### 2. Most Important Features\n",
    "| Feature | Importance |\n",
    "|---------|------------|\n",
    "| **median_income** | Highest - strong predictor |\n",
    "| **Location (lat/long)** | Geographic factors matter |\n",
    "| **ocean_proximity** | Coastal = more expensive |\n",
    "| **housing_median_age** | Older areas have different values |\n",
    "\n",
    "### 3. Key Insights\n",
    "- **Income is king**: Median income is the strongest predictor\n",
    "- **Location matters**: Coastal areas command premiums\n",
    "- **Ensemble methods win**: Random Forest and Gradient Boosting outperform linear models\n",
    "- **Feature engineering helps**: Derived features like rooms_per_household add value\n",
    "\n",
    "---\n",
    "\n",
    "## Regression Checklist\n",
    "\n",
    "- [x] Load and explore data\n",
    "- [x] Handle missing values\n",
    "- [x] Visualize distributions and correlations\n",
    "- [x] Engineer new features\n",
    "- [x] Encode categorical variables\n",
    "- [x] Scale features\n",
    "- [x] Train multiple models\n",
    "- [x] Evaluate with RMSE, MAE, R²\n",
    "- [x] Cross-validate\n",
    "- [x] Analyze residuals\n",
    "- [x] Interpret feature importance\n",
    "\n",
    "---\n",
    "\n",
    "**End of California Housing Prediction Tutorial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"CALIFORNIA HOUSING PREDICTION - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nDATASET\")\n",
    "print(f\"   Samples: {len(data):,}\")\n",
    "print(f\"   Features: {len(feature_cols)}\")\n",
    "print(f\"   Target: median_house_value\")\n",
    "print(f\"   Mean house value: ${y.mean():,.0f}\")\n",
    "\n",
    "print(f\"\\nBEST MODEL\")\n",
    "print(f\"   Name: {best_model_name}\")\n",
    "print(f\"   Test RMSE: ${results[best_model_name]['test_rmse']:,.0f}\")\n",
    "print(f\"   Test MAE: ${results[best_model_name]['test_mae']:,.0f}\")\n",
    "print(f\"   Test R²: {results[best_model_name]['test_r2']:.4f}\")\n",
    "\n",
    "print(f\"\\nALL MODEL R² SCORES\")\n",
    "for name in sorted_results:\n",
    "    print(f\"   {name}: {sorted_results[name]['test_r2']:.3f}\")\n",
    "\n",
    "print(f\"\\nTOP 5 FEATURES\")\n",
    "for _, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"   {row['Feature']}: {row['Importance']*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREDICTION COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
