{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 18: Reinforcement Learning Game\n",
    "\n",
    "**Create an AI that learns and masters simple games**\n",
    "\n",
    "In this tutorial, we'll build AI agents that learn to play games through trial and error - the essence of Reinforcement Learning (RL).\n",
    "\n",
    "**What is Reinforcement Learning?**\n",
    "\n",
    "RL is a machine learning paradigm where an **agent** learns to make decisions by interacting with an **environment** to maximize cumulative **rewards**.\n",
    "\n",
    "```\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │                                         │\n",
    "    │    State (S_t)          Reward (R_t)    │\n",
    "    │        │                    ▲           │\n",
    "    │        ▼                    │           │\n",
    "    │    ┌───────┐          ┌─────────────┐   │\n",
    "    │    │ AGENT │──Action──│ ENVIRONMENT │   │\n",
    "    │    └───────┘   (A_t)  └─────────────┘   │\n",
    "    │                                         │\n",
    "    └─────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Algorithms we'll implement:**\n",
    "1. Q-Learning (tabular)\n",
    "2. Deep Q-Network (DQN)\n",
    "3. Policy Gradient (REINFORCE)\n",
    "4. DQN for Atari-style games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Installation](#1-setup-and-installation)\n",
    "2. [RL Fundamentals](#2-rl-fundamentals)\n",
    "3. [GridWorld Environment](#3-gridworld-environment)\n",
    "4. [Q-Learning (Tabular)](#4-q-learning-tabular)\n",
    "5. [Deep Q-Network (DQN)](#5-deep-q-network-dqn)\n",
    "6. [CartPole with DQN](#6-cartpole-with-dqn)\n",
    "7. [Policy Gradient (REINFORCE)](#7-policy-gradient-reinforce)\n",
    "8. [Pong-Style Game with DQN](#8-pong-style-game-with-dqn)\n",
    "9. [Training Visualization](#9-training-visualization)\n",
    "10. [Summary](#10-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q gymnasium torch numpy matplotlib\n",
    "!pip install -q gymnasium[classic_control]  # For CartPole, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from typing import List, Tuple, Optional\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# RL Environment\n",
    "import gymnasium as gym\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Gymnasium version: {gym.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RL Fundamentals\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description | Example (Pong) |\n",
    "|---------|-------------|----------------|\n",
    "| **State (S)** | Current situation | Ball position, paddle position |\n",
    "| **Action (A)** | What agent can do | Move up, down, stay |\n",
    "| **Reward (R)** | Feedback signal | +1 for scoring, -1 for losing |\n",
    "| **Policy (π)** | Strategy for choosing actions | Neural network |\n",
    "| **Value (V)** | Expected future rewards | How good is this state? |\n",
    "| **Q-Value (Q)** | Value of action in state | How good is this action here? |\n",
    "\n",
    "### The RL Loop\n",
    "\n",
    "```python\n",
    "state = env.reset()\n",
    "while not done:\n",
    "    action = agent.select_action(state)  # Policy\n",
    "    next_state, reward, done = env.step(action)\n",
    "    agent.learn(state, action, reward, next_state)  # Update\n",
    "    state = next_state\n",
    "```\n",
    "\n",
    "### Exploration vs Exploitation\n",
    "\n",
    "- **Exploration**: Try new actions to discover better strategies\n",
    "- **Exploitation**: Use known good actions to maximize reward\n",
    "- **ε-greedy**: With probability ε explore, otherwise exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize exploration vs exploitation tradeoff\n",
    "def epsilon_decay_schedule(episodes, epsilon_start=1.0, epsilon_end=0.01, decay_rate=0.995):\n",
    "    \"\"\"Calculate epsilon values over episodes.\"\"\"\n",
    "    epsilons = []\n",
    "    epsilon = epsilon_start\n",
    "    for _ in range(episodes):\n",
    "        epsilons.append(epsilon)\n",
    "        epsilon = max(epsilon_end, epsilon * decay_rate)\n",
    "    return epsilons\n",
    "\n",
    "# Plot different decay schedules\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "episodes = 500\n",
    "for decay in [0.99, 0.995, 0.999]:\n",
    "    epsilons = epsilon_decay_schedule(episodes, decay_rate=decay)\n",
    "    ax.plot(epsilons, label=f'Decay = {decay}')\n",
    "\n",
    "ax.axhline(y=0.1, color='r', linestyle='--', label='Common minimum')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Epsilon (Exploration Rate)')\n",
    "ax.set_title('Epsilon Decay Schedules: Exploration → Exploitation')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Early episodes: High ε = More exploration (random actions)\")\n",
    "print(\"Later episodes: Low ε = More exploitation (learned actions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GridWorld Environment\n",
    "\n",
    "Let's start with a simple custom environment: GridWorld.\n",
    "\n",
    "```\n",
    "┌───┬───┬───┬───┐\n",
    "│ S │   │   │ G │  S = Start, G = Goal (+1)\n",
    "├───┼───┼───┼───┤\n",
    "│   │ X │   │   │  X = Obstacle (-1)\n",
    "├───┼───┼───┼───┤\n",
    "│   │   │   │   │\n",
    "└───┴───┴───┴───┘\n",
    "```\n",
    "\n",
    "- **State**: (row, col) position\n",
    "- **Actions**: Up, Down, Left, Right\n",
    "- **Rewards**: +1 at goal, -1 at obstacle, -0.01 per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Simple GridWorld environment for learning RL basics.\n",
    "    \n",
    "    The agent starts at position (0,0) and must reach the goal\n",
    "    while avoiding obstacles.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Actions\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "    \n",
    "    ACTION_NAMES = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "    \n",
    "    def __init__(self, rows=4, cols=4):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.n_states = rows * cols\n",
    "        self.n_actions = 4\n",
    "        \n",
    "        # Define special cells\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (0, cols - 1)\n",
    "        self.obstacles = [(1, 1)]  # Obstacles\n",
    "        \n",
    "        # Current position\n",
    "        self.agent_pos = self.start\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state.\"\"\"\n",
    "        self.agent_pos = self.start\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Convert (row, col) to state index.\"\"\"\n",
    "        return self.agent_pos[0] * self.cols + self.agent_pos[1]\n",
    "    \n",
    "    def _pos_from_state(self, state):\n",
    "        \"\"\"Convert state index to (row, col).\"\"\"\n",
    "        return (state // self.cols, state % self.cols)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take action and return (next_state, reward, done, info).\n",
    "        \"\"\"\n",
    "        row, col = self.agent_pos\n",
    "        \n",
    "        # Calculate new position\n",
    "        if action == self.UP:\n",
    "            row = max(0, row - 1)\n",
    "        elif action == self.DOWN:\n",
    "            row = min(self.rows - 1, row + 1)\n",
    "        elif action == self.LEFT:\n",
    "            col = max(0, col - 1)\n",
    "        elif action == self.RIGHT:\n",
    "            col = min(self.cols - 1, col + 1)\n",
    "        \n",
    "        self.agent_pos = (row, col)\n",
    "        \n",
    "        # Calculate reward\n",
    "        if self.agent_pos == self.goal:\n",
    "            reward = 1.0\n",
    "            done = True\n",
    "        elif self.agent_pos in self.obstacles:\n",
    "            reward = -1.0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -0.01  # Small penalty for each step\n",
    "            done = False\n",
    "        \n",
    "        return self._get_state(), reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Visualize the grid.\"\"\"\n",
    "        grid = [['.' for _ in range(self.cols)] for _ in range(self.rows)]\n",
    "        \n",
    "        # Mark special cells\n",
    "        grid[self.goal[0]][self.goal[1]] = 'G'\n",
    "        for obs in self.obstacles:\n",
    "            grid[obs[0]][obs[1]] = 'X'\n",
    "        grid[self.agent_pos[0]][self.agent_pos[1]] = 'A'\n",
    "        \n",
    "        # Print grid\n",
    "        print('┌' + '───┬' * (self.cols - 1) + '───┐')\n",
    "        for i, row in enumerate(grid):\n",
    "            print('│ ' + ' │ '.join(row) + ' │')\n",
    "            if i < self.rows - 1:\n",
    "                print('├' + '───┼' * (self.cols - 1) + '───┤')\n",
    "        print('└' + '───┴' * (self.cols - 1) + '───┘')\n",
    "        print(f\"Agent: {self.agent_pos}, Goal: {self.goal}\")\n",
    "\n",
    "# Test the environment\n",
    "env = GridWorld()\n",
    "state = env.reset()\n",
    "\n",
    "print(\"GridWorld Environment\")\n",
    "print(\"=\" * 30)\n",
    "print(\"A = Agent, G = Goal, X = Obstacle\")\n",
    "print()\n",
    "env.render()\n",
    "\n",
    "# Take some random steps\n",
    "print(\"\\nRandom exploration:\")\n",
    "for i in range(3):\n",
    "    action = random.randint(0, 3)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    print(f\"Action: {env.ACTION_NAMES[action]:5} → State: {next_state}, Reward: {reward:.2f}\")\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Q-Learning (Tabular)\n",
    "\n",
    "Q-Learning is a model-free RL algorithm that learns the value of actions directly.\n",
    "\n",
    "### Q-Value Update Rule (Bellman Equation)\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = learning rate\n",
    "- $\\gamma$ = discount factor (importance of future rewards)\n",
    "- $r$ = immediate reward\n",
    "- $s'$ = next state\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "```\n",
    "Initialize Q-table with zeros\n",
    "For each episode:\n",
    "    state = env.reset()\n",
    "    While not done:\n",
    "        action = ε-greedy(Q, state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        Q[state, action] += α * (reward + γ * max(Q[next_state]) - Q[state, action])\n",
    "        state = next_state\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Tabular Q-Learning agent.\n",
    "    \n",
    "    Learns Q-values for each (state, action) pair using the\n",
    "    Bellman equation update rule.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, \n",
    "                 learning_rate=0.1, \n",
    "                 discount_factor=0.99,\n",
    "                 epsilon_start=1.0,\n",
    "                 epsilon_end=0.01,\n",
    "                 epsilon_decay=0.995):\n",
    "        \"\"\"\n",
    "        Initialize Q-Learning agent.\n",
    "        \n",
    "        Args:\n",
    "            n_states: Number of states in environment\n",
    "            n_actions: Number of possible actions\n",
    "            learning_rate: How much to update Q-values (α)\n",
    "            discount_factor: Importance of future rewards (γ)\n",
    "            epsilon_*: Exploration parameters\n",
    "        \"\"\"\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # Initialize Q-table with zeros\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select action using ε-greedy policy.\n",
    "        \n",
    "        With probability ε: random action (explore)\n",
    "        Otherwise: best action from Q-table (exploit)\n",
    "        \"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update Q-value using Bellman equation.\n",
    "        \n",
    "        Q(s,a) ← Q(s,a) + α[r + γ·max(Q(s')) - Q(s,a)]\n",
    "        \"\"\"\n",
    "        # Current Q-value\n",
    "        current_q = self.q_table[state, action]\n",
    "        \n",
    "        # Target Q-value\n",
    "        if done:\n",
    "            target_q = reward  # No future rewards if done\n",
    "        else:\n",
    "            target_q = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        \n",
    "        # Update Q-value\n",
    "        self.q_table[state, action] += self.lr * (target_q - current_q)\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def get_policy(self):\n",
    "        \"\"\"Get optimal action for each state.\"\"\"\n",
    "        return np.argmax(self.q_table, axis=1)\n",
    "\n",
    "\n",
    "def train_q_learning(env, agent, n_episodes=500, max_steps=100):\n",
    "    \"\"\"\n",
    "    Train Q-Learning agent on environment.\n",
    "    \n",
    "    Returns:\n",
    "        list: Episode rewards\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select and perform action\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Learn from experience\n",
    "            agent.learn(state, action, reward, next_state, done)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay exploration\n",
    "        agent.decay_epsilon()\n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.3f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "# Train Q-Learning agent on GridWorld\n",
    "print(\"Training Q-Learning Agent on GridWorld\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "env = GridWorld()\n",
    "agent = QLearningAgent(\n",
    "    n_states=env.n_states,\n",
    "    n_actions=env.n_actions,\n",
    "    learning_rate=0.1,\n",
    "    discount_factor=0.99\n",
    ")\n",
    "\n",
    "rewards = train_q_learning(env, agent, n_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot rewards\n",
    "ax1 = axes[0]\n",
    "ax1.plot(rewards, alpha=0.3, label='Episode Reward')\n",
    "# Moving average\n",
    "window = 50\n",
    "moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(range(window-1, len(rewards)), moving_avg, label=f'{window}-Episode Moving Avg')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('Q-Learning Training Progress')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Visualize Q-table as heatmap\n",
    "ax2 = axes[1]\n",
    "q_max = np.max(agent.q_table, axis=1).reshape(env.rows, env.cols)\n",
    "im = ax2.imshow(q_max, cmap='YlOrRd')\n",
    "ax2.set_title('Learned Value Function (Max Q per State)')\n",
    "ax2.set_xlabel('Column')\n",
    "ax2.set_ylabel('Row')\n",
    "plt.colorbar(im, ax=ax2, label='Max Q-Value')\n",
    "\n",
    "# Annotate with values\n",
    "for i in range(env.rows):\n",
    "    for j in range(env.cols):\n",
    "        ax2.text(j, i, f'{q_max[i, j]:.2f}', ha='center', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned policy\n",
    "print(\"Learned Policy (Best Action per State):\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "policy = agent.get_policy()\n",
    "action_arrows = ['↑', '↓', '←', '→']\n",
    "\n",
    "print('┌' + '───┬' * (env.cols - 1) + '───┐')\n",
    "for i in range(env.rows):\n",
    "    row_str = '│'\n",
    "    for j in range(env.cols):\n",
    "        state = i * env.cols + j\n",
    "        if (i, j) == env.goal:\n",
    "            cell = ' G '\n",
    "        elif (i, j) in env.obstacles:\n",
    "            cell = ' X '\n",
    "        else:\n",
    "            cell = f' {action_arrows[policy[state]]} '\n",
    "        row_str += cell + '│'\n",
    "    print(row_str)\n",
    "    if i < env.rows - 1:\n",
    "        print('├' + '───┼' * (env.cols - 1) + '───┤')\n",
    "print('└' + '───┴' * (env.cols - 1) + '───┘')\n",
    "\n",
    "print(\"\\n↑=Up, ↓=Down, ←=Left, →=Right\")\n",
    "print(\"G=Goal, X=Obstacle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the learned policy\n",
    "print(\"\\nTesting Learned Policy:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "env = GridWorld()\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "path = [env.agent_pos]\n",
    "\n",
    "print(\"Starting position:\")\n",
    "env.render()\n",
    "\n",
    "for step in range(20):\n",
    "    # Use learned policy (no exploration)\n",
    "    action = np.argmax(agent.q_table[state])\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    path.append(env.agent_pos)\n",
    "    \n",
    "    print(f\"\\nStep {step + 1}: {env.ACTION_NAMES[action]}\")\n",
    "    env.render()\n",
    "    \n",
    "    if done:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Episode finished! Total reward: {total_reward:.2f}\")\n",
    "        print(f\"Path: {' → '.join([str(p) for p in path])}\")\n",
    "        break\n",
    "    \n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deep Q-Network (DQN)\n",
    "\n",
    "For environments with large or continuous state spaces, we can't use a table. Instead, we use a **neural network** to approximate Q-values.\n",
    "\n",
    "### Key DQN Innovations\n",
    "\n",
    "1. **Experience Replay**: Store transitions and sample randomly to break correlation\n",
    "2. **Target Network**: Separate network for stable Q-targets, updated periodically\n",
    "3. **Neural Network**: Approximate Q(s, a) instead of table lookup\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "State → [Neural Network] → Q-values for all actions\n",
    "         (FC layers)       [Q(s,a1), Q(s,a2), ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay buffer\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Experience Replay Buffer.\n",
    "    \n",
    "    Stores transitions and samples random batches for training.\n",
    "    This breaks correlation between consecutive samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add transition to buffer.\"\"\"\n",
    "        self.buffer.append(Transition(state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample random batch of transitions.\"\"\"\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        return Transition(*zip(*transitions))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network.\n",
    "    \n",
    "    Takes state as input and outputs Q-values for all actions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    DQN Agent with experience replay and target network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim,\n",
    "                 learning_rate=1e-3,\n",
    "                 discount_factor=0.99,\n",
    "                 epsilon_start=1.0,\n",
    "                 epsilon_end=0.01,\n",
    "                 epsilon_decay=0.995,\n",
    "                 buffer_size=10000,\n",
    "                 batch_size=64,\n",
    "                 target_update_freq=10):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = discount_factor\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        # Exploration\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # Networks\n",
    "        self.policy_net = DQN(state_dim, action_dim).to(device)\n",
    "        self.target_net = DQN(state_dim, action_dim).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()  # Target network is not trained directly\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Training counter\n",
    "        self.train_step = 0\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using ε-greedy policy.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in replay buffer.\"\"\"\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"Update network using batch from replay buffer.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(np.array(batch.state)).to(device)\n",
    "        actions = torch.LongTensor(batch.action).unsqueeze(1).to(device)\n",
    "        rewards = torch.FloatTensor(batch.reward).unsqueeze(1).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(batch.next_state)).to(device)\n",
    "        dones = torch.FloatTensor(batch.done).unsqueeze(1).to(device)\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q = self.policy_net(states).gather(1, actions)\n",
    "        \n",
    "        # Target Q-values (from target network)\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        self.train_step += 1\n",
    "        if self.train_step % self.target_update_freq == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "print(\"DQN Agent implemented!\")\n",
    "print(f\"Components:\")\n",
    "print(f\"  - Policy Network: Learns Q-values\")\n",
    "print(f\"  - Target Network: Stable Q-targets\")\n",
    "print(f\"  - Replay Buffer: Breaks correlation\")\n",
    "print(f\"  - ε-greedy: Exploration strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CartPole with DQN\n",
    "\n",
    "CartPole is a classic RL benchmark:\n",
    "- **Goal**: Balance a pole on a moving cart\n",
    "- **State**: Cart position, cart velocity, pole angle, pole velocity (4 values)\n",
    "- **Actions**: Push cart left or right (2 actions)\n",
    "- **Reward**: +1 for each timestep the pole is balanced\n",
    "- **Success**: Average reward ≥ 195 over 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_cartpole(n_episodes=500, max_steps=500, render_freq=None):\n",
    "    \"\"\"\n",
    "    Train DQN agent on CartPole environment.\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]  # 4\n",
    "    action_dim = env.action_space.n  # 2\n",
    "    \n",
    "    print(f\"CartPole Environment:\")\n",
    "    print(f\"  State dimension: {state_dim}\")\n",
    "    print(f\"  Action dimension: {action_dim}\")\n",
    "    print(f\"  Goal: Keep pole balanced (reward ≥ 195)\")\n",
    "    print()\n",
    "    \n",
    "    # Create agent\n",
    "    agent = DQNAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        learning_rate=1e-3,\n",
    "        discount_factor=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "        buffer_size=10000,\n",
    "        batch_size=64,\n",
    "        target_update_freq=10\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_loss = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Learn\n",
    "            loss = agent.learn()\n",
    "            if loss is not None:\n",
    "                episode_loss.append(loss)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Record metrics\n",
    "        episode_rewards.append(total_reward)\n",
    "        if episode_loss:\n",
    "            losses.append(np.mean(episode_loss))\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.1f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "            \n",
    "            # Check if solved\n",
    "            if avg_reward >= 195:\n",
    "                print(f\"\\n*** Environment SOLVED in {episode + 1} episodes! ***\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, episode_rewards, losses\n",
    "\n",
    "# Train the agent\n",
    "print(\"Training DQN on CartPole\")\n",
    "print(\"=\" * 50)\n",
    "agent_cartpole, rewards_cartpole, losses_cartpole = train_dqn_cartpole(n_episodes=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CartPole training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Rewards\n",
    "ax1 = axes[0]\n",
    "ax1.plot(rewards_cartpole, alpha=0.3, label='Episode Reward')\n",
    "window = 50\n",
    "if len(rewards_cartpole) >= window:\n",
    "    moving_avg = np.convolve(rewards_cartpole, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(rewards_cartpole)), moving_avg, \n",
    "             label=f'{window}-Episode Moving Avg', linewidth=2)\n",
    "ax1.axhline(y=195, color='g', linestyle='--', label='Solved Threshold (195)')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('DQN CartPole Training - Rewards')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "ax2 = axes[1]\n",
    "if losses_cartpole:\n",
    "    ax2.plot(losses_cartpole, alpha=0.5)\n",
    "    if len(losses_cartpole) >= window:\n",
    "        loss_avg = np.convolve(losses_cartpole, np.ones(window)/window, mode='valid')\n",
    "        ax2.plot(range(window-1, len(losses_cartpole)), loss_avg, linewidth=2)\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('DQN CartPole Training - Loss')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(f\"\\nFinal Statistics:\")\n",
    "print(f\"  Last 100 episodes avg reward: {np.mean(rewards_cartpole[-100:]):.1f}\")\n",
    "print(f\"  Best episode reward: {max(rewards_cartpole)}\")\n",
    "print(f\"  Episodes to reach 195 avg: \", end=\"\")\n",
    "for i in range(100, len(rewards_cartpole)):\n",
    "    if np.mean(rewards_cartpole[i-100:i]) >= 195:\n",
    "        print(f\"{i}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Not reached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Policy Gradient (REINFORCE)\n",
    "\n",
    "Instead of learning Q-values, **Policy Gradient** methods directly learn the policy (probability of actions).\n",
    "\n",
    "### REINFORCE Algorithm\n",
    "\n",
    "The policy gradient theorem:\n",
    "\n",
    "$$\\nabla J(\\theta) = \\mathbb{E}\\left[ \\sum_t \\nabla \\log \\pi_\\theta(a_t|s_t) \\cdot G_t \\right]$$\n",
    "\n",
    "Where $G_t$ is the return (cumulative discounted reward) from time $t$.\n",
    "\n",
    "### Key Differences from DQN\n",
    "\n",
    "| Aspect | DQN | Policy Gradient |\n",
    "|--------|-----|----------------|\n",
    "| Output | Q-values | Action probabilities |\n",
    "| Action selection | argmax Q | Sample from distribution |\n",
    "| Update | After each step | After episode |\n",
    "| Exploration | ε-greedy | Built-in (stochastic policy) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy Network for REINFORCE.\n",
    "    \n",
    "    Outputs action probabilities (softmax).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)  # Output probabilities\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class REINFORCEAgent:\n",
    "    \"\"\"\n",
    "    REINFORCE (Monte Carlo Policy Gradient) Agent.\n",
    "    \n",
    "    Learns policy directly by optimizing expected return.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, learning_rate=1e-3, discount_factor=0.99):\n",
    "        self.gamma = discount_factor\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Policy network\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Episode memory\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select action by sampling from policy distribution.\n",
    "        \"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.policy(state_tensor)\n",
    "        \n",
    "        # Sample action from distribution\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # Store log probability for learning\n",
    "        self.log_probs.append(dist.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"Store reward for current step.\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Update policy using REINFORCE algorithm.\n",
    "        \n",
    "        Called at the end of each episode.\n",
    "        \"\"\"\n",
    "        if not self.rewards:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate returns (discounted cumulative rewards)\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for reward in reversed(self.rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        # Normalize returns for stability\n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Calculate policy gradient loss\n",
    "        policy_loss = []\n",
    "        for log_prob, G in zip(self.log_probs, returns):\n",
    "            # Negative because we want to maximize\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        \n",
    "        # Update policy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.stack(policy_loss).sum()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear episode memory\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "def train_reinforce_cartpole(n_episodes=1000, max_steps=500):\n",
    "    \"\"\"\n",
    "    Train REINFORCE agent on CartPole.\n",
    "    \"\"\"\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    print(f\"Training REINFORCE on CartPole\")\n",
    "    print(f\"=\" * 50)\n",
    "    \n",
    "    agent = REINFORCEAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        learning_rate=1e-3,\n",
    "        discount_factor=0.99\n",
    "    )\n",
    "    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.store_reward(reward)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Learn at end of episode\n",
    "        agent.learn()\n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes} | Avg Reward: {avg_reward:.1f}\")\n",
    "            \n",
    "            if avg_reward >= 195:\n",
    "                print(f\"\\n*** Environment SOLVED! ***\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, episode_rewards\n",
    "\n",
    "# Train REINFORCE agent\n",
    "agent_reinforce, rewards_reinforce = train_reinforce_cartpole(n_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare DQN vs REINFORCE\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "window = 50\n",
    "\n",
    "# DQN\n",
    "if len(rewards_cartpole) >= window:\n",
    "    dqn_avg = np.convolve(rewards_cartpole, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards_cartpole)), dqn_avg, \n",
    "            label='DQN', linewidth=2, color='blue')\n",
    "\n",
    "# REINFORCE\n",
    "if len(rewards_reinforce) >= window:\n",
    "    reinforce_avg = np.convolve(rewards_reinforce, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards_reinforce)), reinforce_avg, \n",
    "            label='REINFORCE', linewidth=2, color='orange')\n",
    "\n",
    "ax.axhline(y=195, color='g', linestyle='--', label='Solved (195)')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Average Reward (50 episodes)')\n",
    "ax.set_title('DQN vs REINFORCE on CartPole')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"  DQN final avg: {np.mean(rewards_cartpole[-100:]):.1f}\")\n",
    "print(f\"  REINFORCE final avg: {np.mean(rewards_reinforce[-100:]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pong-Style Game with DQN\n",
    "\n",
    "Now let's create a simplified Pong-like game and train a DQN agent to play it!\n",
    "\n",
    "### SimplePong Environment\n",
    "- **State**: Ball position (x, y), ball velocity (vx, vy), paddle position\n",
    "- **Actions**: Stay, Move Up, Move Down\n",
    "- **Reward**: +1 for hitting ball, -1 for missing, small negative for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePong:\n",
    "    \"\"\"\n",
    "    Simplified Pong environment.\n",
    "    \n",
    "    Single player: keep the ball in play by moving paddle.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, width=20, height=10, paddle_size=3):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.paddle_size = paddle_size\n",
    "        \n",
    "        # State dimensions: ball_x, ball_y, ball_vx, ball_vy, paddle_y (normalized)\n",
    "        self.state_dim = 5\n",
    "        self.action_dim = 3  # Stay, Up, Down\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset game to initial state.\"\"\"\n",
    "        # Ball starts in center, moving toward paddle\n",
    "        self.ball_x = self.width // 2\n",
    "        self.ball_y = self.height // 2\n",
    "        self.ball_vx = -1  # Moving toward paddle (left side)\n",
    "        self.ball_vy = random.choice([-1, 0, 1])\n",
    "        \n",
    "        # Paddle on left side\n",
    "        self.paddle_y = self.height // 2 - self.paddle_size // 2\n",
    "        \n",
    "        self.score = 0\n",
    "        self.steps = 0\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Return normalized state.\"\"\"\n",
    "        return np.array([\n",
    "            self.ball_x / self.width,\n",
    "            self.ball_y / self.height,\n",
    "            self.ball_vx / 2,  # Velocity normalized\n",
    "            self.ball_vy / 2,\n",
    "            self.paddle_y / self.height\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take action and update game state.\n",
    "        \n",
    "        Actions: 0=Stay, 1=Up, 2=Down\n",
    "        \"\"\"\n",
    "        self.steps += 1\n",
    "        reward = -0.01  # Small penalty per step\n",
    "        done = False\n",
    "        \n",
    "        # Move paddle\n",
    "        if action == 1:  # Up\n",
    "            self.paddle_y = max(0, self.paddle_y - 1)\n",
    "        elif action == 2:  # Down\n",
    "            self.paddle_y = min(self.height - self.paddle_size, self.paddle_y + 1)\n",
    "        \n",
    "        # Move ball\n",
    "        self.ball_x += self.ball_vx\n",
    "        self.ball_y += self.ball_vy\n",
    "        \n",
    "        # Ball bounces off top/bottom walls\n",
    "        if self.ball_y <= 0 or self.ball_y >= self.height - 1:\n",
    "            self.ball_vy = -self.ball_vy\n",
    "            self.ball_y = max(0, min(self.height - 1, self.ball_y))\n",
    "        \n",
    "        # Ball bounces off right wall\n",
    "        if self.ball_x >= self.width - 1:\n",
    "            self.ball_vx = -self.ball_vx\n",
    "            self.ball_x = self.width - 1\n",
    "        \n",
    "        # Check if ball reaches left side (paddle side)\n",
    "        if self.ball_x <= 1:\n",
    "            # Check if paddle hits ball\n",
    "            paddle_top = self.paddle_y\n",
    "            paddle_bottom = self.paddle_y + self.paddle_size\n",
    "            \n",
    "            if paddle_top <= self.ball_y <= paddle_bottom:\n",
    "                # Hit! Bounce back\n",
    "                self.ball_vx = -self.ball_vx\n",
    "                self.ball_x = 2\n",
    "                # Add some randomness to vertical velocity\n",
    "                self.ball_vy = random.choice([-1, 0, 1])\n",
    "                reward = 1.0\n",
    "                self.score += 1\n",
    "            else:\n",
    "                # Miss!\n",
    "                reward = -1.0\n",
    "                done = True\n",
    "        \n",
    "        # Max steps limit\n",
    "        if self.steps >= 500:\n",
    "            done = True\n",
    "        \n",
    "        return self._get_state(), reward, done, {'score': self.score}\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Text-based rendering of game.\"\"\"\n",
    "        grid = [['.' for _ in range(self.width)] for _ in range(self.height)]\n",
    "        \n",
    "        # Draw paddle\n",
    "        for i in range(self.paddle_size):\n",
    "            y = self.paddle_y + i\n",
    "            if 0 <= y < self.height:\n",
    "                grid[y][0] = '|'\n",
    "        \n",
    "        # Draw ball\n",
    "        ball_y = int(max(0, min(self.height - 1, self.ball_y)))\n",
    "        ball_x = int(max(0, min(self.width - 1, self.ball_x)))\n",
    "        grid[ball_y][ball_x] = 'O'\n",
    "        \n",
    "        # Print\n",
    "        print('╔' + '═' * self.width + '╗')\n",
    "        for row in grid:\n",
    "            print('║' + ''.join(row) + '║')\n",
    "        print('╚' + '═' * self.width + '╝')\n",
    "        print(f\"Score: {self.score} | Steps: {self.steps}\")\n",
    "\n",
    "# Test Pong environment\n",
    "print(\"SimplePong Environment\")\n",
    "print(\"=\" * 30)\n",
    "pong = SimplePong()\n",
    "state = pong.reset()\n",
    "print(f\"State shape: {state.shape}\")\n",
    "print(f\"State: {state}\")\n",
    "print()\n",
    "pong.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_pong(n_episodes=1000):\n",
    "    \"\"\"\n",
    "    Train DQN agent on SimplePong.\n",
    "    \"\"\"\n",
    "    env = SimplePong()\n",
    "    \n",
    "    print(\"Training DQN on SimplePong\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    agent = DQNAgent(\n",
    "        state_dim=env.state_dim,\n",
    "        action_dim=env.action_dim,\n",
    "        learning_rate=1e-3,\n",
    "        discount_factor=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.05,\n",
    "        epsilon_decay=0.998,\n",
    "        buffer_size=50000,\n",
    "        batch_size=64,\n",
    "        target_update_freq=20\n",
    "    )\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_scores = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            agent.learn()\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_scores.append(info['score'])\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            avg_score = np.mean(episode_scores[-100:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                  f\"Avg Score: {avg_score:.1f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    return agent, episode_rewards, episode_scores\n",
    "\n",
    "# Train Pong agent\n",
    "agent_pong, rewards_pong, scores_pong = train_dqn_pong(n_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Pong training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "window = 50\n",
    "\n",
    "# Rewards\n",
    "ax1 = axes[0]\n",
    "ax1.plot(rewards_pong, alpha=0.3)\n",
    "if len(rewards_pong) >= window:\n",
    "    avg = np.convolve(rewards_pong, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(rewards_pong)), avg, linewidth=2)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('SimplePong Training - Rewards')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Scores (ball hits)\n",
    "ax2 = axes[1]\n",
    "ax2.plot(scores_pong, alpha=0.3)\n",
    "if len(scores_pong) >= window:\n",
    "    avg = np.convolve(scores_pong, np.ones(window)/window, mode='valid')\n",
    "    ax2.plot(range(window-1, len(scores_pong)), avg, linewidth=2, color='green')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Score (Ball Hits)')\n",
    "ax2.set_title('SimplePong Training - Score')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Statistics:\")\n",
    "print(f\"  Avg Score (last 100): {np.mean(scores_pong[-100:]):.1f}\")\n",
    "print(f\"  Best Score: {max(scores_pong)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch trained agent play Pong\n",
    "print(\"Trained Agent Playing SimplePong\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "env = SimplePong()\n",
    "state = env.reset()\n",
    "\n",
    "# Disable exploration for testing\n",
    "agent_pong.epsilon = 0\n",
    "\n",
    "frames = []\n",
    "while True:\n",
    "    # Select best action\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        q_values = agent_pong.policy_net(state_tensor)\n",
    "        action = q_values.argmax().item()\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Store frame data\n",
    "    frames.append({\n",
    "        'ball': (env.ball_x, env.ball_y),\n",
    "        'paddle': env.paddle_y,\n",
    "        'score': info['score'],\n",
    "        'action': ['Stay', 'Up', 'Down'][action]\n",
    "    })\n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# Show last few frames\n",
    "print(f\"\\nGame ended with score: {info['score']}\")\n",
    "print(f\"Total steps: {len(frames)}\")\n",
    "\n",
    "# Render final state\n",
    "print(\"\\nFinal game state:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Visualization\n",
    "\n",
    "Let's create comprehensive visualizations of the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive training comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "window = 50\n",
    "\n",
    "# 1. GridWorld Q-Learning\n",
    "ax = axes[0, 0]\n",
    "ax.plot(rewards, alpha=0.3, color='blue')\n",
    "if len(rewards) >= window:\n",
    "    avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards)), avg, linewidth=2, color='blue')\n",
    "ax.set_title('1. Q-Learning on GridWorld')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. CartPole DQN\n",
    "ax = axes[0, 1]\n",
    "ax.plot(rewards_cartpole, alpha=0.3, color='green')\n",
    "if len(rewards_cartpole) >= window:\n",
    "    avg = np.convolve(rewards_cartpole, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards_cartpole)), avg, linewidth=2, color='green')\n",
    "ax.axhline(y=195, color='r', linestyle='--', alpha=0.7)\n",
    "ax.set_title('2. DQN on CartPole')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. CartPole REINFORCE\n",
    "ax = axes[1, 0]\n",
    "ax.plot(rewards_reinforce, alpha=0.3, color='orange')\n",
    "if len(rewards_reinforce) >= window:\n",
    "    avg = np.convolve(rewards_reinforce, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards_reinforce)), avg, linewidth=2, color='orange')\n",
    "ax.axhline(y=195, color='r', linestyle='--', alpha=0.7)\n",
    "ax.set_title('3. REINFORCE on CartPole')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. SimplePong DQN\n",
    "ax = axes[1, 1]\n",
    "ax.plot(scores_pong, alpha=0.3, color='purple')\n",
    "if len(scores_pong) >= window:\n",
    "    avg = np.convolve(scores_pong, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(scores_pong)), avg, linewidth=2, color='purple')\n",
    "ax.set_title('4. DQN on SimplePong (Score)')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Score (Ball Hits)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Algorithms Implemented\n",
    "\n",
    "| Algorithm | Type | Key Idea | Best For |\n",
    "|-----------|------|----------|----------|\n",
    "| **Q-Learning** | Value-based | Learn Q(s,a) table | Small, discrete states |\n",
    "| **DQN** | Value-based | Neural network Q-function | Large/continuous states |\n",
    "| **REINFORCE** | Policy-based | Direct policy optimization | Continuous actions |\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "1. **RL Framework**: Agent, Environment, State, Action, Reward\n",
    "2. **Exploration vs Exploitation**: ε-greedy strategy\n",
    "3. **Value Functions**: Q(s,a) estimates expected return\n",
    "4. **Experience Replay**: Breaks correlation, improves sample efficiency\n",
    "5. **Target Networks**: Stabilizes training in deep RL\n",
    "6. **Policy Gradients**: Directly optimize policy parameters\n",
    "\n",
    "### Environments Conquered\n",
    "\n",
    "1. **GridWorld**: Simple navigation (Q-Learning)\n",
    "2. **CartPole**: Classic control (DQN, REINFORCE)\n",
    "3. **SimplePong**: Game playing (DQN)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Double DQN**: Reduce overestimation\n",
    "- **Dueling DQN**: Separate value and advantage\n",
    "- **PPO**: More stable policy gradients\n",
    "- **A3C**: Parallel training\n",
    "- **Atari Games**: Full pixel-based learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary of trained agents\n",
    "print(\"=\"*60)\n",
    "print(\"REINFORCEMENT LEARNING GAME - FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "Algorithms Implemented:\n",
    "───────────────────────\n",
    "1. Q-Learning (Tabular)\n",
    "   - Bellman equation: Q(s,a) ← Q(s,a) + α[r + γ·max(Q(s')) - Q(s,a)]\n",
    "   - Works for small, discrete state spaces\n",
    "   \n",
    "2. Deep Q-Network (DQN)\n",
    "   - Neural network approximates Q-function\n",
    "   - Experience replay for stable learning\n",
    "   - Target network for stable Q-targets\n",
    "   \n",
    "3. REINFORCE (Policy Gradient)\n",
    "   - Directly learns policy π(a|s)\n",
    "   - Updates using: ∇J = E[∇log π(a|s) · G]\n",
    "   - Monte Carlo returns for credit assignment\n",
    "\n",
    "Environments Trained:\n",
    "────────────────────\n",
    "\"\"\")\n",
    "\n",
    "print(f\"1. GridWorld (Q-Learning)\")\n",
    "print(f\"   Final avg reward: {np.mean(rewards[-100:]):.3f}\")\n",
    "\n",
    "print(f\"\\n2. CartPole (DQN)\")\n",
    "print(f\"   Final avg reward: {np.mean(rewards_cartpole[-100:]):.1f}\")\n",
    "print(f\"   Solved: {'Yes' if np.mean(rewards_cartpole[-100:]) >= 195 else 'Getting there'}\")\n",
    "\n",
    "print(f\"\\n3. CartPole (REINFORCE)\")\n",
    "print(f\"   Final avg reward: {np.mean(rewards_reinforce[-100:]):.1f}\")\n",
    "\n",
    "print(f\"\\n4. SimplePong (DQN)\")\n",
    "print(f\"   Final avg score: {np.mean(scores_pong[-100:]):.1f} ball hits\")\n",
    "print(f\"   Best score: {max(scores_pong)} ball hits\")\n",
    "\n",
    "print(\"\"\"\n",
    "Key Takeaways:\n",
    "─────────────\n",
    "• RL learns through trial and error (no labeled data needed)\n",
    "• Exploration is crucial early, exploitation later\n",
    "• Experience replay breaks correlation in training\n",
    "• Neural networks enable RL on complex state spaces\n",
    "• Different algorithms suit different problems\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
