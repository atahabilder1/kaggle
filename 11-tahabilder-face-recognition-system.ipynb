{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition System - Complete Tutorial\n",
    "\n",
    "**Author:** Anik Tahabilder  \n",
    "**Project:** 11 of 22 - Kaggle ML Portfolio  \n",
    "**Dataset:** LFW (Labeled Faces in the Wild)  \n",
    "**Difficulty:** 8/10 | **Learning Value:** 9/10\n",
    "\n",
    "---\n",
    "\n",
    "## What Will You Learn?\n",
    "\n",
    "This tutorial teaches **Face Recognition from fundamentals to implementation**.\n",
    "\n",
    "| Topic | What You'll Understand |\n",
    "|-------|------------------------|\n",
    "| **Face Detection vs Recognition** | Two different problems! |\n",
    "| **Detection Algorithms** | Haar Cascades, HOG+SVM, CNN-based |\n",
    "| **Recognition Algorithms** | Eigenfaces, LBPH, Deep Learning embeddings |\n",
    "| **Face Embeddings** | How faces become 128-dimensional vectors |\n",
    "| **Similarity Metrics** | Euclidean distance, cosine similarity |\n",
    "| **Parameter Tuning** | How to optimize each algorithm |\n",
    "| **Complete Pipeline** | Detection → Alignment → Encoding → Recognition |\n",
    "\n",
    "---\n",
    "\n",
    "## The Two Problems in Face Recognition\n",
    "\n",
    "```\n",
    "FACE DETECTION                      FACE RECOGNITION\n",
    "\"Where are the faces?\"              \"Who is this person?\"\n",
    "\n",
    "┌─────────────────┐                 ┌─────────────────┐\n",
    "│  Input Image    │                 │  Detected Face  │\n",
    "│  ┌───┐  ┌───┐   │                 │     ┌───┐       │\n",
    "│  │ ? │  │ ? │   │    ────────>    │     │???│       │    ────────>   \"John\"\n",
    "│  └───┘  └───┘   │                 │     └───┘       │\n",
    "└─────────────────┘                 └─────────────────┘\n",
    "Output: Bounding boxes              Output: Person identity\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: Understanding Face Detection vs Recognition](#part1)\n",
    "2. [Part 2: Face Detection Algorithms](#part2)\n",
    "3. [Part 3: Face Recognition Algorithms](#part3)\n",
    "4. [Part 4: Face Embeddings & Similarity](#part4)\n",
    "5. [Part 5: Dataset Loading & Preprocessing](#part5)\n",
    "6. [Part 6: Face Detection Implementation](#part6)\n",
    "7. [Part 7: Face Recognition Implementation](#part7)\n",
    "8. [Part 8: Complete Recognition Pipeline](#part8)\n",
    "9. [Part 9: Evaluation & Results](#part9)\n",
    "10. [Part 10: Summary & Key Takeaways](#part10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part1'></a>\n",
    "# Part 1: Understanding Face Detection vs Recognition\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 The Two Separate Problems\n",
    "\n",
    "| Aspect | Face Detection | Face Recognition |\n",
    "|--------|---------------|------------------|\n",
    "| **Question** | \"Where are faces in this image?\" | \"Who is this person?\" |\n",
    "| **Input** | Any image | Cropped face image |\n",
    "| **Output** | Bounding box coordinates | Person identity/name |\n",
    "| **Type** | Object Detection | Classification/Verification |\n",
    "| **Difficulty** | Easier | Harder |\n",
    "\n",
    "## 1.2 Face Recognition Sub-Tasks\n",
    "\n",
    "| Task | Description | Example |\n",
    "|------|-------------|--------|\n",
    "| **Verification (1:1)** | \"Is this person X?\" | Phone unlock, passport control |\n",
    "| **Identification (1:N)** | \"Who is this person?\" | Finding person in database |\n",
    "| **Clustering** | \"Group similar faces\" | Photo organization |\n",
    "\n",
    "## 1.3 The Complete Pipeline\n",
    "\n",
    "```\n",
    "┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐\n",
    "│  Input   │───>│  Face    │───>│  Face    │───>│  Face    │───>│  Match   │\n",
    "│  Image   │    │Detection │    │Alignment │    │ Encoding │    │ /Identify│\n",
    "└──────────┘    └──────────┘    └──────────┘    └──────────┘    └──────────┘\n",
    "                     │               │               │               │\n",
    "                     v               v               v               v\n",
    "               Bounding box    Normalized face   128-D vector    \"Person X\"\n",
    "```\n",
    "\n",
    "## 1.4 Why Is Face Recognition Hard?\n",
    "\n",
    "| Challenge | Description |\n",
    "|-----------|-------------|\n",
    "| **Pose Variation** | Same person looks different from different angles |\n",
    "| **Illumination** | Lighting changes appearance dramatically |\n",
    "| **Expression** | Smiling vs neutral vs surprised |\n",
    "| **Occlusion** | Glasses, masks, hair covering face |\n",
    "| **Age** | People change over time |\n",
    "| **Image Quality** | Blur, low resolution, compression |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP AND IMPORTS\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FACE RECOGNITION SYSTEM - TUTORIAL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"OpenCV: {cv2.__version__}\")\n",
    "print(\"\\nAll libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part2'></a>\n",
    "# Part 2: Face Detection Algorithms\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Overview of Detection Methods\n",
    "\n",
    "| Algorithm | Year | Type | Speed | Accuracy | Best For |\n",
    "|-----------|------|------|-------|----------|----------|\n",
    "| **Haar Cascades** | 2001 | Classical CV | Very Fast | Medium | Real-time, embedded |\n",
    "| **HOG + SVM** | 2005 | Classical ML | Fast | Good | Balanced performance |\n",
    "| **MTCNN** | 2016 | Deep Learning | Medium | Very Good | Accuracy-critical |\n",
    "| **RetinaFace** | 2019 | Deep Learning | Slow | Excellent | State-of-the-art |\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Haar Cascade Classifier\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. **Haar Features**: Simple rectangular features that capture patterns\n",
    "2. **Integral Image**: Fast computation of feature values\n",
    "3. **AdaBoost**: Selects best features and creates weak classifiers\n",
    "4. **Cascade**: Chain of classifiers that quickly reject non-faces\n",
    "\n",
    "```\n",
    "Haar Features:\n",
    "┌───┬───┐   ┌───────┐   ┌───┬───┐\n",
    "│ W │ B │   │   W   │   │ W │ B │\n",
    "├───┼───┤   ├───────┤   │   │   │\n",
    "│ B │ W │   │   B   │   │ B │ W │\n",
    "└───┴───┘   └───────┘   └───┴───┘\n",
    "Edge feat.  Line feat.  Four-rect.\n",
    "\n",
    "Value = Σ(white pixels) - Σ(black pixels)\n",
    "```\n",
    "\n",
    "### Key Parameters:\n",
    "\n",
    "| Parameter | Description | Default | Tuning Tip |\n",
    "|-----------|-------------|---------|------------|\n",
    "| `scaleFactor` | Image size reduction per scale | 1.1 | Lower = more accurate, slower |\n",
    "| `minNeighbors` | Min detections to keep | 3 | Higher = fewer false positives |\n",
    "| `minSize` | Minimum face size | (30,30) | Set based on expected face size |\n",
    "| `maxSize` | Maximum face size | None | Limit for efficiency |\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3 HOG + SVM (Histogram of Oriented Gradients)\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. **Compute Gradients**: Find edge directions in image\n",
    "2. **Create Histograms**: Count gradient orientations in cells\n",
    "3. **Normalize Blocks**: Improve invariance to lighting\n",
    "4. **SVM Classifier**: Trained to separate faces vs non-faces\n",
    "\n",
    "```\n",
    "Original     Gradient      HOG Cells     Concatenated\n",
    " Image      Directions                    Feature Vector\n",
    "┌─────┐      ┌─────┐      ┌─┬─┬─┐       ┌─────────────┐\n",
    "│     │  ->  │↗↘↙↖│  ->  │█│▄│░│  ->   │ 0.1 0.3 ... │\n",
    "│  :) │      │↗↘↙↖│      │▄│█│▄│       │    3780-D   │\n",
    "└─────┘      └─────┘      └─┴─┴─┘       └─────────────┘\n",
    "```\n",
    "\n",
    "### Advantages over Haar:\n",
    "\n",
    "| Aspect | Haar | HOG |\n",
    "|--------|------|-----|\n",
    "| Feature type | Simple rectangles | Gradient histograms |\n",
    "| Rotation | Sensitive | More robust |\n",
    "| Lighting | Sensitive | More robust |\n",
    "| Speed | Faster | Slightly slower |\n",
    "\n",
    "---\n",
    "\n",
    "## 2.4 CNN-Based Detection (MTCNN)\n",
    "\n",
    "### Architecture: Three-Stage Cascade\n",
    "\n",
    "```\n",
    "Input Image\n",
    "     │\n",
    "     v\n",
    "┌─────────┐     ┌─────────┐     ┌─────────┐\n",
    "│ P-Net   │────>│ R-Net   │────>│ O-Net   │\n",
    "│(Proposal)│     │(Refine) │     │(Output) │\n",
    "└─────────┘     └─────────┘     └─────────┘\n",
    "     │               │               │\n",
    "     v               v               v\n",
    " Candidate       Filtered        Final boxes\n",
    "  boxes          boxes          + landmarks\n",
    "```\n",
    "\n",
    "| Stage | Purpose | Output |\n",
    "|-------|---------|--------|\n",
    "| **P-Net** | Fast scanning, propose candidates | Many rough boxes |\n",
    "| **R-Net** | Refine candidates, remove false positives | Fewer, better boxes |\n",
    "| **O-Net** | Final refinement + facial landmarks | Precise boxes + 5 landmarks |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FACE DETECTION ALGORITHMS COMPARISON\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"FACE DETECTION ALGORITHMS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Detection Algorithm Summary Table\n",
    "detection_methods = pd.DataFrame({\n",
    "    'Algorithm': ['Haar Cascades', 'HOG + SVM', 'MTCNN', 'RetinaFace'],\n",
    "    'Type': ['Classical CV', 'Classical ML', 'Deep Learning', 'Deep Learning'],\n",
    "    'Year': [2001, 2005, 2016, 2019],\n",
    "    'Speed': ['Very Fast', 'Fast', 'Medium', 'Slow'],\n",
    "    'Accuracy': ['Medium', 'Good', 'Very Good', 'Excellent'],\n",
    "    'Key_Feature': ['Haar features + AdaBoost', 'Gradient histograms + SVM', \n",
    "                    'Three-stage CNN cascade', 'Single-stage detector + FPN'],\n",
    "    'Best_For': ['Real-time/Embedded', 'Balanced', 'High accuracy', 'State-of-the-art']\n",
    "})\n",
    "\n",
    "print(\"\\nFace Detection Methods Comparison:\")\n",
    "print(detection_methods.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part3'></a>\n",
    "# Part 3: Face Recognition Algorithms\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Overview of Recognition Methods\n",
    "\n",
    "| Algorithm | Year | Type | Training Data | Accuracy | Interpretable |\n",
    "|-----------|------|------|---------------|----------|---------------|\n",
    "| **Eigenfaces (PCA)** | 1991 | Statistical | Medium | Low-Medium | Yes |\n",
    "| **Fisherfaces (LDA)** | 1997 | Statistical | Medium | Medium | Yes |\n",
    "| **LBPH** | 2006 | Texture-based | Low | Medium | Somewhat |\n",
    "| **Deep Learning** | 2014+ | Neural Network | Very High | Very High | No |\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 Eigenfaces (PCA-based)\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. **Flatten faces**: Convert each face image to a vector\n",
    "2. **Compute mean face**: Average of all training faces\n",
    "3. **PCA**: Find principal components (eigenfaces)\n",
    "4. **Project**: Represent each face as combination of eigenfaces\n",
    "5. **Classify**: Compare projections using distance metric\n",
    "\n",
    "```\n",
    "Training Faces         Eigenfaces              Recognition\n",
    "┌───┐ ┌───┐ ┌───┐      ┌───┐ ┌───┐ ┌───┐      New Face -> Project -> Compare\n",
    "│ A │ │ B │ │ C │  ->  │EF1│ │EF2│ │EF3│  ->  [0.3, 0.7, 0.1] ≈ Person B\n",
    "└───┘ └───┘ └───┘      └───┘ └───┘ └───┘\n",
    "```\n",
    "\n",
    "### Mathematical Formulation:\n",
    "\n",
    "| Step | Formula | Description |\n",
    "|------|---------|-------------|\n",
    "| Mean face | $\\mu = \\frac{1}{N}\\sum_{i=1}^{N} x_i$ | Average of all faces |\n",
    "| Centered | $\\Phi_i = x_i - \\mu$ | Subtract mean |\n",
    "| Covariance | $C = \\frac{1}{N}\\sum \\Phi_i \\Phi_i^T$ | Compute covariance |\n",
    "| Eigenfaces | $C \\cdot v_k = \\lambda_k \\cdot v_k$ | Top eigenvectors |\n",
    "| Projection | $\\omega = U^T (x - \\mu)$ | Face in eigenface space |\n",
    "\n",
    "### Key Parameters:\n",
    "\n",
    "| Parameter | Description | Tuning Tip |\n",
    "|-----------|-------------|------------|\n",
    "| `n_components` | Number of eigenfaces to keep | 50-150 typically works well |\n",
    "| `whiten` | Normalize variance | Often improves results |\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 LBPH (Local Binary Pattern Histograms)\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. **Divide face into regions** (e.g., 8x8 grid)\n",
    "2. **For each pixel**: Compare with neighbors, create binary pattern\n",
    "3. **Create histogram**: Count pattern occurrences per region\n",
    "4. **Concatenate**: Combine all region histograms\n",
    "5. **Compare**: Use histogram distance for matching\n",
    "\n",
    "```\n",
    "LBP Computation:\n",
    "                    Binary Pattern\n",
    "┌───┬───┬───┐      ┌───┬───┬───┐\n",
    "│ 6 │ 5 │ 2 │      │ 1 │ 1 │ 0 │    Pattern: 11010011\n",
    "├───┼───┼───┤  ->  ├───┼───┼───┤    Decimal: 211\n",
    "│ 7 │[4]│ 1 │      │ 1 │   │ 0 │\n",
    "├───┼───┼───┤      ├───┼───┼───┤    (Compare each neighbor\n",
    "│ 8 │ 3 │ 9 │      │ 1 │ 0 │ 1 │     with center value 4)\n",
    "└───┴───┴───┘      └───┴───┴───┘\n",
    "```\n",
    "\n",
    "### Key Parameters:\n",
    "\n",
    "| Parameter | Description | Default | Tuning Tip |\n",
    "|-----------|-------------|---------|------------|\n",
    "| `radius` | Radius of circular pattern | 1 | Larger = captures larger features |\n",
    "| `neighbors` | Number of sampling points | 8 | 8 or 16 typical |\n",
    "| `grid_x, grid_y` | Number of cells | 8x8 | More cells = more detail |\n",
    "\n",
    "### Advantages of LBPH:\n",
    "\n",
    "| Advantage | Description |\n",
    "|-----------|-------------|\n",
    "| **Illumination invariant** | Compares relative values, not absolute |\n",
    "| **Computationally simple** | No complex training |\n",
    "| **Works with few samples** | Can work with single training image |\n",
    "| **Real-time capable** | Fast enough for live recognition |\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Deep Learning Embeddings\n",
    "\n",
    "### How Modern Face Recognition Works:\n",
    "\n",
    "```\n",
    "Face Image        CNN Backbone         Embedding          Comparison\n",
    "┌─────────┐      ┌───────────┐      ┌─────────────┐      ┌─────────┐\n",
    "│         │      │           │      │             │      │         │\n",
    "│   :)    │ ---> │  ResNet   │ ---> │ 128-D vector│ ---> │ Distance│\n",
    "│         │      │  VGGFace  │      │ [0.1, 0.3...│      │  < 0.6  │\n",
    "└─────────┘      └───────────┘      └─────────────┘      └─────────┘\n",
    "                                                          Same person!\n",
    "```\n",
    "\n",
    "### Popular Deep Learning Models:\n",
    "\n",
    "| Model | Architecture | Embedding Size | Training Data | LFW Accuracy |\n",
    "|-------|--------------|----------------|---------------|---------------|\n",
    "| **FaceNet** | Inception | 128-D | 200M faces | 99.63% |\n",
    "| **VGGFace** | VGG-16 | 4096-D | 2.6M faces | 98.95% |\n",
    "| **ArcFace** | ResNet | 512-D | 5.8M faces | 99.83% |\n",
    "| **dlib** | ResNet | 128-D | 3M faces | 99.38% |\n",
    "\n",
    "### Training Objectives:\n",
    "\n",
    "| Loss Function | Description |\n",
    "|---------------|-------------|\n",
    "| **Triplet Loss** | Push same-person embeddings together, different apart |\n",
    "| **Contrastive Loss** | Similar to triplet, uses pairs |\n",
    "| **ArcFace/CosFace** | Angular margin for better separation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FACE RECOGNITION ALGORITHMS COMPARISON\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"FACE RECOGNITION ALGORITHMS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "recognition_methods = pd.DataFrame({\n",
    "    'Algorithm': ['Eigenfaces (PCA)', 'Fisherfaces (LDA)', 'LBPH', 'Deep Learning'],\n",
    "    'Year': [1991, 1997, 2006, '2014+'],\n",
    "    'Type': ['Statistical', 'Statistical', 'Texture-based', 'Neural Network'],\n",
    "    'How_It_Works': [\n",
    "        'PCA dimensionality reduction',\n",
    "        'LDA maximizes class separation',\n",
    "        'Local binary patterns + histograms',\n",
    "        'CNN extracts 128-D embeddings'\n",
    "    ],\n",
    "    'Training_Needed': ['Medium', 'Medium', 'Low', 'Very High (pre-trained)'],\n",
    "    'Accuracy': ['Low-Medium', 'Medium', 'Medium', 'Very High'],\n",
    "    'Pros': [\n",
    "        'Simple, interpretable',\n",
    "        'Better class separation',\n",
    "        'Works with few samples, fast',\n",
    "        'State-of-the-art accuracy'\n",
    "    ],\n",
    "    'Cons': [\n",
    "        'Sensitive to lighting/pose',\n",
    "        'Needs multiple samples per class',\n",
    "        'Not as accurate as DL',\n",
    "        'Needs large training data'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nFace Recognition Methods:\")\n",
    "for _, row in recognition_methods.iterrows():\n",
    "    print(f\"\\n{row['Algorithm']} ({row['Year']})\")\n",
    "    print(f\"  Type: {row['Type']}\")\n",
    "    print(f\"  How: {row['How_It_Works']}\")\n",
    "    print(f\"  Accuracy: {row['Accuracy']}\")\n",
    "    print(f\"  Pros: {row['Pros']}\")\n",
    "    print(f\"  Cons: {row['Cons']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part4'></a>\n",
    "# Part 4: Face Embeddings & Similarity Metrics\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 What is a Face Embedding?\n",
    "\n",
    "A face embedding is a **compact numerical representation** of a face.\n",
    "\n",
    "```\n",
    "Face Image (150x150x3)          Face Embedding (128-D)\n",
    "     67,500 values                  128 values\n",
    "┌─────────────────┐             ┌─────────────────────┐\n",
    "│                 │    CNN      │                     │\n",
    "│      :)         │  ────────>  │ [0.12, -0.34, 0.56, │\n",
    "│                 │             │  0.23, -0.11, ...]  │\n",
    "└─────────────────┘             └─────────────────────┘\n",
    "```\n",
    "\n",
    "### Properties of Good Embeddings:\n",
    "\n",
    "| Property | Description |\n",
    "|----------|-------------|\n",
    "| **Compact** | Low-dimensional (typically 128-512) |\n",
    "| **Discriminative** | Different people have different embeddings |\n",
    "| **Robust** | Same person has similar embeddings despite variations |\n",
    "| **Normalized** | Often unit length (L2 normalized) |\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2 Similarity Metrics\n",
    "\n",
    "To compare two face embeddings, we use **distance metrics**:\n",
    "\n",
    "### Euclidean Distance:\n",
    "\n",
    "$$d(a, b) = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}$$\n",
    "\n",
    "| Distance | Interpretation |\n",
    "|----------|----------------|\n",
    "| < 0.6 | Same person (typical threshold) |\n",
    "| 0.6 - 1.0 | Uncertain |\n",
    "| > 1.0 | Different people |\n",
    "\n",
    "### Cosine Similarity:\n",
    "\n",
    "$$\\cos(a, b) = \\frac{a \\cdot b}{\\|a\\| \\|b\\|}$$\n",
    "\n",
    "| Similarity | Interpretation |\n",
    "|------------|----------------|\n",
    "| > 0.7 | Same person |\n",
    "| 0.5 - 0.7 | Uncertain |\n",
    "| < 0.5 | Different people |\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3 Threshold Selection\n",
    "\n",
    "The **threshold** determines who is considered a match:\n",
    "\n",
    "```\n",
    "                    False Reject Rate (FRR)\n",
    "                    ←───────────────────────\n",
    "                         ┌─────┐\n",
    "Same Person:        ████████   │\n",
    "                    ████████   │\n",
    "                         └─────┼─────┐\n",
    "Different Person:              │█████████████\n",
    "                               │█████████████\n",
    "                    ───────────┴─────────────>\n",
    "                           Threshold\n",
    "                    ───────────────────────>\n",
    "                    False Accept Rate (FAR)\n",
    "```\n",
    "\n",
    "| Threshold | FAR | FRR | Use Case |\n",
    "|-----------|-----|-----|----------|\n",
    "| **Strict (0.4)** | Very Low | High | High security |\n",
    "| **Balanced (0.6)** | Low | Medium | General use |\n",
    "| **Lenient (0.8)** | Medium | Low | Convenience |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SIMILARITY METRICS IMPLEMENTATION\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"SIMILARITY METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    \"\"\"Compute Euclidean distance between two vectors.\"\"\"\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# Example with synthetic embeddings\n",
    "print(\"\\nExample: Comparing Face Embeddings\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Simulate embeddings\n",
    "np.random.seed(42)\n",
    "person_a_face1 = np.random.randn(128) * 0.1 + np.array([1] * 64 + [0] * 64)  # Person A\n",
    "person_a_face2 = person_a_face1 + np.random.randn(128) * 0.05  # Same person, different photo\n",
    "person_b_face = np.random.randn(128) * 0.1 + np.array([0] * 64 + [1] * 64)   # Person B\n",
    "\n",
    "# Normalize (like real embeddings)\n",
    "person_a_face1 = person_a_face1 / np.linalg.norm(person_a_face1)\n",
    "person_a_face2 = person_a_face2 / np.linalg.norm(person_a_face2)\n",
    "person_b_face = person_b_face / np.linalg.norm(person_b_face)\n",
    "\n",
    "# Compare\n",
    "print(\"\\nComparisons:\")\n",
    "print(f\"  Person A (face1) vs Person A (face2):\")\n",
    "print(f\"    Euclidean: {euclidean_distance(person_a_face1, person_a_face2):.4f}\")\n",
    "print(f\"    Cosine:    {cosine_similarity(person_a_face1, person_a_face2):.4f}\")\n",
    "print(f\"    Result:    SAME PERSON (distance < 0.6)\")\n",
    "\n",
    "print(f\"\\n  Person A vs Person B:\")\n",
    "print(f\"    Euclidean: {euclidean_distance(person_a_face1, person_b_face):.4f}\")\n",
    "print(f\"    Cosine:    {cosine_similarity(person_a_face1, person_b_face):.4f}\")\n",
    "print(f\"    Result:    DIFFERENT PEOPLE (distance > 0.6)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n<a id='part5'></a>\n# Part 5: Dataset Loading & Preprocessing\n\n---\n\n## 5.1 LFW Dataset (Labeled Faces in the Wild)\n\n| Attribute | Value |\n|-----------|-------|\n| **Kaggle Dataset** | `lfwpeople` |\n| **Kaggle Path** | `/kaggle/input/lfwpeople/lfw_funneled` |\n| **Total Images** | 13,233 |\n| **People** | 5,749 |\n| **Image Size** | 250x250 (original) |\n| **Format** | JPEG |\n| **Challenge** | Unconstrained (real-world conditions) |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# LOAD LFW DATASET FROM KAGGLE\n# ============================================================\nprint(\"=\"*70)\nprint(\"LOADING LFW DATASET\")\nprint(\"=\"*70)\n\n# ============================================================\n# KAGGLE PATH CONFIGURATION\n# ============================================================\n# Dataset: https://www.kaggle.com/datasets/jessicali9530/lfw-dataset\n#\n# HOW TO ADD DATASET IN KAGGLE:\n# 1. Click \"Add data\" button (top right of notebook)\n# 2. Search for \"lfw\" or \"lfwpeople\"\n# 3. Click \"Add\" to attach the dataset\n\n# Check if running on Kaggle\nUSE_KAGGLE = os.path.exists('/kaggle/input')\n\nif USE_KAGGLE:\n    # Try multiple possible paths (dataset structure may vary)\n    POSSIBLE_PATHS = [\n        '/kaggle/input/lfwpeople/lfw_funneled',           # Primary path\n        '/kaggle/input/lfwpeople',                         # Alternative\n        '/kaggle/input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled',\n        '/kaggle/input/lfw-dataset/lfw-deepfunneled',\n        '/kaggle/input/lfw-dataset'\n    ]\n    DATASET_PATH = None\n    for path in POSSIBLE_PATHS:\n        if os.path.exists(path):\n            # Check if it has person subdirectories\n            subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n            if len(subdirs) > 10:  # LFW has many person folders\n                DATASET_PATH = path\n                break\n\n    if DATASET_PATH is None:\n        print(\"WARNING: LFW dataset not found!\")\n        print(\"Please add the dataset to your Kaggle notebook:\")\n        print(\"  1. Click 'Add data' button\")\n        print(\"  2. Search for 'lfwpeople' or 'lfw-dataset'\")\n        print(\"  3. Click 'Add' and re-run this cell\")\n        print(\"\\nFalling back to sklearn...\")\n        USE_KAGGLE = False\nelse:\n    DATASET_PATH = None\n\ndef load_lfw_from_kaggle(base_path, min_faces_per_person=20, image_size=(100, 100)):\n    \"\"\"\n    Load LFW dataset from Kaggle directory structure.\n\n    Parameters:\n    - base_path: Path to lfw folder\n    - min_faces_per_person: Minimum images per person to include\n    - image_size: Resize images to this size\n\n    Returns:\n    - images: numpy array of face images\n    - labels: numpy array of person names\n    - label_names: list of unique person names\n    \"\"\"\n    images = []\n    labels = []\n\n    # Get all person directories\n    person_dirs = [d for d in os.listdir(base_path)\n                   if os.path.isdir(os.path.join(base_path, d))]\n\n    print(f\"Found {len(person_dirs)} people in dataset\")\n\n    # Filter by minimum faces\n    valid_persons = []\n    for person in person_dirs:\n        person_path = os.path.join(base_path, person)\n        # Check for both .jpg and .png files\n        n_images = len([f for f in os.listdir(person_path) \n                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n        if n_images >= min_faces_per_person:\n            valid_persons.append((person, n_images))\n\n    print(f\"People with >= {min_faces_per_person} images: {len(valid_persons)}\")\n\n    # Load images\n    for person, _ in valid_persons:\n        person_path = os.path.join(base_path, person)\n        for img_file in os.listdir(person_path):\n            if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n                img_path = os.path.join(person_path, img_file)\n\n                # Load and preprocess\n                img = cv2.imread(img_path)\n                if img is not None:\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                    img = cv2.resize(img, image_size)\n                    images.append(img)\n                    labels.append(person)\n\n    images = np.array(images)\n    labels = np.array(labels)\n    label_names = list(set(labels))\n\n    return images, labels, label_names\n\n# Load dataset\nif USE_KAGGLE and DATASET_PATH:\n    try:\n        print(f\"\\nLoading from: {DATASET_PATH}\")\n        images, labels, label_names = load_lfw_from_kaggle(\n            DATASET_PATH,\n            min_faces_per_person=20,\n            image_size=(100, 100)\n        )\n        print(f\"\\nDataset loaded successfully!\")\n    except Exception as e:\n        print(f\"Error loading from Kaggle: {e}\")\n        print(\"Falling back to sklearn...\")\n        USE_KAGGLE = False\n\nif not USE_KAGGLE or DATASET_PATH is None:\n    # Import here so it's available when needed\n    from sklearn.datasets import fetch_lfw_people\n    print(\"\\nLoading from sklearn (this may take a moment)...\")\n    lfw = fetch_lfw_people(min_faces_per_person=20, resize=0.4)\n    images = lfw.images\n    labels = np.array([lfw.target_names[i] for i in lfw.target])\n    label_names = list(lfw.target_names)\n    # Convert grayscale to RGB-like\n    images = np.stack([images] * 3, axis=-1) if len(images.shape) == 3 else images\n\nprint(f\"\\n\" + \"=\"*50)\nprint(\"DATASET SUMMARY\")\nprint(\"=\"*50)\nprint(f\"Total images: {len(images)}\")\nprint(f\"Image shape: {images[0].shape}\")\nprint(f\"Number of people: {len(label_names)}\")\nprint(f\"\\nImages per person:\")\nlabel_counts = Counter(labels)\nfor person, count in sorted(label_counts.items(), key=lambda x: -x[1])[:10]:\n    print(f\"  {person}: {count} images\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample faces\n",
    "print(\"=\"*70)\n",
    "print(\"SAMPLE FACES FROM DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show samples from different people\n",
    "unique_labels = list(set(labels))\n",
    "n_show = min(10, len(unique_labels))\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 7))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < n_show:\n",
    "        person = unique_labels[i]\n",
    "        # Get first image of this person\n",
    "        idx = np.where(labels == person)[0][0]\n",
    "        \n",
    "        ax.imshow(images[idx])\n",
    "        ax.set_title(person.replace('_', ' '), fontsize=10, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample Faces from LFW Dataset', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PREPROCESS DATA\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "n_classes = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"\\nLabel encoding: {n_classes} classes\")\n",
    "\n",
    "# Normalize images to [0, 1]\n",
    "X = images.astype('float32') / 255.0\n",
    "\n",
    "# For classical methods, convert to grayscale and flatten\n",
    "X_gray = np.array([cv2.cvtColor((img * 255).astype('uint8'), cv2.COLOR_RGB2GRAY) \n",
    "                   for img in X])\n",
    "X_flat = X_gray.reshape(len(X_gray), -1)\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  Original (RGB):  {X.shape}\")\n",
    "print(f\"  Grayscale:       {X_gray.shape}\")\n",
    "print(f\"  Flattened:       {X_flat.shape}\")\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_flat, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Also split grayscale images (for LBPH)\n",
    "X_train_gray, X_test_gray, _, _ = train_test_split(\n",
    "    X_gray, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain/Test split:\")\n",
    "print(f\"  Train: {len(X_train)} samples\")\n",
    "print(f\"  Test:  {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part6'></a>\n",
    "# Part 6: Face Detection Implementation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HAAR CASCADE FACE DETECTION\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"HAAR CASCADE FACE DETECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load pre-trained Haar cascade\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "print(\"\"\"\n",
    "Haar Cascade Parameters:\n",
    "========================\n",
    "\n",
    "| Parameter      | Description                    | Our Value | Effect |\n",
    "|----------------|--------------------------------|-----------|--------|\n",
    "| scaleFactor    | Image pyramid scale            | 1.1       | Smaller = more accurate, slower |\n",
    "| minNeighbors   | Detections needed to confirm   | 5         | Higher = fewer false positives |\n",
    "| minSize        | Minimum face size              | (30, 30)  | Filter out small faces |\n",
    "\"\"\")\n",
    "\n",
    "def detect_faces_haar(image, scale_factor=1.1, min_neighbors=5, min_size=(30, 30)):\n",
    "    \"\"\"\n",
    "    Detect faces using Haar Cascade.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: Input image (RGB or grayscale)\n",
    "    - scale_factor: How much image size is reduced at each scale\n",
    "    - min_neighbors: Minimum number of neighbor rectangles to retain\n",
    "    - min_size: Minimum face size to detect\n",
    "    \n",
    "    Returns:\n",
    "    - faces: List of (x, y, w, h) bounding boxes\n",
    "    \"\"\"\n",
    "    # Convert to grayscale if needed\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=scale_factor,\n",
    "        minNeighbors=min_neighbors,\n",
    "        minSize=min_size\n",
    "    )\n",
    "    \n",
    "    return faces\n",
    "\n",
    "# Test on sample images\n",
    "print(\"\\nTesting face detection on sample images...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Convert normalized image back to uint8\n",
    "    img = (X[i] * 255).astype('uint8')\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = detect_faces_haar(img)\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    img_with_boxes = img.copy()\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img_with_boxes, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    ax.imshow(img_with_boxes)\n",
    "    ax.set_title(f'Detected: {len(faces)} face(s)', fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Haar Cascade Face Detection Results', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EFFECT OF HAAR CASCADE PARAMETERS\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"PARAMETER TUNING: EFFECT OF minNeighbors\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test different minNeighbors values\n",
    "test_img = (X[0] * 255).astype('uint8')\n",
    "min_neighbors_values = [1, 3, 5, 10]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for ax, min_n in zip(axes, min_neighbors_values):\n",
    "    faces = detect_faces_haar(test_img, min_neighbors=min_n)\n",
    "    \n",
    "    img_copy = test_img.copy()\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img_copy, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    ax.imshow(img_copy)\n",
    "    ax.set_title(f'minNeighbors={min_n}\\n({len(faces)} detections)', fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Effect of minNeighbors Parameter', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"  - Low minNeighbors: More detections, more false positives\")\n",
    "print(\"  - High minNeighbors: Fewer detections, may miss faces\")\n",
    "print(\"  - Recommended: 3-5 for balanced results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part7'></a>\n",
    "# Part 7: Face Recognition Implementation\n",
    "\n",
    "---\n",
    "\n",
    "## 7.1 Eigenfaces (PCA-based Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EIGENFACES IMPLEMENTATION\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"EIGENFACES (PCA-BASED) FACE RECOGNITION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "Eigenfaces Algorithm:\n",
    "=====================\n",
    "1. Flatten each face image into a vector\n",
    "2. Compute mean face (average of all faces)\n",
    "3. Apply PCA to find principal components (eigenfaces)\n",
    "4. Project faces into eigenface space\n",
    "5. Use classifier (KNN/SVM) on projections\n",
    "\n",
    "Key Parameter: n_components (number of eigenfaces)\n",
    "  - Too few: Loses important information\n",
    "  - Too many: Keeps noise, slower\n",
    "  - Typical: 50-150 components\n",
    "\"\"\")\n",
    "\n",
    "class EigenfaceRecognizer:\n",
    "    \"\"\"\n",
    "    Face recognition using Eigenfaces (PCA + Classifier).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=100, classifier='svm'):\n",
    "        \"\"\"\n",
    "        Initialize Eigenface recognizer.\n",
    "        \n",
    "        Parameters:\n",
    "        - n_components: Number of eigenfaces to keep\n",
    "        - classifier: 'svm' or 'knn'\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.pca = PCA(n_components=n_components, whiten=True)\n",
    "        \n",
    "        if classifier == 'svm':\n",
    "            self.classifier = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "        else:\n",
    "            self.classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "        \n",
    "        self.mean_face = None\n",
    "        self.eigenfaces = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the recognizer.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Training faces, shape (n_samples, n_pixels)\n",
    "        - y: Labels\n",
    "        \"\"\"\n",
    "        # Compute mean face\n",
    "        self.mean_face = np.mean(X, axis=0)\n",
    "        \n",
    "        # Fit PCA\n",
    "        X_pca = self.pca.fit_transform(X)\n",
    "        \n",
    "        # Store eigenfaces\n",
    "        self.eigenfaces = self.pca.components_\n",
    "        \n",
    "        # Train classifier\n",
    "        self.classifier.fit(X_pca, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict identities.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Test faces, shape (n_samples, n_pixels)\n",
    "        \n",
    "        Returns:\n",
    "        - Predicted labels\n",
    "        \"\"\"\n",
    "        X_pca = self.pca.transform(X)\n",
    "        return self.classifier.predict(X_pca)\n",
    "    \n",
    "    def get_explained_variance(self):\n",
    "        \"\"\"Get cumulative explained variance by components.\"\"\"\n",
    "        return np.cumsum(self.pca.explained_variance_ratio_)\n",
    "\n",
    "# Train Eigenface recognizer\n",
    "print(\"\\nTraining Eigenface recognizer...\")\n",
    "print(f\"  n_components: 100\")\n",
    "print(f\"  Classifier: SVM (RBF kernel)\")\n",
    "\n",
    "eigenface_recognizer = EigenfaceRecognizer(n_components=100, classifier='svm')\n",
    "eigenface_recognizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_eigen = eigenface_recognizer.predict(X_test)\n",
    "accuracy_eigen = accuracy_score(y_test, y_pred_eigen)\n",
    "\n",
    "print(f\"\\nEigenfaces Accuracy: {accuracy_eigen*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize eigenfaces\n",
    "print(\"=\"*70)\n",
    "print(\"VISUALIZING EIGENFACES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 7))\n",
    "\n",
    "# Mean face\n",
    "img_shape = X_gray[0].shape\n",
    "\n",
    "axes[0, 0].imshow(eigenface_recognizer.mean_face.reshape(img_shape), cmap='gray')\n",
    "axes[0, 0].set_title('Mean Face', fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# First 9 eigenfaces\n",
    "for i in range(1, 10):\n",
    "    row, col = i // 5, i % 5\n",
    "    eigenface = eigenface_recognizer.eigenfaces[i-1].reshape(img_shape)\n",
    "    axes[row, col].imshow(eigenface, cmap='gray')\n",
    "    axes[row, col].set_title(f'Eigenface {i}', fontweight='bold')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle('Mean Face and Top Eigenfaces', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Eigenface 1-3: Capture lighting variations\")\n",
    "print(\"  - Eigenface 4+: Capture facial structure differences\")\n",
    "print(\"  - Each face = mean + weighted sum of eigenfaces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot explained variance\n",
    "print(\"=\"*70)\n",
    "print(\"PCA EXPLAINED VARIANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "explained_var = eigenface_recognizer.get_explained_variance()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(range(1, len(explained_var) + 1), explained_var * 100, 'b-', linewidth=2)\n",
    "ax.axhline(y=95, color='r', linestyle='--', label='95% variance')\n",
    "ax.axhline(y=90, color='orange', linestyle='--', label='90% variance')\n",
    "\n",
    "# Find components for 90% and 95%\n",
    "n_90 = np.argmax(explained_var >= 0.90) + 1\n",
    "n_95 = np.argmax(explained_var >= 0.95) + 1\n",
    "\n",
    "ax.axvline(x=n_90, color='orange', linestyle=':', alpha=0.7)\n",
    "ax.axvline(x=n_95, color='r', linestyle=':', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Number of Components')\n",
    "ax.set_ylabel('Cumulative Explained Variance (%)')\n",
    "ax.set_title('PCA: Explained Variance vs Components', fontweight='bold', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nComponents needed:\")\n",
    "print(f\"  90% variance: {n_90} components\")\n",
    "print(f\"  95% variance: {n_95} components\")\n",
    "print(f\"\\nUsing {eigenface_recognizer.n_components} components captures {explained_var[-1]*100:.1f}% variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LBPH FACE RECOGNITION\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"LBPH (LOCAL BINARY PATTERN HISTOGRAM) RECOGNITION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "LBPH Algorithm:\n",
    "===============\n",
    "1. Divide face into grid of cells\n",
    "2. For each pixel, compute LBP code (compare with neighbors)\n",
    "3. Build histogram of LBP codes for each cell\n",
    "4. Concatenate histograms into feature vector\n",
    "5. Compare using histogram distance\n",
    "\n",
    "Key Parameters:\n",
    "  - radius: Radius for LBP (1, 2, or 3)\n",
    "  - neighbors: Number of sampling points (8, 16, 24)\n",
    "  - grid_x, grid_y: Number of cells in grid\n",
    "\"\"\")\n",
    "\n",
    "# Create LBPH recognizer\n",
    "lbph_recognizer = cv2.face.LBPHFaceRecognizer_create(\n",
    "    radius=1,        # Radius of circular LBP pattern\n",
    "    neighbors=8,     # Number of neighbors to sample\n",
    "    grid_x=8,        # Number of cells in X direction\n",
    "    grid_y=8         # Number of cells in Y direction\n",
    ")\n",
    "\n",
    "print(\"\\nLBPH Parameters:\")\n",
    "print(f\"  radius: 1 (local pattern size)\")\n",
    "print(f\"  neighbors: 8 (sampling points)\")\n",
    "print(f\"  grid: 8x8 (spatial granularity)\")\n",
    "\n",
    "# Train LBPH\n",
    "print(\"\\nTraining LBPH recognizer...\")\n",
    "lbph_recognizer.train(X_train_gray, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_lbph = []\n",
    "for face in X_test_gray:\n",
    "    label, confidence = lbph_recognizer.predict(face)\n",
    "    y_pred_lbph.append(label)\n",
    "\n",
    "y_pred_lbph = np.array(y_pred_lbph)\n",
    "accuracy_lbph = accuracy_score(y_test, y_pred_lbph)\n",
    "\n",
    "print(f\"\\nLBPH Accuracy: {accuracy_lbph*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPARE DIFFERENT LBPH PARAMETERS\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"LBPH PARAMETER TUNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test different parameters\n",
    "param_combinations = [\n",
    "    {'radius': 1, 'neighbors': 8, 'grid_x': 8, 'grid_y': 8},\n",
    "    {'radius': 2, 'neighbors': 8, 'grid_x': 8, 'grid_y': 8},\n",
    "    {'radius': 1, 'neighbors': 16, 'grid_x': 8, 'grid_y': 8},\n",
    "    {'radius': 1, 'neighbors': 8, 'grid_x': 4, 'grid_y': 4},\n",
    "    {'radius': 1, 'neighbors': 8, 'grid_x': 12, 'grid_y': 12},\n",
    "]\n",
    "\n",
    "results = []\n",
    "for params in param_combinations:\n",
    "    lbph = cv2.face.LBPHFaceRecognizer_create(**params)\n",
    "    lbph.train(X_train_gray, y_train)\n",
    "    \n",
    "    y_pred = [lbph.predict(face)[0] for face in X_test_gray]\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'radius': params['radius'],\n",
    "        'neighbors': params['neighbors'],\n",
    "        'grid': f\"{params['grid_x']}x{params['grid_y']}\",\n",
    "        'accuracy': acc\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['accuracy'] = results_df['accuracy'] * 100\n",
    "\n",
    "print(\"\\nLBPH Parameter Comparison:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "best_idx = results_df['accuracy'].idxmax()\n",
    "print(f\"\\nBest parameters: {param_combinations[best_idx]}\")\n",
    "print(f\"Best accuracy: {results_df.loc[best_idx, 'accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part8'></a>\n",
    "# Part 8: Complete Recognition Pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPLETE FACE RECOGNITION PIPELINE\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"COMPLETE FACE RECOGNITION PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class FaceRecognitionPipeline:\n",
    "    \"\"\"\n",
    "    Complete face recognition pipeline:\n",
    "    Detection -> Preprocessing -> Recognition\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, recognizer_type='eigenfaces', n_components=100):\n",
    "        \"\"\"\n",
    "        Initialize pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        - recognizer_type: 'eigenfaces' or 'lbph'\n",
    "        - n_components: For eigenfaces\n",
    "        \"\"\"\n",
    "        # Face detector (Haar cascade)\n",
    "        self.face_cascade = cv2.CascadeClassifier(\n",
    "            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "        )\n",
    "        \n",
    "        # Face recognizer\n",
    "        self.recognizer_type = recognizer_type\n",
    "        if recognizer_type == 'eigenfaces':\n",
    "            self.recognizer = EigenfaceRecognizer(n_components=n_components)\n",
    "        else:\n",
    "            self.recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "        \n",
    "        self.label_encoder = None\n",
    "        self.target_size = (100, 100)\n",
    "        self.is_trained = False\n",
    "    \n",
    "    def detect_face(self, image):\n",
    "        \"\"\"\n",
    "        Detect and extract face from image.\n",
    "        Returns cropped face or None.\n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n",
    "        \n",
    "        faces = self.face_cascade.detectMultiScale(\n",
    "            gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)\n",
    "        )\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            return None, None\n",
    "        \n",
    "        # Take largest face\n",
    "        x, y, w, h = max(faces, key=lambda f: f[2] * f[3])\n",
    "        face = gray[y:y+h, x:x+w]\n",
    "        face = cv2.resize(face, self.target_size)\n",
    "        \n",
    "        return face, (x, y, w, h)\n",
    "    \n",
    "    def train(self, images, labels):\n",
    "        \"\"\"\n",
    "        Train the recognizer.\n",
    "        \n",
    "        Parameters:\n",
    "        - images: List of face images (already cropped)\n",
    "        - labels: List of person names\n",
    "        \"\"\"\n",
    "        # Encode labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        y = self.label_encoder.fit_transform(labels)\n",
    "        \n",
    "        # Preprocess images\n",
    "        X_processed = []\n",
    "        for img in images:\n",
    "            if len(img.shape) == 3:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "            img = cv2.resize(img, self.target_size)\n",
    "            X_processed.append(img)\n",
    "        X_processed = np.array(X_processed)\n",
    "        \n",
    "        # Train recognizer\n",
    "        if self.recognizer_type == 'eigenfaces':\n",
    "            X_flat = X_processed.reshape(len(X_processed), -1)\n",
    "            self.recognizer.fit(X_flat, y)\n",
    "        else:\n",
    "            self.recognizer.train(X_processed, y)\n",
    "        \n",
    "        self.is_trained = True\n",
    "        return self\n",
    "    \n",
    "    def recognize(self, image, return_confidence=False):\n",
    "        \"\"\"\n",
    "        Recognize person in image.\n",
    "        \n",
    "        Returns:\n",
    "        - name: Predicted person name\n",
    "        - bbox: Face bounding box\n",
    "        - confidence: (optional) Recognition confidence\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Pipeline not trained! Call train() first.\")\n",
    "        \n",
    "        # Detect face\n",
    "        face, bbox = self.detect_face(image)\n",
    "        \n",
    "        if face is None:\n",
    "            return \"No face detected\", None, 0\n",
    "        \n",
    "        # Recognize\n",
    "        if self.recognizer_type == 'eigenfaces':\n",
    "            face_flat = face.flatten().reshape(1, -1)\n",
    "            pred = self.recognizer.predict(face_flat)[0]\n",
    "            confidence = 1.0  # Eigenfaces doesn't give confidence\n",
    "        else:\n",
    "            pred, confidence = self.recognizer.predict(face)\n",
    "            confidence = max(0, 100 - confidence) / 100  # Convert to 0-1 scale\n",
    "        \n",
    "        name = self.label_encoder.inverse_transform([pred])[0]\n",
    "        \n",
    "        if return_confidence:\n",
    "            return name, bbox, confidence\n",
    "        return name, bbox\n",
    "\n",
    "# Create and train pipeline\n",
    "print(\"\\nCreating Face Recognition Pipeline...\")\n",
    "pipeline = FaceRecognitionPipeline(recognizer_type='lbph')\n",
    "\n",
    "# Train on grayscale images\n",
    "print(\"Training pipeline...\")\n",
    "pipeline.train(X_gray, labels)\n",
    "\n",
    "print(\"\\nPipeline ready!\")\n",
    "print(f\"  Recognizer: LBPH\")\n",
    "print(f\"  Trained on: {len(X_gray)} images\")\n",
    "print(f\"  Classes: {len(set(labels))} people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pipeline\n",
    "print(\"=\"*70)\n",
    "print(\"TESTING RECOGNITION PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test on some images\n",
    "n_test = 12\n",
    "test_indices = np.random.choice(len(X), n_test, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "correct = 0\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = test_indices[i]\n",
    "    img = (X[idx] * 255).astype('uint8')\n",
    "    true_name = labels[idx]\n",
    "    \n",
    "    # Recognize\n",
    "    pred_name, bbox, conf = pipeline.recognize(img, return_confidence=True)\n",
    "    \n",
    "    # Draw result\n",
    "    img_display = img.copy()\n",
    "    if bbox is not None:\n",
    "        x, y, w, h = bbox\n",
    "        color = (0, 255, 0) if pred_name == true_name else (255, 0, 0)\n",
    "        cv2.rectangle(img_display, (x, y), (x+w, y+h), color, 2)\n",
    "    \n",
    "    ax.imshow(img_display)\n",
    "    \n",
    "    is_correct = pred_name.replace('_', ' ') == true_name.replace('_', ' ')\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    \n",
    "    color = 'green' if is_correct else 'red'\n",
    "    ax.set_title(f\"True: {true_name.replace('_', ' ')}\\nPred: {pred_name.replace('_', ' ')}\", \n",
    "                 fontsize=9, color=color, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(f'Face Recognition Results ({correct}/{n_test} correct)', \n",
    "             fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTest Accuracy: {correct}/{n_test} = {correct/n_test*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part9'></a>\n",
    "# Part 9: Evaluation & Results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPREHENSIVE EVALUATION\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compare methods\n",
    "comparison_results = pd.DataFrame({\n",
    "    'Method': ['Eigenfaces (PCA + SVM)', 'LBPH'],\n",
    "    'Accuracy': [accuracy_eigen * 100, accuracy_lbph * 100],\n",
    "    'Training': ['Requires PCA + SVM training', 'Builds histograms per person'],\n",
    "    'Speed': ['Medium', 'Fast'],\n",
    "    'Min Samples': ['Many (for good PCA)', 'Few (even 1 per person)'],\n",
    "})\n",
    "\n",
    "print(\"\\nMethod Comparison:\")\n",
    "print(comparison_results.to_string(index=False))\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "methods = comparison_results['Method']\n",
    "accuracies = comparison_results['Accuracy']\n",
    "\n",
    "bars = ax.bar(methods, accuracies, color=['steelblue', 'coral'], edgecolor='black')\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "            f'{acc:.1f}%', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Face Recognition Method Comparison', fontweight='bold', fontsize=14)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.axhline(y=90, color='green', linestyle='--', alpha=0.5, label='90% baseline')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "print(\"=\"*70)\n",
    "print(\"CONFUSION MATRIX (TOP CLASSES)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get predictions\n",
    "best_pred = y_pred_lbph if accuracy_lbph > accuracy_eigen else y_pred_eigen\n",
    "best_name = \"LBPH\" if accuracy_lbph > accuracy_eigen else \"Eigenfaces\"\n",
    "\n",
    "# Get top N classes by frequency\n",
    "n_top = 10\n",
    "top_classes = [label_encoder.transform([c])[0] for c in \n",
    "               [x[0] for x in Counter(labels).most_common(n_top)]]\n",
    "\n",
    "# Filter to top classes\n",
    "mask = np.isin(y_test, top_classes)\n",
    "y_test_top = y_test[mask]\n",
    "best_pred_top = best_pred[mask]\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_top, best_pred_top, labels=top_classes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "top_names = [label_encoder.inverse_transform([c])[0].replace('_', ' ')[:15] \n",
    "             for c in top_classes]\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=top_names, yticklabels=top_names)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title(f'Confusion Matrix - {best_name} (Top {n_top} Classes)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part10'></a>\n",
    "# Part 10: Summary & Key Takeaways\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"FACE RECOGNITION SYSTEM - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "WHAT WE LEARNED:\n",
    "================\n",
    "\n",
    "1. FACE DETECTION vs RECOGNITION\n",
    "   - Detection: WHERE are faces? (bounding boxes)\n",
    "   - Recognition: WHO is this? (identity)\n",
    "\n",
    "2. DETECTION METHODS:\n",
    "   ┌─────────────┬──────────┬──────────┬─────────────┐\n",
    "   │ Method      │ Type     │ Speed    │ Accuracy    │\n",
    "   ├─────────────┼──────────┼──────────┼─────────────┤\n",
    "   │ Haar Cascade│ Classical│ Very Fast│ Medium      │\n",
    "   │ HOG + SVM   │ ML       │ Fast     │ Good        │\n",
    "   │ MTCNN       │ DL       │ Medium   │ Very Good   │\n",
    "   └─────────────┴──────────┴──────────┴─────────────┘\n",
    "\n",
    "3. RECOGNITION METHODS:\n",
    "   ┌─────────────┬──────────────────┬─────────────┐\n",
    "   │ Method      │ How It Works     │ Best For    │\n",
    "   ├─────────────┼──────────────────┼─────────────┤\n",
    "   │ Eigenfaces  │ PCA projection   │ Learning    │\n",
    "   │ LBPH        │ Texture patterns │ Few samples │\n",
    "   │ Deep Learning│ CNN embeddings  │ Production  │\n",
    "   └─────────────┴──────────────────┴─────────────┘\n",
    "\n",
    "4. KEY PARAMETERS TO TUNE:\n",
    "   - Haar: scaleFactor, minNeighbors, minSize\n",
    "   - Eigenfaces: n_components (typically 50-150)\n",
    "   - LBPH: radius, neighbors, grid_x, grid_y\n",
    "\n",
    "5. FACE EMBEDDINGS:\n",
    "   - Convert face to 128-D vector\n",
    "   - Compare using Euclidean or cosine distance\n",
    "   - Threshold determines match (typically 0.6)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nRESULTS ON LFW DATASET:\")\n",
    "print(f\"  Eigenfaces: {accuracy_eigen*100:.2f}%\")\n",
    "print(f\"  LBPH:       {accuracy_lbph*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Taxonomy\n",
    "\n",
    "### Face Detection Methods\n",
    "\n",
    "| Method | Year | Type | Key Idea | Parameters |\n",
    "|--------|------|------|----------|------------|\n",
    "| **Haar Cascades** | 2001 | Classical | Haar features + AdaBoost cascade | scaleFactor, minNeighbors |\n",
    "| **HOG + SVM** | 2005 | ML | Gradient histograms + linear SVM | cell_size, block_size |\n",
    "| **MTCNN** | 2016 | Deep Learning | 3-stage CNN cascade | confidence threshold |\n",
    "\n",
    "### Face Recognition Methods\n",
    "\n",
    "| Method | Year | Type | Key Idea | Parameters |\n",
    "|--------|------|------|----------|------------|\n",
    "| **Eigenfaces** | 1991 | Statistical | PCA dimensionality reduction | n_components |\n",
    "| **Fisherfaces** | 1997 | Statistical | LDA class separation | n_components |\n",
    "| **LBPH** | 2006 | Texture | Local binary patterns | radius, neighbors, grid |\n",
    "| **FaceNet** | 2015 | Deep Learning | Triplet loss embeddings | embedding_size |\n",
    "| **ArcFace** | 2019 | Deep Learning | Angular margin loss | margin, scale |\n",
    "\n",
    "---\n",
    "\n",
    "## Checklist\n",
    "\n",
    "- [x] Understood Face Detection vs Recognition\n",
    "- [x] Learned Haar Cascade parameters and tuning\n",
    "- [x] Implemented Eigenfaces from scratch\n",
    "- [x] Implemented LBPH recognition\n",
    "- [x] Understood face embeddings and similarity\n",
    "- [x] Built complete recognition pipeline\n",
    "- [x] Evaluated and compared methods\n",
    "- [x] Learned parameter tuning for each algorithm\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "| Step | What to Learn |\n",
    "|------|---------------|\n",
    "| 1 | Try deep learning with `face_recognition` library |\n",
    "| 2 | Implement real-time video recognition |\n",
    "| 3 | Add face alignment for better accuracy |\n",
    "| 4 | Deploy as web application |\n",
    "\n",
    "---\n",
    "\n",
    "**End of Face Recognition Tutorial**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}