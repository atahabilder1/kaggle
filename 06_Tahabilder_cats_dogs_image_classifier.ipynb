{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Cats vs Dogs Image Classification with CNNs\n",
    "\n",
    "**Author:** Anik Tahabilder  \n",
    "**Project:** 6 of 22 - Kaggle ML Portfolio  \n",
    "**Dataset:** Microsoft Cats vs Dogs  \n",
    "**Difficulty:** 6/10 | **Resume Value:** 7/10 | **Learning Value:** 8/10 | **Impact:** 8/10\n",
    "\n",
    "---\n",
    "\n",
    "## What is Image Classification?\n",
    "\n",
    "**Image Classification** is a computer vision task where we teach computers to recognize and categorize objects in images. It's one of the most fundamental problems in deep learning.\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "| Application | Example |\n",
    "|-------------|----------|\n",
    "| **Medical Diagnosis** | Detecting cancer from X-rays |\n",
    "| **Autonomous Vehicles** | Identifying pedestrians, stop signs |\n",
    "| **Face Recognition** | Unlocking phones, security systems |\n",
    "| **Quality Control** | Detecting defects in manufacturing |\n",
    "| **Wildlife Monitoring** | Identifying endangered species |\n",
    "\n",
    "---\n",
    "\n",
    "## Why Cats vs Dogs?\n",
    "\n",
    "This is the \"Hello World\" of computer vision! It's:\n",
    "- **Simple enough** to understand the fundamentals\n",
    "- **Complex enough** to require deep learning (not solvable with simple ML)\n",
    "- **Visually intuitive** - you can see what the model learns\n",
    "- **Practically relevant** - techniques transfer to real problems\n",
    "\n",
    "### The Challenge:\n",
    "\n",
    "Traditional machine learning struggles with images because:\n",
    "- Images have **high dimensionality** (224x224x3 = 150,528 pixels!)\n",
    "- **Position matters** - a cat in the top-left vs bottom-right is still a cat\n",
    "- **Scale varies** - cats can be close-up or far away\n",
    "- **Lighting changes** - same cat in sunlight vs shadow looks different\n",
    "\n",
    "**Solution:** Convolutional Neural Networks (CNNs)!\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this notebook, we'll:\n",
    "\n",
    "1. **Understand CNNs** - How they work, why they're powerful\n",
    "2. **Load and explore** image data\n",
    "3. **Preprocess images** - Resizing, normalization, augmentation\n",
    "4. **Build CNNs from scratch** - Custom architectures\n",
    "5. **Use Transfer Learning** - Leverage pre-trained models (VGG16, ResNet)\n",
    "6. **Visualize** what the model learns\n",
    "7. **Compare** different architectures\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: Understanding CNNs](#part1)\n",
    "2. [Part 2: Setup and Data Loading](#part2)\n",
    "3. [Part 3: Exploratory Data Analysis](#part3)\n",
    "4. [Part 4: Image Preprocessing](#part4)\n",
    "5. [Part 5: Data Augmentation](#part5)\n",
    "6. [Part 6: Building a Simple CNN](#part6)\n",
    "7. [Part 7: Training the Model](#part7)\n",
    "8. [Part 8: Model Evaluation](#part8)\n",
    "9. [Part 9: Transfer Learning with VGG16](#part9)\n",
    "10. [Part 10: Model Comparison](#part10)\n",
    "11. [Part 11: Visualizing What CNNs Learn](#part11)\n",
    "12. [Part 12: Making Predictions](#part12)\n",
    "13. [Part 13: Summary and Key Takeaways](#part13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part1'></a>\n",
    "# Part 1: Understanding Convolutional Neural Networks (CNNs)\n",
    "\n",
    "## Why Not Regular Neural Networks?\n",
    "\n",
    "Imagine a 224x224 RGB image:\n",
    "- Total pixels: 224 × 224 × 3 = **150,528 inputs**\n",
    "- With just 1000 neurons in first layer: **150 million parameters**!\n",
    "- This leads to:\n",
    "  - **Overfitting** (too many parameters)\n",
    "  - **Slow training** (massive computations)\n",
    "  - **Ignoring spatial structure** (treats pixels as independent)\n",
    "\n",
    "---\n",
    "\n",
    "## The CNN Solution\n",
    "\n",
    "CNNs are inspired by how the **human visual cortex** works:\n",
    "- Neurons respond to specific regions (receptive fields)\n",
    "- Early layers detect simple patterns (edges, colors)\n",
    "- Later layers detect complex patterns (eyes, ears, faces)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Building Blocks\n",
    "\n",
    "### 1. Convolutional Layer\n",
    "\n",
    "**What it does:** Scans the image with small filters (kernels) to detect patterns.\n",
    "\n",
    "```\n",
    "Input Image (5x5)       Filter/Kernel (3x3)       Output Feature Map\n",
    "┌─────────────┐         ┌─────┐                   ┌─────────┐\n",
    "│ 1 0 1 0 1  │         │ 1 0 1│                   │  ?  ?  ?│\n",
    "│ 0 1 0 1 0  │    *    │ 0 1 0│        =          │  ?  ?  ?│\n",
    "│ 1 0 1 0 1  │         │ 1 0 1│                   │  ?  ?  ?│\n",
    "│ 0 1 0 1 0  │         └─────┘                   └─────────┘\n",
    "│ 1 0 1 0 1  │\n",
    "└─────────────┘\n",
    "```\n",
    "\n",
    "**Key Properties:**\n",
    "- **Parameter Sharing:** Same filter scans entire image\n",
    "- **Translation Invariance:** Detects pattern anywhere in image\n",
    "- **Fewer Parameters:** 3×3 filter has only 9 weights!\n",
    "\n",
    "### Common Filter Types:\n",
    "\n",
    "| Filter Type | What it Detects |\n",
    "|-------------|----------------|\n",
    "| **Edge Detector** | Vertical/horizontal/diagonal edges |\n",
    "| **Sharpen** | Enhances edges |\n",
    "| **Blur** | Smooths image |\n",
    "| **Custom** | Learned by the network! |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Activation Function (ReLU)\n",
    "\n",
    "**ReLU (Rectified Linear Unit):** $f(x) = max(0, x)$\n",
    "\n",
    "- Introduces **non-linearity** (allows learning complex patterns)\n",
    "- Simple and fast to compute\n",
    "- Helps with vanishing gradient problem\n",
    "\n",
    "```\n",
    "Before ReLU:  [-2, 5, -1, 3]\n",
    "After ReLU:   [0, 5, 0, 3]   (negative values → 0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Pooling Layer\n",
    "\n",
    "**What it does:** Reduces spatial dimensions (downsampling).\n",
    "\n",
    "**Max Pooling (2×2):**\n",
    "```\n",
    "Input (4x4)              Output (2x2)\n",
    "┌──────────┐             ┌────┐\n",
    "│ 1  3│2  4│             │ 3  4│\n",
    "│ 2  1│1  3│     →       │ 8  9│\n",
    "├──────────┤             └────┘\n",
    "│ 0  2│5  1│\n",
    "│ 1  8│9  2│\n",
    "└──────────┘\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Reduces computation (smaller feature maps)\n",
    "- Provides spatial invariance (small shifts don't matter)\n",
    "- Prevents overfitting (reduces parameters)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Fully Connected Layer\n",
    "\n",
    "After several conv/pool layers:\n",
    "- **Flatten** feature maps into a vector\n",
    "- **Dense layers** combine features for final classification\n",
    "- **Output layer** (2 neurons for Cat vs Dog, softmax activation)\n",
    "\n",
    "---\n",
    "\n",
    "## Typical CNN Architecture\n",
    "\n",
    "```\n",
    "Input Image (224x224x3)\n",
    "      ↓\n",
    "[Conv → ReLU → Pool] × N    ← Feature Extraction\n",
    "      ↓\n",
    "Flatten\n",
    "      ↓\n",
    "[Dense → ReLU] × M           ← Classification\n",
    "      ↓\n",
    "Output (Dense + Softmax)\n",
    "```\n",
    "\n",
    "**Pattern:**\n",
    "- Image size **decreases** (224 → 112 → 56 → 28...)\n",
    "- Number of filters **increases** (32 → 64 → 128 → 256...)\n",
    "- Network learns **hierarchical features**:\n",
    "  - Early layers: edges, colors, textures\n",
    "  - Middle layers: parts (ears, eyes, nose)\n",
    "  - Later layers: whole objects (cat face, dog body)\n",
    "\n",
    "---\n",
    "\n",
    "## CNN vs Traditional ML\n",
    "\n",
    "| Aspect | Traditional ML | CNN |\n",
    "|--------|---------------|-----|\n",
    "| **Feature Engineering** | Manual (HOG, SIFT, etc.) | Automatic (learned) |\n",
    "| **Spatial Structure** | Lost (flatten image) | Preserved (convolutions) |\n",
    "| **Parameters** | Millions (fully connected) | Thousands (shared weights) |\n",
    "| **Accuracy** | 60-70% (cats vs dogs) | 95-99%+ |\n",
    "\n",
    "---\n",
    "\n",
    "Now let's implement this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part2'></a>\n",
    "# Part 2: Setup and Data Loading\n",
    "\n",
    "## Required Libraries\n",
    "\n",
    "For image classification with CNNs, we need:\n",
    "\n",
    "| Library | Purpose |\n",
    "|---------|----------|\n",
    "| **TensorFlow/Keras** | Building and training neural networks |\n",
    "| **NumPy** | Numerical operations |\n",
    "| **Matplotlib** | Visualization |\n",
    "| **OpenCV/PIL** | Image processing |\n",
    "| **scikit-learn** | Evaluation metrics |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Core libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom PIL import Image\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Suppress TensorFlow warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n\n# TensorFlow and Keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, optimizers, callbacks\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.applications import VGG16, ResNet50\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (Dense, Conv2D, MaxPooling2D, Flatten, \n                                      Dropout, BatchNormalization, GlobalAveragePooling2D)\n\n# Sklearn for metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Visualization settings\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nprint(\"=\"*60)\nprint(\"SETUP COMPLETE\")\nprint(\"=\"*60)\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Keras version: {keras.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\n\n# Check for GPU availability\ntry:\n    gpus = tf.config.list_physical_devices('GPU')\n    if gpus:\n        print(f\"\\nGPU Available: {len(gpus)} GPU(s) detected\")\n        print(\"GPU will be used for training!\")\n        # Optional: Set memory growth to avoid OOM errors\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    else:\n        print(\"\\nNo GPU detected\")\n        print(\"Using CPU for training (slower, but works!)\")\nexcept Exception as e:\n    print(\"\\nGPU check failed, using CPU\")\n    print(\"Using CPU for training (slower, but works!)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Dataset Information\n",
    "\n",
    "**Microsoft Cats vs Dogs Dataset:**\n",
    "- **Source:** Kaggle\n",
    "- **Size:** ~25,000 images\n",
    "- **Classes:** 2 (Cat, Dog)\n",
    "- **Format:** JPEG images of various sizes\n",
    "- **Challenge:** Real-world images (different sizes, angles, lighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "# On Kaggle, data is in /kaggle/input/\n",
    "# Locally, adjust the path accordingly\n",
    "\n",
    "# Check if running on Kaggle\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    BASE_DIR = '/kaggle/input/microsoft-catsvsdogs-dataset/PetImages'\n",
    "    print(\"Running on Kaggle\")\n",
    "else:\n",
    "    # For local execution, adjust this path\n",
    "    BASE_DIR = 'PetImages'  # Change to your local path\n",
    "    print(\"Running locally\")\n",
    "\n",
    "CAT_DIR = os.path.join(BASE_DIR, 'Cat')\n",
    "DOG_DIR = os.path.join(BASE_DIR, 'Dog')\n",
    "\n",
    "print(f\"\\nDataset location: {BASE_DIR}\")\n",
    "print(f\"Cat images: {CAT_DIR}\")\n",
    "print(f\"Dog images: {DOG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images in each category\n",
    "num_cats = len([f for f in os.listdir(CAT_DIR) if f.endswith('.jpg')])\n",
    "num_dogs = len([f for f in os.listdir(DOG_DIR) if f.endswith('.jpg')])\n",
    "total_images = num_cats + num_dogs\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Cat images: {num_cats:,}\")\n",
    "print(f\"Dog images: {num_dogs:,}\")\n",
    "print(f\"Total images: {total_images:,}\")\n",
    "print(f\"\\nClass Balance: {num_cats/total_images*100:.1f}% cats, {num_dogs/total_images*100:.1f}% dogs\")\n",
    "\n",
    "if abs(num_cats - num_dogs) / total_images < 0.1:\n",
    "    print(\"✓ Dataset is balanced!\")\n",
    "else:\n",
    "    print(\"⚠ Dataset is imbalanced (may need class weights)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unl6mr5nsrh",
   "source": "---\n\n<a id='part3'></a>\n# Part 3: Exploratory Data Analysis (EDA)\n\nBefore building models, we must **understand our data**:\n- Are there corrupted images?\n- What are the image sizes?\n- What do the images look like?\n- Are there any patterns we should know about?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ax6fo7mo137",
   "source": "# Check for corrupted images\n# Some images in this dataset are corrupted and will cause errors\n\ndef check_image(filepath):\n    \"\"\"Check if an image can be loaded properly.\"\"\"\n    try:\n        img = Image.open(filepath)\n        img.verify()  # Verify it's actually an image\n        return True\n    except:\n        return False\n\nprint(\"Checking for corrupted images...\")\nprint(\"This may take a few minutes...\")\n\ncorrupted_cats = []\ncorrupted_dogs = []\n\n# Check cat images\ncat_files = [f for f in os.listdir(CAT_DIR) if f.endswith('.jpg')]\nfor filename in cat_files[:1000]:  # Check first 1000 for speed\n    filepath = os.path.join(CAT_DIR, filename)\n    if not check_image(filepath):\n        corrupted_cats.append(filename)\n\n# Check dog images  \ndog_files = [f for f in os.listdir(DOG_DIR) if f.endswith('.jpg')]\nfor filename in dog_files[:1000]:  # Check first 1000 for speed\n    filepath = os.path.join(DOG_DIR, filename)\n    if not check_image(filepath):\n        corrupted_dogs.append(filename)\n\nprint(f\"\\nCorrupted cat images found: {len(corrupted_cats)}\")\nprint(f\"Corrupted dog images found: {len(corrupted_dogs)}\")\nprint(f\"Total corrupted: {len(corrupted_cats) + len(corrupted_dogs)}\")\n\nif len(corrupted_cats) + len(corrupted_dogs) > 0:\n    print(\"\\n⚠ We'll need to filter these out during data loading!\")\nelse:\n    print(\"\\n✓ No corrupted images found in sample!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2n1h2v2m3wc",
   "source": "# Analyze image dimensions\nprint(\"Analyzing image dimensions...\")\nprint(\"Sampling 500 images from each class...\\n\")\n\ndef get_image_dimensions(directory, num_samples=500):\n    \"\"\"Get dimensions of sample images.\"\"\"\n    files = [f for f in os.listdir(directory) if f.endswith('.jpg')]\n    files = files[:num_samples]\n    \n    widths = []\n    heights = []\n    \n    for filename in files:\n        filepath = os.path.join(directory, filename)\n        try:\n            img = Image.open(filepath)\n            w, h = img.size\n            widths.append(w)\n            heights.append(h)\n        except:\n            continue\n    \n    return widths, heights\n\ncat_widths, cat_heights = get_image_dimensions(CAT_DIR)\ndog_widths, dog_heights = get_image_dimensions(DOG_DIR)\n\nall_widths = cat_widths + dog_widths\nall_heights = cat_heights + dog_heights\n\nprint(\"=\"*60)\nprint(\"IMAGE DIMENSION STATISTICS\")\nprint(\"=\"*60)\nprint(f\"\\nWidth  - Min: {min(all_widths):4d}, Max: {max(all_widths):4d}, Mean: {np.mean(all_widths):.1f}\")\nprint(f\"Height - Min: {min(all_heights):4d}, Max: {max(all_heights):4d}, Mean: {np.mean(all_heights):.1f}\")\nprint(f\"\\nMost common aspect ratios: landscape and portrait\")\nprint(\"Note: Images have VARYING sizes - we'll need to resize them!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "gf0qnvnb3em",
   "source": "# Visualize image dimension distribution\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Width distribution\naxes[0].hist(all_widths, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\naxes[0].axvline(np.mean(all_widths), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(all_widths):.0f}')\naxes[0].set_xlabel('Width (pixels)')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('Distribution of Image Widths')\naxes[0].legend()\n\n# Height distribution\naxes[1].hist(all_heights, bins=50, color='lightcoral', edgecolor='black', alpha=0.7)\naxes[1].axvline(np.mean(all_heights), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(all_heights):.0f}')\naxes[1].set_xlabel('Height (pixels)')\naxes[1].set_ylabel('Frequency')\naxes[1].set_title('Distribution of Image Heights')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key Observation: Images have highly variable dimensions!\")\nprint(\"We'll standardize to 150x150 or 224x224 for training.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2jii2cniimq",
   "source": "## 3.3 Visualizing Sample Images\n\nLet's see what our cats and dogs actually look like!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "uhwgvd6ix89",
   "source": "# Visualize a grid of sample images\ndef load_sample_images(directory, num_samples=12):\n    \"\"\"Load sample images from a directory.\"\"\"\n    files = [f for f in os.listdir(directory) if f.endswith('.jpg')][:num_samples]\n    images = []\n    \n    for filename in files:\n        filepath = os.path.join(directory, filename)\n        try:\n            img = load_img(filepath, target_size=(150, 150))\n            images.append(img_to_array(img) / 255.0)  # Normalize\n        except:\n            continue\n    \n    return images\n\n# Load sample cats and dogs\nprint(\"Loading sample images...\")\nsample_cats = load_sample_images(CAT_DIR, num_samples=8)\nsample_dogs = load_sample_images(DOG_DIR, num_samples=8)\n\nprint(f\"Loaded {len(sample_cats)} cat images\")\nprint(f\"Loaded {len(sample_dogs)} dog images\")\n\n# Visualize cats\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\nfig.suptitle('Sample Cat Images', fontweight='bold', fontsize=16)\n\nfor i, ax in enumerate(axes.flat):\n    if i < len(sample_cats):\n        ax.imshow(sample_cats[i])\n        ax.axis('off')\n        ax.set_title(f'Cat {i+1}')\n\nplt.tight_layout()\nplt.show()\n\n# Visualize dogs\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\nfig.suptitle('Sample Dog Images', fontweight='bold', fontsize=16)\n\nfor i, ax in enumerate(axes.flat):\n    if i < len(sample_dogs):\n        ax.imshow(sample_dogs[i])\n        ax.axis('off')\n        ax.set_title(f'Dog {i+1}')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nObservations:\")\nprint(\"- Images vary in SIZE (different dimensions)\")\nprint(\"- Images vary in POSE (sitting, standing, lying down)\")\nprint(\"- Images vary in LIGHTING (bright, dark, outdoor, indoor)\")\nprint(\"- Images vary in ANGLE (front, side, close-up, far away)\")\nprint(\"\\nThis VARIETY is what makes the problem challenging and interesting!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xnr1gqdciom",
   "source": "---\n\n<a id='part4'></a>\n# Part 4: Image Preprocessing\n\nBefore feeding images to a CNN, we need to preprocess them. This is CRITICAL for model performance!\n\n## Why Preprocess Images?\n\n| Issue | Problem | Solution |\n|-------|---------|----------|\n| **Different Sizes** | CNNs expect fixed input size | Resize all images to same dimensions |\n| **Pixel Range** | Raw pixels are 0-255 (large values) | Normalize to 0-1 or -1 to 1 |\n| **Class Imbalance** | Model might favor majority class | Balance classes or use class weights |\n| **Limited Data** | Risk of overfitting | Data augmentation (Part 5) |\n\n## Resizing: Why 150x150?\n\nCommon CNN input sizes:\n- **224x224**: Standard for many pre-trained models (VGG, ResNet)\n- **150x150**: Good balance between detail and computation\n- **299x299**: For Inception models\n- **32x32**: For lightweight models (CIFAR-10)\n\nSmaller = Faster training but less detail  \nLarger = More detail but slower training\n\n## Normalization: Why Divide by 255?\n\n**Raw pixel values**: 0-255 (integers)  \n**Normalized values**: 0.0-1.0 (floats)\n\nBenefits:\n1. **Smaller gradient updates** (more stable training)\n2. **Faster convergence** (weights update efficiently)\n3. **Prevents saturation** (activation functions work better)\n\nFormula: `normalized_pixel = pixel / 255.0`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "fl83syrbxn",
   "source": "# Demonstration: Before and After Normalization\nsample_img_path = os.path.join(CAT_DIR, [f for f in os.listdir(CAT_DIR) if f.endswith('.jpg')][0])\n\n# Load image\nimg = load_img(sample_img_path, target_size=(150, 150))\nimg_array = img_to_array(img)\n\nprint(\"Image Preprocessing Demo\")\nprint(\"=\"*60)\nprint(f\"\\nOriginal image shape: {img_array.shape}\")\nprint(f\"Pixel value range: [{img_array.min():.0f}, {img_array.max():.0f}]\")\nprint(f\"Data type: {img_array.dtype}\")\n\n# Normalize\nimg_normalized = img_array / 255.0\n\nprint(f\"\\nNormalized image shape: {img_normalized.shape}\")\nprint(f\"Pixel value range: [{img_normalized.min():.3f}, {img_normalized.max():.3f}]\")\nprint(f\"Data type: {img_normalized.dtype}\")\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].imshow(img_array.astype('uint8'))\naxes[0].set_title('Original Image\\\\n(Pixels: 0-255)', fontweight='bold')\naxes[0].axis('off')\n\naxes[1].imshow(img_normalized)\naxes[1].set_title('Normalized Image\\\\n(Pixels: 0.0-1.0)', fontweight='bold')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nNote: Both images LOOK the same, but pixel values are scaled!\")\nprint(\"This scaling helps neural networks learn better.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ut7jyxfkj7p",
   "source": "## 4.1 Creating Train/Validation/Test Splits\n\nFor image classification, we typically split data into THREE sets:\n\n| Set | Size | Purpose |\n|-----|------|---------|\n| **Training** | 70-80% | Train the model |\n| **Validation** | 10-15% | Tune hyperparameters, monitor overfitting |\n| **Test** | 10-15% | Final evaluation (never seen during training) |\n\n**Why Validation Set?**  \nWithout it, we might overfit to the test set by trying different hyperparameters!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ektoymowujm",
   "source": "# Create data generators for training and validation\n# We'll use a subset of data for faster training (you can use more!)\n\n# Image parameters\nIMG_HEIGHT = 150\nIMG_WIDTH = 150\nBATCH_SIZE = 32\n\n# For this notebook, we'll use a smaller subset for demonstration\n# In production, you'd use all the data\nTRAIN_SAMPLES = 2000  # Per class\nVAL_SAMPLES = 500     # Per class  \nTEST_SAMPLES = 500    # Per class\n\nprint(\"Data Split Configuration\")\nprint(\"=\"*60)\nprint(f\"Image dimensions: {IMG_HEIGHT}x{IMG_WIDTH}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"\\nSamples per class:\")\nprint(f\"  Training: {TRAIN_SAMPLES}\")\nprint(f\"  Validation: {VAL_SAMPLES}\")\nprint(f\"  Test: {TEST_SAMPLES}\")\nprint(f\"\\nTotal samples:\")\nprint(f\"  Training: {TRAIN_SAMPLES * 2:,} (cats + dogs)\")\nprint(f\"  Validation: {VAL_SAMPLES * 2:,}\")\nprint(f\"  Test: {TEST_SAMPLES * 2:,}\")\n\n# Create basic data generator (no augmentation yet)\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,           # Normalize to 0-1\n    validation_split=0.2      # 20% for validation\n)\n\n# Test generator (only normalization, no augmentation)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\nprint(\"\\nData generators created!\")\nprint(\"  - Training: Rescaling to [0, 1]\")\nprint(\"  - Validation: Rescaling to [0, 1]\")\nprint(\"  - Test: Rescaling to [0, 1]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c4yg2bohcgf",
   "source": "---\n\n<a id='part5'></a>\n# Part 5: Data Augmentation\n\n## What is Data Augmentation?\n\n**Data augmentation** artificially increases training data by applying random transformations to existing images.\n\n### Why Do We Need It?\n\n| Problem | Solution |\n|---------|----------|\n| **Limited data** | Generate more training examples |\n| **Overfitting** | Model learns robust features, not memorization |\n| **Real-world variance** | Images vary in rotation, zoom, lighting |\n\n## Common Augmentation Techniques\n\n| Technique | What It Does | Example Use |\n|-----------|--------------|-------------|\n| **Rotation** | Rotate image by angle | Cat can be tilted |\n| **Horizontal Flip** | Mirror image left-right | Dog facing either direction |\n| **Zoom** | Zoom in/out | Cat close-up or far away |\n| **Width/Height Shift** | Shift image position | Object not always centered |\n| **Shear** | Slant image | Different perspectives |\n| **Brightness** | Adjust lighting | Indoor vs outdoor |\n\n### Important: Only Augment Training Data!\n\n- **Training set**: Apply augmentation\n- **Validation/Test sets**: NO augmentation (we want real-world performance)\n\n## How It Helps Learning\n\nWithout augmentation:\n```\nModel sees: Cat facing left, centered, bright lighting\nModel learns: \"Cat = exactly this pose and lighting\"\nReal world: Cat facing right, off-center, dim lighting → FAILS!\n```\n\nWith augmentation:\n```\nModel sees: Cats in many poses, positions, lighting conditions\nModel learns: \"Cat = general cat features (ears, whiskers, fur)\"\nReal world: Any cat → SUCCEEDS!\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "mwjhi29w6j",
   "source": "# Create augmented data generator\naugmented_datagen = ImageDataGenerator(\n    rescale=1./255,                # Normalize\n    rotation_range=40,             # Randomly rotate by 0-40 degrees\n    width_shift_range=0.2,         # Randomly shift horizontally by 20%\n    height_shift_range=0.2,        # Randomly shift vertically by 20%\n    shear_range=0.2,               # Shear transformation\n    zoom_range=0.2,                # Randomly zoom in/out by 20%\n    horizontal_flip=True,          # Randomly flip images\n    fill_mode='nearest',           # How to fill pixels after transformation\n    validation_split=0.2           # 20% for validation\n)\n\nprint(\"Augmented Data Generator Created!\")\nprint(\"=\"*60)\nprint(\"\\nAugmentation Parameters:\")\nprint(f\"  Rotation: 0-40 degrees\")\nprint(f\"  Horizontal shift: ±20%\")\nprint(f\"  Vertical shift: ±20%\")\nprint(f\"  Shear: 0.2\")\nprint(f\"  Zoom: ±20%\")\nprint(f\"  Horizontal flip: Yes\")\nprint(f\"  Fill mode: nearest\")\nprint(\"\\nThese transformations are applied RANDOMLY during training!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4mrj8npb6t",
   "source": "# Visualize augmentation examples\n# Show how ONE image looks after multiple random augmentations\n\n# Load a sample image\nsample_cat_img = load_img(sample_img_path, target_size=(150, 150))\nsample_array = img_to_array(sample_cat_img)\nsample_array = sample_array.reshape((1,) + sample_array.shape)  # Reshape for generator\n\nprint(\"Visualizing Data Augmentation\")\nprint(\"=\"*60)\nprint(\"Showing how ONE image looks after random augmentations...\")\nprint(\"Each augmented version is slightly different!\\n\")\n\nfig, axes = plt.subplots(3, 4, figsize=(15, 12))\nfig.suptitle('Same Image with Different Random Augmentations', fontweight='bold', fontsize=16)\n\n# Original image in first subplot\naxes[0, 0].imshow(sample_cat_img)\naxes[0, 0].set_title('ORIGINAL', fontweight='bold', fontsize=12)\naxes[0, 0].axis('off')\n\n# Generate 11 augmented versions\ni = 0\nfor batch in augmented_datagen.flow(sample_array, batch_size=1):\n    ax_row = (i + 1) // 4\n    ax_col = (i + 1) % 4\n    \n    axes[ax_row, ax_col].imshow(batch[0])\n    axes[ax_row, ax_col].set_title(f'Augmented #{i+1}', fontsize=11)\n    axes[ax_row, ax_col].axis('off')\n    \n    i += 1\n    if i >= 11:\n        break\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey Observations:\")\nprint(\"- Each version is DIFFERENT (random rotation, zoom, shift, flip)\")\nprint(\"- The cat is still recognizable in all versions\")\nprint(\"- This teaches the model to be ROBUST to these variations\")\nprint(\"- During training, EVERY image gets augmented differently each epoch!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0gfyoici8d2i",
   "source": "---\n\n<a id='part6'></a>\n# Part 6: Building a Simple CNN from Scratch\n\nNow for the exciting part - building our own Convolutional Neural Network!\n\n## Our CNN Architecture\n\nWe'll build a classic CNN with the following structure:\n\n```\nInput (150x150x3)\n    ↓\n[Conv2D (32 filters) → ReLU → MaxPool] → 75x75x32\n    ↓\n[Conv2D (64 filters) → ReLU → MaxPool] → 37x37x64\n    ↓\n[Conv2D (128 filters) → ReLU → MaxPool] → 18x18x128\n    ↓\nFlatten → 41,472 features\n    ↓\nDense (512) → ReLU\n    ↓\nDropout (0.5)\n    ↓\nDense (1) → Sigmoid (Cat vs Dog)\n```\n\n## Layer-by-Layer Explanation\n\n| Layer Type | Purpose | Parameters |\n|------------|---------|------------|\n| **Conv2D** | Extract features (edges, textures, patterns) | Filters, kernel size |\n| **ReLU** | Non-linearity (allows learning complex patterns) | None |\n| **MaxPooling2D** | Downsample (reduce size, prevent overfitting) | Pool size |\n| **Flatten** | Convert 2D features to 1D vector | None |\n| **Dense** | Fully connected layer (combines features) | Units |\n| **Dropout** | Randomly drop neurons (prevents overfitting) | Dropout rate |\n| **Sigmoid** | Output probability (0 = Cat, 1 = Dog) | None |\n\n## Why This Architecture?\n\n- **3 Conv blocks**: Each learns increasingly complex features\n- **Filter progression (32→64→128)**: More complex features need more filters\n- **MaxPooling**: Reduces spatial dimensions, prevents overfitting\n- **Dropout**: Prevents overfitting by randomly disabling neurons\n- **Sigmoid output**: Binary classification (Cat or Dog)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "i6b1ab9dpo",
   "source": "# Build the Simple CNN model\ndef build_simple_cnn(input_shape=(150, 150, 3)):\n    \"\"\"\n    Build a simple CNN for binary image classification.\n    \n    Architecture:\n    - 3 Convolutional blocks (Conv2D + MaxPool)\n    - Flatten layer\n    - Dense layer with dropout\n    - Output layer with sigmoid\n    \n    Parameters:\n    -----------\n    input_shape : tuple\n        Shape of input images (height, width, channels)\n    \n    Returns:\n    --------\n    model : keras.Model\n        Compiled CNN model\n    \"\"\"\n    model = Sequential([\n        # Block 1: 32 filters\n        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, name='conv1'),\n        MaxPooling2D((2, 2), name='pool1'),\n        \n        # Block 2: 64 filters\n        Conv2D(64, (3, 3), activation='relu', name='conv2'),\n        MaxPooling2D((2, 2), name='pool2'),\n        \n        # Block 3: 128 filters\n        Conv2D(128, (3, 3), activation='relu', name='conv3'),\n        MaxPooling2D((2, 2), name='pool3'),\n        \n        # Flatten and Dense layers\n        Flatten(name='flatten'),\n        Dense(512, activation='relu', name='dense1'),\n        Dropout(0.5, name='dropout'),\n        \n        # Output layer (binary classification)\n        Dense(1, activation='sigmoid', name='output')\n    ])\n    \n    return model\n\n# Create the model\nprint(\"Building Simple CNN...\")\nprint(\"=\"*60)\n\nsimple_cnn = build_simple_cnn()\n\nprint(\"Model created successfully!\")\nprint(\"\\nModel architecture:\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4m36d4nyaot",
   "source": "# Display model summary\nsimple_cnn.summary()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"UNDERSTANDING THE MODEL SUMMARY\")\nprint(\"=\"*60)\nprint(\"\\nKey Information:\")\nprint(\"1. LAYER TYPES: Conv2D, MaxPooling2D, Flatten, Dense, Dropout\")\nprint(\"2. OUTPUT SHAPE: How data transforms at each layer\")\nprint(\"3. PARAMETERS: Learnable weights (these get trained!)\")\nprint(\"\\nNote the progression:\")\nprint(\"  - Spatial size DECREASES: 150→75→37→18 (pooling)\")\nprint(\"  - Number of filters INCREASES: 32→64→128 (more complex features)\")\nprint(\"  - Final output: Single neuron (0=Cat, 1=Dog)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c0tqe01fcrk",
   "source": "---\n\n<a id='part7'></a>\n# Part 7: Training the Model\n\n## Compiling the Model\n\nBefore training, we need to configure the learning process:\n\n| Component | Choice | Why |\n|-----------|--------|-----|\n| **Optimizer** | Adam | Adaptive learning rate, works well for CNNs |\n| **Loss Function** | Binary Crossentropy | Binary classification (Cat or Dog) |\n| **Metrics** | Accuracy | Easy to understand, balanced dataset |\n\n### Loss Functions for Classification\n\n| Problem Type | Loss Function |\n|--------------|---------------|\n| **Binary** (2 classes) | Binary Crossentropy |\n| **Multi-class** (3+ classes) | Categorical Crossentropy |\n| **Multi-label** (multiple tags) | Binary Crossentropy |\n\n## Callbacks: Monitoring Training\n\nCallbacks are functions that run during training:\n\n| Callback | Purpose |\n|----------|---------|\n| **EarlyStopping** | Stop if validation loss doesn't improve (prevents overfitting) |\n| **ModelCheckpoint** | Save best model weights (don't lose progress!) |\n| **ReduceLROnPlateau** | Reduce learning rate when stuck (helps convergence) |\n\n## What Happens During Training?\n\nEach **epoch** (one pass through all training data):\n1. **Forward pass**: Make predictions\n2. **Compute loss**: How wrong are we?\n3. **Backward pass**: Compute gradients (backpropagation)\n4. **Update weights**: Adjust using optimizer (Adam)\n5. **Validate**: Check performance on validation set\n6. **Repeat**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "75drvz00wk6",
   "source": "# Compile the model\nsimple_cnn.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nprint(\"Model compiled successfully!\")\nprint(\"=\"*60)\nprint(\"Configuration:\")\nprint(f\"  Optimizer: Adam\")\nprint(f\"  Loss function: Binary Crossentropy\")\nprint(f\"  Metrics: Accuracy\")\n\n# Setup callbacks\nearly_stop = callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    restore_best_weights=True,\n    verbose=1\n)\n\ncheckpoint = callbacks.ModelCheckpoint(\n    'best_simple_cnn.h5',\n    monitor='val_accuracy',\n    save_best_only=True,\n    verbose=1\n)\n\nreduce_lr = callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=3,\n    min_lr=1e-7,\n    verbose=1\n)\n\nprint(\"\\nCallbacks configured:\")\nprint(\"  - EarlyStopping: Stop if val_loss doesn't improve for 5 epochs\")\nprint(\"  - ModelCheckpoint: Save best model based on val_accuracy\")\nprint(\"  - ReduceLROnPlateau: Reduce LR by 50% if val_loss plateaus\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cw8ss9jnmei",
   "source": "# Create data generators from directory\n# Note: In a real scenario, you'd organize images into train/val/test folders\n# For this demo, we'll use flow_from_directory with a subset\n\n# Since we're using the raw directory structure, let's create generators directly\n# We'll use a small subset for demonstration (you can use more in production)\n\nprint(\"Setting up data generators...\")\nprint(\"=\"*60)\n\n# Training generator with augmentation\ntrain_generator = augmented_datagen.flow_from_directory(\n    BASE_DIR,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='training',\n    shuffle=True,\n    seed=42\n)\n\n# Validation generator (no augmentation)\nval_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n\nval_generator = val_datagen.flow_from_directory(\n    BASE_DIR,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='validation',\n    shuffle=False,\n    seed=42\n)\n\nprint(f\"\\nTraining batches: {len(train_generator)}\")\nprint(f\"Validation batches: {len(val_generator)}\")\nprint(f\"\\nClass indices: {train_generator.class_indices}\")\nprint(\"(0 = Cat, 1 = Dog or vice versa depending on alphabetical order)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "z8ardbk0x6k",
   "source": "# Train the model\n# Note: For demonstration, we'll train for fewer epochs\n# In production, you might train for 25-50 epochs\n\nEPOCHS = 20  # Adjust based on your needs\n\nprint(\"Starting training...\")\nprint(\"=\"*60)\nprint(f\"Epochs: {EPOCHS}\")\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {val_generator.samples}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(\"\\nThis may take a while depending on your hardware...\")\nprint(\"(GPU: ~2-5 min, CPU: ~20-30 min)\")\nprint(\"=\"*60)\n\n# Train!\nhistory = simple_cnn.fit(\n    train_generator,\n    epochs=EPOCHS,\n    validation_data=val_generator,\n    callbacks=[early_stop, checkpoint, reduce_lr],\n    verbose=1\n)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "92tubxf8mgq",
   "source": "# Plot training history\ndef plot_training_history(history):\n    \"\"\"Plot training and validation accuracy/loss curves.\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Accuracy curves\n    axes[0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Accuracy')\n    axes[0].set_title('Model Accuracy Over Time', fontweight='bold')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    # Loss curves\n    axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n    axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Loss')\n    axes[1].set_title('Model Loss Over Time', fontweight='bold')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_training_history(history)\n\nprint(\"\\nInterpretation:\")\nprint(\"=\"*60)\nprint(\"\\nLEFT (Accuracy):\")\nprint(\"  - Training accuracy (blue): How well model fits training data\")\nprint(\"  - Validation accuracy (orange): How well model generalizes\")\nprint(\"  - Goal: Both should increase and converge\")\nprint(\"\\nRIGHT (Loss):\")\nprint(\"  - Training loss (blue): Error on training data\")\nprint(\"  - Validation loss (orange): Error on validation data\")\nprint(\"  - Goal: Both should decrease and converge\")\nprint(\"\\nSigns of overfitting:\")\nprint(\"  - Training accuracy much higher than validation accuracy\")\nprint(\"  - Validation loss starts increasing while training loss decreases\")\nprint(\"\\nSigns of good training:\")\nprint(\"  - Both curves improve together\")\nprint(\"  - Small gap between training and validation metrics\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7w4ei0ojiil",
   "source": "---\n\n<a id='part8'></a>\n# Part 8: Model Evaluation\n\nNow let's evaluate our trained model on the test set!\n\n## Evaluation Metrics for Binary Classification\n\n| Metric | Formula | Meaning |\n|--------|---------|---------|\n| **Accuracy** | (TP + TN) / Total | Overall correctness |\n| **Precision** | TP / (TP + FP) | Of predicted cats, how many are actually cats? |\n| **Recall** | TP / (TP + FN) | Of actual cats, how many did we find? |\n| **F1-Score** | 2 × (P × R) / (P + R) | Harmonic mean of precision and recall |\n\n## Confusion Matrix\n\nShows the breakdown of predictions:\n\n```\n                 Predicted\n             Cat        Dog\nActual Cat  [TP]       [FN]\n       Dog  [FP]       [TN]\n```\n\n- **TP (True Positive)**: Correctly predicted cat\n- **TN (True Negative)**: Correctly predicted dog\n- **FP (False Positive)**: Predicted cat, but actually dog\n- **FN (False Negative)**: Predicted dog, but actually cat",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "umh09dcfi1",
   "source": "# Evaluate on validation set\nval_loss, val_accuracy = simple_cnn.evaluate(val_generator, verbose=0)\n\nprint(\"Validation Performance\")\nprint(\"=\"*60)\nprint(f\"Validation Loss: {val_loss:.4f}\")\nprint(f\"Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n\n# Get predictions for confusion matrix\nval_generator.reset()\npredictions = simple_cnn.predict(val_generator, verbose=1)\npredicted_classes = (predictions > 0.5).astype(int).flatten()\ntrue_classes = val_generator.classes\n\n# Calculate metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CLASSIFICATION REPORT\")\nprint(\"=\"*60)\nprint(classification_report(true_classes, predicted_classes, \n                           target_names=['Cat', 'Dog']))\n\n# Confusion matrix\ncm = confusion_matrix(true_classes, predicted_classes)\nprint(\"\\n\" + \"=\"*60)\nprint(\"CONFUSION MATRIX\")\nprint(\"=\"*60)\nprint(cm)\nprint(\"\\nInterpretation:\")\nprint(f\"  True Cats correctly identified: {cm[0,0]}\")\nprint(f\"  True Cats misclassified as Dogs: {cm[0,1]}\")\nprint(f\"  True Dogs misclassified as Cats: {cm[1,0]}\")\nprint(f\"  True Dogs correctly identified: {cm[1,1]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8hfada9njxr",
   "source": "# Visualize confusion matrix\nfig, ax = plt.subplots(figsize=(8, 6))\n\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n            xticklabels=['Cat', 'Dog'], yticklabels=['Cat', 'Dog'],\n            annot_kws={'size': 16, 'weight': 'bold'})\n\nax.set_xlabel('Predicted Label', fontsize=12)\nax.set_ylabel('True Label', fontsize=12)\nax.set_title('Confusion Matrix - Simple CNN', fontweight='bold', fontsize=14)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nHow to Read:\")\nprint(\"- Diagonal (top-left to bottom-right): Correct predictions\")\nprint(\"- Off-diagonal: Misclassifications\")\nprint(\"- Darker blue = higher count\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fgocs5ytx95",
   "source": "## 8.1 Visualizing Misclassified Examples\n\nLet's see where our model makes mistakes - this helps us understand its limitations!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "f89fp7mmqcu",
   "source": "# Find misclassified images\nmisclassified_indices = np.where(predicted_classes != true_classes)[0]\nclass_names = list(val_generator.class_indices.keys())\n\nprint(f\"Total misclassified images: {len(misclassified_indices)}\")\nprint(f\"Misclassification rate: {len(misclassified_indices)/len(true_classes)*100:.2f}%\")\n\n# Visualize some misclassified examples\nif len(misclassified_indices) > 0:\n    num_examples = min(8, len(misclassified_indices))\n    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n    fig.suptitle('Misclassified Examples', fontweight='bold', fontsize=16)\n    \n    # Get image files\n    val_generator.reset()\n    filenames = val_generator.filenames\n    \n    for i, ax in enumerate(axes.flat):\n        if i < num_examples:\n            idx = misclassified_indices[i]\n            img_path = os.path.join(BASE_DIR, filenames[idx])\n            \n            try:\n                img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n                ax.imshow(img)\n                \n                true_label = class_names[true_classes[idx]]\n                pred_label = class_names[predicted_classes[idx]]\n                confidence = predictions[idx][0] if predicted_classes[idx] == 1 else 1 - predictions[idx][0]\n                \n                ax.set_title(f'True: {true_label}\\\\nPredicted: {pred_label}\\\\n(Conf: {confidence:.2%})',\n                           fontsize=10)\n                ax.axis('off')\n            except:\n                ax.axis('off')\n        else:\n            ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nWhy misclassifications happen:\")\n    print(\"- Unusual poses or angles\")\n    print(\"- Poor lighting or image quality\")\n    print(\"- Partial views of the animal\")\n    print(\"- Multiple animals in one image\")\n    print(\"- Breed characteristics that confuse the model\")\nelse:\n    print(\"\\nNo misclassifications! Perfect model (unlikely in real scenarios).\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mdfiawn6xq",
   "source": "---\n\n<a id='part9'></a>\n# Part 9: Transfer Learning with VGG16\n\n## What is Transfer Learning?\n\n**Transfer Learning** = Using a pre-trained model as a starting point for your task.\n\nInstead of training from scratch, we use a model that was already trained on millions of images!\n\n### The Analogy\n\nImagine learning to drive a car:\n\n| From Scratch | Transfer Learning |\n|--------------|-------------------|\n| Learn basic physics | Already know how vehicles work |\n| Learn traffic rules | Already know traffic rules |\n| Practice driving | Just learn this specific car! |\n\n## How Transfer Learning Works\n\n1. **Take a pre-trained model** (trained on ImageNet - 1.4M images, 1000 classes)\n2. **Remove the top layers** (classifier specific to ImageNet)\n3. **Freeze the base** (keep learned features like edges, textures, shapes)\n4. **Add custom classifier** (for our task: Cat vs Dog)\n5. **Train only new layers** (much faster!)\n\n## Why Transfer Learning?\n\n| Benefit | Explanation |\n|---------|-------------|\n| **Less data needed** | Pre-trained model already knows general features |\n| **Faster training** | Only train classifier, not entire network |\n| **Better performance** | Leverages knowledge from millions of images |\n| **Less compute** | No need for powerful GPUs for weeks |\n\n## Popular Pre-trained Models\n\n| Model | Parameters | ImageNet Top-1 Accuracy | Use Case |\n|-------|------------|------------------------|----------|\n| **VGG16** | 138M | 71.3% | Good general features, easy to use |\n| **ResNet50** | 25M | 76.1% | Good balance of size and accuracy |\n| **InceptionV3** | 24M | 77.9% | Multi-scale features |\n| **MobileNet** | 4M | 70.4% | Lightweight, for mobile devices |\n\nWe'll use **VGG16** because it's simple and effective!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "r8nfkaq5dw",
   "source": "# Build Transfer Learning model with VGG16\ndef build_vgg16_model(input_shape=(150, 150, 3)):\n    \"\"\"\n    Build a transfer learning model using VGG16 as base.\n    \n    Parameters:\n    -----------\n    input_shape : tuple\n        Shape of input images\n    \n    Returns:\n    --------\n    model : keras.Model\n        Transfer learning model\n    \"\"\"\n    # Load VGG16 pre-trained on ImageNet (without top classification layers)\n    base_model = VGG16(\n        weights='imagenet',      # Use ImageNet pre-trained weights\n        include_top=False,       # Exclude final dense layers\n        input_shape=input_shape\n    )\n    \n    # Freeze the base model (don't train these layers)\n    base_model.trainable = False\n    \n    # Build custom classifier on top\n    model = Sequential([\n        base_model,\n        Flatten(),\n        Dense(256, activation='relu'),\n        Dropout(0.5),\n        Dense(1, activation='sigmoid')  # Binary classification\n    ])\n    \n    return model, base_model\n\nprint(\"Building VGG16 Transfer Learning Model...\")\nprint(\"=\"*60)\n\nvgg_model, vgg_base = build_vgg16_model()\n\nprint(\"Model created successfully!\")\nprint(f\"\\nBase model (VGG16) layers: {len(vgg_base.layers)}\")\nprint(f\"Base model trainable: {vgg_base.trainable}\")\nprint(\"\\nAll VGG16 layers are FROZEN (won't be trained)\")\nprint(\"Only our custom classifier layers will be trained!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "i6t0lcjvxla",
   "source": "# Model summary\nvgg_model.summary()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"KEY OBSERVATIONS\")\nprint(\"=\"*60)\nprint(\"\\n1. VGG16 Base:\")\nprint(\"   - 14.7M parameters (all pre-trained on ImageNet)\")\nprint(\"   - These parameters are FROZEN (not trainable)\")\nprint(\"\\n2. Our Custom Classifier:\")\nprint(\"   - Only a few thousand parameters to train\")\nprint(\"   - Much faster than training from scratch!\")\nprint(\"\\n3. Total Parameters:\")\nprint(\"   - Millions of parameters, but only training a fraction\")\nprint(\"   - This is the power of transfer learning!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8jgwyy0m80q",
   "source": "# Compile and train VGG16 model\nvgg_model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nprint(\"VGG16 model compiled!\")\nprint(\"\\nStarting training (Transfer Learning)...\")\nprint(\"=\"*60)\nprint(\"Note: This should be FASTER than training from scratch\")\nprint(\"      We're only training the classifier, not the entire network!\")\nprint(\"=\"*60)\n\n# Train (fewer epochs needed due to transfer learning)\nvgg_history = vgg_model.fit(\n    train_generator,\n    epochs=10,  # Fewer epochs needed!\n    validation_data=val_generator,\n    callbacks=[early_stop, checkpoint, reduce_lr],\n    verbose=1\n)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"VGG16 TRANSFER LEARNING COMPLETE!\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "l94ddwpux2d",
   "source": "# Plot VGG16 training history\nplot_training_history(vgg_history)\n\nprint(\"\\nCompare with Simple CNN:\")\nprint(\"- Transfer learning often converges faster\")\nprint(\"- May achieve higher accuracy with fewer epochs\")\nprint(\"- Less prone to overfitting (pre-trained features are robust)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "yagk1k6nuj7",
   "source": "---\n\n<a id='part10'></a>\n# Part 10: Model Comparison\n\nLet's compare our Simple CNN vs VGG16 Transfer Learning!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "qrtpfp3q41d",
   "source": "# Compare models side by side\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Accuracy comparison\naxes[0, 0].plot(history.history['accuracy'], label='Simple CNN (Train)', linewidth=2)\naxes[0, 0].plot(vgg_history.history['accuracy'], label='VGG16 (Train)', linewidth=2, linestyle='--')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Accuracy')\naxes[0, 0].set_title('Training Accuracy Comparison', fontweight='bold')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].plot(history.history['val_accuracy'], label='Simple CNN (Val)', linewidth=2)\naxes[0, 1].plot(vgg_history.history['val_accuracy'], label='VGG16 (Val)', linewidth=2, linestyle='--')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('Accuracy')\naxes[0, 1].set_title('Validation Accuracy Comparison', fontweight='bold')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Loss comparison\naxes[1, 0].plot(history.history['loss'], label='Simple CNN (Train)', linewidth=2)\naxes[1, 0].plot(vgg_history.history['loss'], label='VGG16 (Train)', linewidth=2, linestyle='--')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('Loss')\naxes[1, 0].set_title('Training Loss Comparison', fontweight='bold')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].plot(history.history['val_loss'], label='Simple CNN (Val)', linewidth=2)\naxes[1, 1].plot(vgg_history.history['val_loss'], label='VGG16 (Val)', linewidth=2, linestyle='--')\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].set_ylabel('Loss')\naxes[1, 1].set_title('Validation Loss Comparison', fontweight='bold')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey Insights:\")\nprint(\"=\"*60)\nprint(\"Note: Epoch counts differ, so compare final performance:\")\nprint(f\"\\nSimple CNN:\")\nprint(f\"  Final train accuracy: {history.history['accuracy'][-1]:.4f}\")\nprint(f\"  Final val accuracy: {history.history['val_accuracy'][-1]:.4f}\")\nprint(f\"\\nVGG16 Transfer Learning:\")\nprint(f\"  Final train accuracy: {vgg_history.history['accuracy'][-1]:.4f}\")\nprint(f\"  Final val accuracy: {vgg_history.history['val_accuracy'][-1]:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "y7ufd8td4ss",
   "source": "# Create comparison table\ncomparison_data = {\n    'Model': ['Simple CNN', 'VGG16 Transfer Learning'],\n    'Parameters': [simple_cnn.count_params(), vgg_model.count_params()],\n    'Trainable Params': [\n        sum([tf.size(w).numpy() for w in simple_cnn.trainable_weights]),\n        sum([tf.size(w).numpy() for w in vgg_model.trainable_weights])\n    ],\n    'Train Accuracy': [\n        f\"{history.history['accuracy'][-1]:.4f}\",\n        f\"{vgg_history.history['accuracy'][-1]:.4f}\"\n    ],\n    'Val Accuracy': [\n        f\"{history.history['val_accuracy'][-1]:.4f}\",\n        f\"{vgg_history.history['val_accuracy'][-1]:.4f}\"\n    ],\n    'Training Time': ['Baseline', 'Similar or faster']\n}\n\ncomparison_df = pd.DataFrame(comparison_data)\n\nprint(\"\\nMODEL COMPARISON TABLE\")\nprint(\"=\"*80)\nprint(comparison_df.to_string(index=False))\nprint(\"=\"*80)\n\nprint(\"\\nConclusions:\")\nprint(\"1. Transfer Learning (VGG16):\")\nprint(\"   - More total parameters, but most are pre-trained (frozen)\")\nprint(\"   - Often achieves higher accuracy with less training\")\nprint(\"   - Great for small datasets\")\nprint(\"\\n2. Simple CNN:\")\nprint(\"   - Fewer parameters, trains everything from scratch\")\nprint(\"   - Good for understanding CNN fundamentals\")\nprint(\"   - May need more data and training time\")\nprint(\"\\n3. When to use each:\")\nprint(\"   - Use Transfer Learning: Limited data, want quick results\")\nprint(\"   - Build from Scratch: Lots of data, very specific domain\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5nfo8i67x8",
   "source": "---\n\n<a id='part11'></a>\n# Part 11: Visualizing What CNNs Learn\n\nOne of the most fascinating aspects of CNNs is visualizing what they actually learn!\n\n## What Do Different Layers Learn?\n\n| Layer | What It Detects | Example |\n|-------|-----------------|---------|\n| **First Conv Layer** | Basic features (edges, colors, textures) | Horizontal/vertical edges |\n| **Second Conv Layer** | Simple patterns (corners, circles) | Curves, basic shapes |\n| **Third Conv Layer** | Complex patterns (parts) | Eyes, ears, whiskers |\n| **Deeper Layers** | High-level features (whole objects) | Cat face, dog body |\n\n## Visualization Techniques\n\n1. **Filter Visualization**: See the filters (kernels) the network learned\n2. **Feature Maps**: See what each layer activates on for a specific image\n3. **Grad-CAM**: Heatmap showing which parts of image the network focuses on\n\nLet's visualize filters from our Simple CNN!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tujy31jo3zi",
   "source": "# Visualize filters from first convolutional layer\ndef visualize_filters(model, layer_name, num_filters=32):\n    \"\"\"Visualize filters from a convolutional layer.\"\"\"\n    # Get the layer\n    layer = model.get_layer(layer_name)\n    filters = layer.get_weights()[0]  # Shape: (height, width, channels, num_filters)\n    \n    # Normalize filter values to 0-1\n    f_min, f_max = filters.min(), filters.max()\n    filters = (filters - f_min) / (f_max - f_min)\n    \n    # Plot filters\n    n_filters = min(num_filters, filters.shape[3])\n    n_cols = 8\n    n_rows = n_filters // n_cols + (1 if n_filters % n_cols else 0)\n    \n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 2))\n    fig.suptitle(f'Filters from {layer_name}', fontweight='bold', fontsize=16)\n    \n    for i in range(n_rows * n_cols):\n        row = i // n_cols\n        col = i % n_cols\n        ax = axes[row, col] if n_rows > 1 else axes[col]\n        \n        if i < n_filters:\n            # Get filter (handle 3-channel input for first layer)\n            filter_img = filters[:, :, :, i]\n            if filter_img.shape[2] == 3:  # RGB\n                ax.imshow(filter_img)\n            else:  # Grayscale\n                ax.imshow(filter_img[:, :, 0], cmap='gray')\n            ax.set_title(f'Filter {i+1}', fontsize=9)\n        ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\nprint(\"Visualizing filters from first convolutional layer...\")\nvisualize_filters(simple_cnn, 'conv1', num_filters=32)\n\nprint(\"\\nWhat you're seeing:\")\nprint(\"- Each small square is a LEARNED FILTER (3x3 kernel)\")\nprint(\"- These filters detect basic features like:\")\nprint(\"  - Edges (vertical, horizontal, diagonal)\")\nprint(\"  - Color patterns (red, green, blue combinations)\")\nprint(\"  - Textures (rough, smooth)\")\nprint(\"\\nThe network LEARNED these automatically from data!\")\nprint(\"We didn't tell it to look for edges - it discovered that on its own!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ccja8o2xbnv",
   "source": "## 11.1 Visualizing Feature Maps (Activations)\n\nLet's see how each layer responds to a specific image!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5whq0i2rjif",
   "source": "# Visualize feature maps for a sample image\ndef visualize_feature_maps(model, img_array, layer_names):\n    \"\"\"\n    Visualize activations (feature maps) from specified layers.\n    \n    Parameters:\n    -----------\n    model : keras.Model\n        The CNN model\n    img_array : numpy array\n        Input image (already preprocessed)\n    layer_names : list\n        Names of layers to visualize\n    \"\"\"\n    # Create a model that outputs activations from specified layers\n    layer_outputs = [model.get_layer(name).output for name in layer_names]\n    activation_model = Model(inputs=model.input, outputs=layer_outputs)\n    \n    # Get activations\n    activations = activation_model.predict(img_array)\n    \n    # Plot feature maps for each layer\n    for layer_name, activation in zip(layer_names, activations):\n        n_features = activation.shape[-1]\n        size = activation.shape[1]\n        \n        # Display up to 16 feature maps\n        n_cols = 8\n        n_rows = min(2, n_features // n_cols)\n        \n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 2))\n        fig.suptitle(f'Feature Maps from {layer_name} ({size}x{size})', \n                     fontweight='bold', fontsize=14)\n        \n        for i in range(n_rows * n_cols):\n            row = i // n_cols\n            col = i % n_cols\n            ax = axes[row, col] if n_rows > 1 else axes[col]\n            \n            if i < n_features:\n                ax.imshow(activation[0, :, :, i], cmap='viridis')\n                ax.set_title(f'Filter {i+1}', fontsize=9)\n            ax.axis('off')\n        \n        plt.tight_layout()\n        plt.show()\n\n# Load a test image\ntest_img_path = os.path.join(CAT_DIR, [f for f in os.listdir(CAT_DIR) if f.endswith('.jpg')][10])\ntest_img = load_img(test_img_path, target_size=(150, 150))\ntest_img_array = img_to_array(test_img) / 255.0\ntest_img_array = np.expand_dims(test_img_array, axis=0)\n\n# Display original image\nplt.figure(figsize=(5, 5))\nplt.imshow(test_img)\nplt.title('Original Image', fontweight='bold')\nplt.axis('off')\nplt.show()\n\nprint(\"Visualizing feature maps from each convolutional layer...\")\nprint(\"=\"*60)\n\n# Visualize activations from all conv layers\nvisualize_feature_maps(simple_cnn, test_img_array, ['conv1', 'conv2', 'conv3'])\n\nprint(\"\\nKey Observations:\")\nprint(\"=\"*60)\nprint(\"1. FIRST LAYER (conv1):\")\nprint(\"   - Feature maps show edge detection\")\nprint(\"   - Different filters detect different orientations\")\nprint(\"   - Still recognizable as original image\")\nprint(\"\\n2. SECOND LAYER (conv2):\")\nprint(\"   - More abstract features (combinations of edges)\")\nprint(\"   - Less recognizable as original image\")\nprint(\"   - Detects patterns like fur texture, eye shapes\")\nprint(\"\\n3. THIRD LAYER (conv3):\")\nprint(\"   - Very abstract, high-level features\")\nprint(\"   - Barely recognizable\")\nprint(\"   - Represents complex patterns specific to cats/dogs\")\nprint(\"\\nThis HIERARCHY of features is why CNNs work so well!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7io2fiymes3",
   "source": "---\n\n<a id='part12'></a>\n# Part 12: Making Predictions on New Images\n\nLet's use our trained model to predict whether new images are cats or dogs!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "sieetkqimvd",
   "source": "# Function to predict and visualize\ndef predict_image(model, img_path, class_indices):\n    \"\"\"\n    Make prediction on a single image and display result.\n    \n    Parameters:\n    -----------\n    model : keras.Model\n        Trained model\n    img_path : str\n        Path to image file\n    class_indices : dict\n        Dictionary mapping class names to indices\n    \"\"\"\n    # Load and preprocess image\n    img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n    img_array = img_to_array(img) / 255.0\n    img_array = np.expand_dims(img_array, axis=0)\n    \n    # Make prediction\n    prediction = model.predict(img_array, verbose=0)[0][0]\n    \n    # Get class names\n    class_names = {v: k for k, v in class_indices.items()}\n    \n    # Determine predicted class\n    if prediction > 0.5:\n        predicted_class = class_names[1]\n        confidence = prediction\n    else:\n        predicted_class = class_names[0]\n        confidence = 1 - prediction\n    \n    return img, predicted_class, confidence\n\n# Predict on random samples\nprint(\"Making Predictions on Random Images\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\nfig.suptitle('Predictions with Confidence Scores', fontweight='bold', fontsize=16)\n\n# Get random images (mix of cats and dogs)\ncat_files = [f for f in os.listdir(CAT_DIR) if f.endswith('.jpg')]\ndog_files = [f for f in os.listdir(DOG_DIR) if f.endswith('.jpg')]\n\nnp.random.seed(42)\nsample_images = (\n    [os.path.join(CAT_DIR, f) for f in np.random.choice(cat_files, 4, replace=False)] +\n    [os.path.join(DOG_DIR, f) for f in np.random.choice(dog_files, 4, replace=False)]\n)\n\nfor i, ax in enumerate(axes.flat):\n    try:\n        img, pred_class, confidence = predict_image(\n            simple_cnn, sample_images[i], train_generator.class_indices\n        )\n        \n        ax.imshow(img)\n        \n        # Color code: green for high confidence, yellow for medium, red for low\n        if confidence > 0.9:\n            color = 'green'\n        elif confidence > 0.7:\n            color = 'orange'\n        else:\n            color = 'red'\n        \n        ax.set_title(f'{pred_class.upper()}\\\\nConfidence: {confidence:.1%}',\n                    fontweight='bold', color=color, fontsize=12)\n        ax.axis('off')\n    except:\n        ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nColor Code:\")\nprint(\"  GREEN: High confidence (>90%)\")\nprint(\"  ORANGE: Medium confidence (70-90%)\")\nprint(\"  RED: Low confidence (<70%)\")\nprint(\"\\nNote: Low confidence predictions are more likely to be wrong!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2hm2ng4yban",
   "source": "---\n\n<a id='part13'></a>\n# Part 13: Summary and Key Takeaways\n\n## What We Accomplished\n\nIn this comprehensive notebook, we:\n\n1. Understood CNN fundamentals\n2. Explored the Cats vs Dogs dataset\n3. Preprocessed images (resizing, normalization)\n4. Applied data augmentation\n5. Built a CNN from scratch\n6. Implemented transfer learning with VGG16\n7. Evaluated and compared models\n8. Visualized what CNNs learn\n9. Made predictions on new images\n\n## Key Concepts Learned\n\n| Concept | Key Takeaway |\n|---------|--------------|\n| **CNNs** | Automatically learn hierarchical features from images |\n| **Convolution** | Extract local patterns using sliding filters |\n| **Pooling** | Reduce spatial dimensions, provide translation invariance |\n| **Data Augmentation** | Artificially increase data diversity to prevent overfitting |\n| **Transfer Learning** | Leverage pre-trained models for better performance |\n| **Feature Hierarchy** | Early layers detect edges, deeper layers detect complex patterns |\n\n## Performance Summary\n\nOur models achieved competitive performance on the Cats vs Dogs classification task!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "05ilpqn8qon9",
   "source": "# Create final summary dashboard\nfig = plt.figure(figsize=(18, 12))\ngs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n\n# 1. Training curves comparison\nax1 = fig.add_subplot(gs[0, :])\nax1.plot(history.history['val_accuracy'], label='Simple CNN', linewidth=2, marker='o')\nax1.plot(vgg_history.history['val_accuracy'], label='VGG16', linewidth=2, marker='s')\nax1.set_xlabel('Epoch', fontsize=11)\nax1.set_ylabel('Validation Accuracy', fontsize=11)\nax1.set_title('Model Comparison: Validation Accuracy Over Time', fontweight='bold', fontsize=13)\nax1.legend(fontsize=11)\nax1.grid(True, alpha=0.3)\n\n# 2. Confusion Matrix - Simple CNN\nax2 = fig.add_subplot(gs[1, 0])\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2, cbar=False,\n            xticklabels=['Cat', 'Dog'], yticklabels=['Cat', 'Dog'])\nax2.set_title('Simple CNN\\\\nConfusion Matrix', fontweight='bold', fontsize=11)\n\n# 3. Sample predictions\nax3 = fig.add_subplot(gs[1, 1])\n# Create a simple visualization\nsample_img, pred_class, confidence = predict_image(\n    simple_cnn, sample_images[0], train_generator.class_indices\n)\nax3.imshow(sample_img)\nax3.set_title(f'Sample Prediction\\\\n{pred_class.upper()} ({confidence:.1%})', \n             fontweight='bold', fontsize=11)\nax3.axis('off')\n\n# 4. Key metrics table\nax4 = fig.add_subplot(gs[1, 2])\nax4.axis('off')\nmetrics_text = f\"\"\"\nFINAL METRICS\n\nSimple CNN:\n  Accuracy: {history.history['val_accuracy'][-1]:.2%}\n  Parameters: {simple_cnn.count_params():,}\n  \nVGG16 Transfer:\n  Accuracy: {vgg_history.history['val_accuracy'][-1]:.2%}\n  Total Params: {vgg_model.count_params():,}\n  Trainable: {sum([tf.size(w).numpy() for w in vgg_model.trainable_weights]):,}\n\nDataset:\n  Training: {train_generator.samples:,}\n  Validation: {val_generator.samples:,}\n\"\"\"\nax4.text(0.1, 0.5, metrics_text, fontsize=10, verticalalignment='center',\n        family='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n\n# 5. Data augmentation example\nax5 = fig.add_subplot(gs[2, 0])\naug_img = next(augmented_datagen.flow(sample_array, batch_size=1))[0]\nax5.imshow(aug_img)\nax5.set_title('Data Augmentation\\\\nExample', fontweight='bold', fontsize=11)\nax5.axis('off')\n\n# 6. First layer filters\nax6 = fig.add_subplot(gs[2, 1])\nlayer = simple_cnn.get_layer('conv1')\nfilters = layer.get_weights()[0]\nf_min, f_max = filters.min(), filters.max()\nfilters_norm = (filters - f_min) / (f_max - f_min)\n# Show one filter\nax6.imshow(filters_norm[:, :, :, 0])\nax6.set_title('Learned Filter\\\\n(Conv Layer 1)', fontweight='bold', fontsize=11)\nax6.axis('off')\n\n# 7. Architecture comparison\nax7 = fig.add_subplot(gs[2, 2])\nax7.axis('off')\narch_comparison = \"\"\"\nARCHITECTURE\n\nSimple CNN:\n  Conv(32) → Pool\n  Conv(64) → Pool  \n  Conv(128) → Pool\n  Dense(512) → Dropout\n  Dense(1) - Sigmoid\n\nVGG16 Transfer:\n  VGG16 Base (FROZEN)\n  Dense(256)\n  Dropout(0.5)\n  Dense(1) - Sigmoid\n\"\"\"\nax7.text(0.1, 0.5, arch_comparison, fontsize=9, verticalalignment='center',\n        family='monospace', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n\nplt.suptitle('CATS VS DOGS CNN - COMPREHENSIVE SUMMARY DASHBOARD', \n            fontweight='bold', fontsize=16, y=0.98)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"NOTEBOOK COMPLETE!\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "098nrlec0u0j",
   "source": "## Key Takeaways Table\n\n| Topic | What We Learned |\n|-------|----------------|\n| **CNNs are Hierarchical** | Early layers detect simple features (edges), deeper layers detect complex patterns (cat faces) |\n| **Convolution > Fully Connected** | Shared weights reduce parameters massively, spatial structure is preserved |\n| **Pooling is Essential** | Reduces dimensions, provides translation invariance, prevents overfitting |\n| **Data Augmentation Helps** | Artificially increases dataset size, makes model robust to variations |\n| **Transfer Learning is Powerful** | Pre-trained models give huge advantage with limited data |\n| **Dropout Prevents Overfitting** | Randomly disabling neurons forces network to learn robust features |\n| **Visualization is Insightful** | We can see what CNNs learn - not a black box! |\n\n## When to Use What\n\n| Scenario | Recommendation |\n|----------|---------------|\n| **Large dataset (>100k images)** | Train from scratch or fine-tune all layers |\n| **Small dataset (<10k images)** | Use transfer learning, freeze base layers |\n| **Very limited data (<1k images)** | Strong augmentation + transfer learning |\n| **Custom domain (medical, satellite)** | May need domain-specific pre-training |\n| **Mobile/Edge deployment** | Use lightweight models (MobileNet, SqueezeNet) |\n| **High accuracy critical** | Ensemble multiple models, use state-of-the-art architectures |\n\n## What's Next?\n\nWant to improve further? Try:\n\n1. **More Data**: Use the full 25k images (we used a subset)\n2. **Advanced Architectures**: ResNet, EfficientNet, Vision Transformers\n3. **Fine-tuning**: Unfreeze some VGG16 layers and train end-to-end\n4. **Ensembling**: Combine predictions from multiple models\n5. **Test-Time Augmentation**: Augment test images and average predictions\n6. **Grad-CAM**: Visualize which regions drive predictions\n7. **Object Detection**: Detect and locate multiple objects in images (YOLO, Faster R-CNN)\n\n## Real-World Applications\n\nThese techniques apply to:\n\n- **Medical Imaging**: Detect diseases from X-rays, MRIs, CT scans\n- **Autonomous Vehicles**: Recognize pedestrians, traffic signs, lane markings\n- **Security**: Face recognition, anomaly detection\n- **Agriculture**: Crop disease detection, weed identification\n- **Retail**: Product recognition, visual search\n- **Wildlife**: Species identification, population monitoring\n\n## Resources for Further Learning\n\n- **Papers**: \n  - AlexNet (2012) - Started the deep learning revolution\n  - VGGNet (2014) - Simple, effective architecture\n  - ResNet (2015) - Skip connections, very deep networks\n  - EfficientNet (2019) - Optimized scaling\n\n- **Courses**:\n  - Stanford CS231n: Convolutional Neural Networks\n  - Fast.ai: Practical Deep Learning for Coders\n  - Coursera: Deep Learning Specialization\n\n- **Practice**:\n  - Kaggle competitions\n  - ImageNet dataset\n  - COCO dataset for object detection",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "capbifq5x3n",
   "source": "# Final summary printout\nprint(\"=\"*70)\nprint(\"CATS VS DOGS CNN IMAGE CLASSIFICATION - COMPLETED!\")\nprint(\"=\"*70)\n\nprint(\"\\n WHAT WE BUILT:\")\nprint(\"  1. Simple CNN from scratch (3 conv blocks)\")\nprint(\"  2. VGG16 Transfer Learning model\")\nprint(\"  3. Complete image classification pipeline\")\nprint(\"  4. Comprehensive visualizations\")\n\nprint(\"\\n TECHNIQUES MASTERED:\")\nprint(\"  - Convolutional Neural Networks architecture\")\nprint(\"  - Image preprocessing and normalization\")\nprint(\"  - Data augmentation for robustness\")\nprint(\"  - Transfer learning with pre-trained models\")\nprint(\"  - Model evaluation and comparison\")\nprint(\"  - Feature visualization\")\n\nprint(\"\\n KEY INSIGHTS:\")\nprint(\"  - CNNs learn hierarchical features automatically\")\nprint(\"  - Data augmentation prevents overfitting\")\nprint(\"  - Transfer learning is powerful for limited data\")\nprint(\"  - Visualization helps understand what models learn\")\n\nprint(\"\\n PERFORMANCE:\")\nprint(f\"  Simple CNN: ~{history.history['val_accuracy'][-1]*100:.1f}% accuracy\")\nprint(f\"  VGG16: ~{vgg_history.history['val_accuracy'][-1]*100:.1f}% accuracy\")\nprint(f\"  (State-of-the-art on this dataset: 98-99%)\")\n\nprint(\"\\n SKILLS GAINED:\")\nprint(\"  Image classification\")\nprint(\"  Deep learning\")\nprint(\"  Computer vision\")\nprint(\"  TensorFlow/Keras\")\nprint(\"  Model evaluation\")\nprint(\"  Transfer learning\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Thank you for completing this comprehensive CNN tutorial!\")\nprint(\"You now have a solid foundation in image classification.\")\nprint(\"=\"*70)\n\nprint(\"\\n Next Steps:\")\nprint(\"  1. Try this on other image datasets\")\nprint(\"  2. Experiment with different architectures\")\nprint(\"  3. Explore object detection (YOLO, Faster R-CNN)\")\nprint(\"  4. Learn about semantic segmentation (U-Net)\")\nprint(\"  5. Study advanced topics (GANs, Vision Transformers)\")\n\nprint(\"\\n Happy Learning!\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}