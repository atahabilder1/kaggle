{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Predictor\n",
    "\n",
    "**Author:** Anik Tahabilder  \n",
    "**Project:** 9 of 22 - Kaggle ML Portfolio  \n",
    "**Dataset:** Yahoo Finance Stock Data  \n",
    "**Difficulty:** 6/10 | **Learning Value:** 8/10\n",
    "\n",
    "---\n",
    "\n",
    "## What is Time Series Forecasting?\n",
    "\n",
    "**Time Series Forecasting** is a technique used to predict future values based on historical data points collected over time. Unlike regular ML where data points are independent, time series data has **temporal dependencies** - what happened yesterday affects today!\n",
    "\n",
    "### Time Series vs Regular ML:\n",
    "\n",
    "| Aspect | Regular ML | Time Series ML |\n",
    "|--------|-----------|----------------|\n",
    "| **Data Order** | Random order OK | Order matters! |\n",
    "| **Train/Test Split** | Random split | Chronological split |\n",
    "| **Independence** | Samples are independent | Samples are dependent |\n",
    "| **Features** | Static features | Lag features, rolling stats |\n",
    "| **Validation** | K-Fold CV | Time Series CV |\n",
    "\n",
    "### Why Stock Prediction is Challenging:\n",
    "\n",
    "| Challenge | Description | Impact |\n",
    "|-----------|-------------|--------|\n",
    "| **Non-Stationary** | Mean/variance change over time | Models trained on old patterns may fail |\n",
    "| **High Noise** | Random fluctuations dominate | Hard to separate signal from noise |\n",
    "| **External Factors** | News, earnings, politics | Unpredictable events move prices |\n",
    "| **Efficient Market Hypothesis** | Prices reflect all known info | True patterns are already priced in |\n",
    "| **Regime Changes** | Bull/bear markets behave differently | One model may not fit all periods |\n",
    "\n",
    "### Types of Time Series Models:\n",
    "\n",
    "| Model Type | Examples | Best For | Limitations |\n",
    "|------------|----------|----------|-------------|\n",
    "| **Statistical** | ARIMA, SARIMA, ETS | Linear trends, seasonality | Can't capture complex patterns |\n",
    "| **Machine Learning** | RF, XGBoost, SVR | Non-linear patterns, many features | Ignores temporal order |\n",
    "| **Deep Learning** | LSTM, GRU, Transformer | Long-term dependencies | Needs lots of data, overfits easily |\n",
    "| **Hybrid** | ML + Technical Indicators | Best of both worlds | Requires domain knowledge |\n",
    "\n",
    "### Our Approach: ML with Technical Indicators\n",
    "\n",
    "```\n",
    "Raw OHLCV Data \u2192 Technical Indicators \u2192 Lag Features \u2192 ML Models \u2192 Next Day Price\n",
    "```\n",
    "\n",
    "**Why this approach?**\n",
    "1. Technical indicators encode domain knowledge (what traders look at)\n",
    "2. Lag features capture autocorrelation\n",
    "3. ML models find non-linear patterns\n",
    "4. More interpretable than deep learning\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: Setup and Data Loading](#part1)\n",
    "2. [Part 2: Understanding Stock Data](#part2)\n",
    "3. [Part 3: Exploratory Data Analysis](#part3)\n",
    "4. [Part 4: Technical Indicators (Feature Engineering)](#part4)\n",
    "5. [Part 5: Data Preprocessing](#part5)\n",
    "6. [Part 6: Model Selection & Training](#part6)\n",
    "7. [Part 7: Model Evaluation & Comparison](#part7)\n",
    "8. [Part 8: Hyperparameter Tuning](#part8)\n",
    "9. [Part 9: Final Predictions & Visualization](#part9)\n",
    "10. [Part 10: Summary and Conclusions](#part10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part1'></a>\n",
    "# Part 1: Setup and Data Loading\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Required Libraries\n",
    "\n",
    "| Library | Purpose | Why We Need It |\n",
    "|---------|---------|----------------|\n",
    "| **pandas** | Data manipulation | Handle time-indexed DataFrames |\n",
    "| **numpy** | Numerical operations | Fast array computations |\n",
    "| **matplotlib/seaborn** | Visualization | Plot time series, distributions |\n",
    "| **yfinance** | Stock data API | Download OHLCV data from Yahoo |\n",
    "| **sklearn** | ML algorithms | Regression models, metrics, preprocessing |\n",
    "| **statsmodels** | Time series | Seasonal decomposition, statistical tests |\n",
    "\n",
    "### Installation (if needed):\n",
    "```bash\n",
    "pip install yfinance pandas numpy matplotlib seaborn scikit-learn statsmodels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Stock data\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    YFINANCE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    YFINANCE_AVAILABLE = False\n",
    "    print(\"Install yfinance: pip install yfinance\")\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Machine Learning - Regression Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Date handling\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Time Series Decomposition\n",
    "try:\n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    STATSMODELS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    STATSMODELS_AVAILABLE = False\n",
    "    print('Install statsmodels: pip install statsmodels')\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.precision', 4)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LIBRARIES LOADED SUCCESSFULLY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Scikit-learn version: {__import__('sklearn').__version__}\")\n",
    "print(f\"yfinance available: {YFINANCE_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.2 Download Stock Data\n\n### Where Does the Data Come From?\n\nWe use **yfinance** - a Python library that pulls **live stock data** from Yahoo Finance.\n\n| Data Source | Library | Cost | API Key | Data Type |\n|-------------|---------|------|---------|-----------|\n| **Yahoo Finance** | `yfinance` | Free | No | Live OHLCV |\n| **Alpha Vantage** | `alpha_vantage` | Free tier | Yes | Live OHLCV |\n| **Kaggle** | `pandas` | Free | No | Static CSV |\n\n### Running on Kaggle?\n\n**yfinance works on Kaggle!** Just enable Internet access:\n1. Open your Kaggle notebook\n2. Click **Settings** (right sidebar)\n3. Toggle **Internet** to **ON**\n\nIf yfinance doesn't work, set `USE_KAGGLE_DATASET = True` and use a CSV dataset from Kaggle.\n\n---\n\n### What is OHLCV Data?\n\nStock data comes in **OHLCV** format - the standard representation of price movement:\n\n| Column | Full Name | Description | Data Type |\n|--------|-----------|-------------|----------|\n| **O** | Open | First trade price of the day | float64 |\n| **H** | High | Maximum price during the day | float64 |\n| **L** | Low | Minimum price during the day | float64 |\n| **C** | Close | Last trade price of the day | float64 |\n| **V** | Volume | Number of shares traded | int64 |\n| **Adj Close** | Adjusted Close | Close price adjusted for dividends/splits | float64 |\n\n### Why Adjusted Close Matters:\n\nWhen a company issues dividends or stock splits:\n- **Raw Close** shows the actual trading price\n- **Adj Close** adjusts historical prices to reflect these events\n\nFor **prediction**, we typically use **Adj Close** for historical analysis, but **Close** for recent data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nTICKER = 'AAPL'  # Stock ticker symbol (Apple Inc.)\nSTART_DATE = '2019-01-01'  # Start date for historical data\nEND_DATE = datetime.now().strftime('%Y-%m-%d')  # Up to today\n\nprint(\"=\"*70)\nprint(\"DOWNLOADING STOCK DATA\")\nprint(\"=\"*70)\n\n# About the stock\nprint(f\"\\nStock: {TICKER}\")\nprint(f\"Period: {START_DATE} to {END_DATE}\")\n\n# ============================================================\n# DATA SOURCE OPTIONS (Choose ONE)\n# ============================================================\n# Option 1: yfinance (LIVE DATA) - Recommended\n# Option 2: Kaggle Dataset (STATIC DATA) - Fallback\n# ============================================================\n\nUSE_KAGGLE_DATASET = False  # Set True to use Kaggle CSV instead of yfinance\n\nif USE_KAGGLE_DATASET:\n    # ============================================================\n    # OPTION 2: KAGGLE DATASET (Fallback)\n    # ============================================================\n    # Download from: https://www.kaggle.com/datasets/varpit94/apple-stock-data-updated-till-22jun2021\n    # Or search \"AAPL stock\" on Kaggle and download any recent dataset\n    \n    KAGGLE_CSV_PATH = '/kaggle/input/apple-stock-data-updated-till-22jun2021/AAPL.csv'  # Update path!\n    \n    print(\"\\nUsing KAGGLE DATASET (static CSV)\")\n    print(f\"Path: {KAGGLE_CSV_PATH}\")\n    \n    try:\n        df = pd.read_csv(KAGGLE_CSV_PATH, parse_dates=['Date'], index_col='Date')\n        df = df.sort_index()\n        print(f\"\\nLoaded from CSV!\")\n    except FileNotFoundError:\n        print(\"ERROR: CSV file not found. Please update KAGGLE_CSV_PATH\")\n        print(\"Download a stock dataset from Kaggle and update the path.\")\n\nelif YFINANCE_AVAILABLE:\n    # ============================================================\n    # OPTION 1: YFINANCE (Live Data) - Recommended\n    # ============================================================\n    print(\"\\nUsing YFINANCE (live data from Yahoo Finance)\")\n    print(\"Note: On Kaggle, enable 'Internet' in Settings (right sidebar)\")\n    \n    # Download historical data\n    df = yf.download(TICKER, start=START_DATE, end=END_DATE, progress=False)\n    \n    # Flatten multi-level columns if present\n    if isinstance(df.columns, pd.MultiIndex):\n        df.columns = df.columns.get_level_values(0)\n    \n    print(f\"\\nDownload successful!\")\nelse:\n    print(\"ERROR: yfinance not available and USE_KAGGLE_DATASET is False\")\n    print(\"Please either:\")\n    print(\"  1. Install yfinance: pip install yfinance\")\n    print(\"  2. Set USE_KAGGLE_DATASET = True and provide a CSV path\")\n\n# Display dataset info\nif 'df' in dir() and len(df) > 0:\n    print(f\"\\nShape: {df.shape[0]} trading days x {df.shape[1]} columns\")\n    print(f\"Date range: {df.index[0].strftime('%Y-%m-%d')} to {df.index[-1].strftime('%Y-%m-%d')}\")\n    print(f\"\\nNote: Stock markets are closed on weekends and holidays.\")\n    print(f\"      That's why we have ~252 trading days per year, not 365.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part2'></a>\n",
    "# Part 2: Understanding Stock Data\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Data Structure and Types\n",
    "\n",
    "Before any analysis, we must understand our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first rows\n",
    "print(\"=\"*70)\n",
    "print(\"FIRST 10 ROWS OF DATA\")\n",
    "print(\"=\"*70)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and structure\n",
    "print(\"=\"*70)\n",
    "print(\"DATA TYPES AND STRUCTURE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. DataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. Column Data Types Explained:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for col in df.columns:\n",
    "    dtype = df[col].dtype\n",
    "    sample = df[col].iloc[0]\n",
    "    print(f\"\\n  {col}:\")\n",
    "    print(f\"    Type: {dtype}\")\n",
    "    print(f\"    Sample: {sample}\")\n",
    "    if dtype == 'float64':\n",
    "        print(f\"    Range: {df[col].min():.2f} to {df[col].max():.2f}\")\n",
    "    elif dtype == 'int64':\n",
    "        print(f\"    Range: {df[col].min():,} to {df[col].max():,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3. Index (DatetimeIndex):\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Type: {type(df.index).__name__}\")\n",
    "print(f\"  Timezone: {df.index.tz}\")\n",
    "print(f\"  Frequency: {df.index.freq} (None = irregular, due to weekends/holidays)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"=\"*70)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=\"*70)\n",
    "print(\"MISSING VALUES CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing = df.isnull().sum()\n",
    "print(missing)\n",
    "print(f\"\\nTotal missing: {missing.sum()}\")\n",
    "\n",
    "if missing.sum() > 0:\n",
    "    print(\"\\nHandling missing values...\")\n",
    "    # For stock data, forward fill is common (use last known price)\n",
    "    df = df.ffill()\n",
    "    print(\"Used forward fill (ffill) - last known value propagates forward.\")\n",
    "else:\n",
    "    print(\"\\nNo missing values - data is clean!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Understanding Price Relationships\n",
    "\n",
    "### The OHLC Relationship:\n",
    "\n",
    "For any trading day, these rules ALWAYS hold:\n",
    "- **Low \u2264 Open \u2264 High**\n",
    "- **Low \u2264 Close \u2264 High**\n",
    "- **Low = min(all trades)**\n",
    "- **High = max(all trades)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify OHLC relationships\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFYING OHLC RELATIONSHIPS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check: Low <= Open <= High\n",
    "low_open_check = (df['Low'] <= df['Open']).all() and (df['Open'] <= df['High']).all()\n",
    "print(f\"\\nLow <= Open <= High: {low_open_check}\")\n",
    "\n",
    "# Check: Low <= Close <= High  \n",
    "low_close_check = (df['Low'] <= df['Close']).all() and (df['Close'] <= df['High']).all()\n",
    "print(f\"Low <= Close <= High: {low_close_check}\")\n",
    "\n",
    "# Check: Low is minimum\n",
    "low_min_check = (df['Low'] == df[['Open', 'High', 'Low', 'Close']].min(axis=1)).all()\n",
    "print(f\"Low is minimum of OHLC: {low_min_check}\")\n",
    "\n",
    "# Check: High is maximum\n",
    "high_max_check = (df['High'] == df[['Open', 'High', 'Low', 'Close']].max(axis=1)).all()\n",
    "print(f\"High is maximum of OHLC: {high_max_check}\")\n",
    "\n",
    "print(\"\\nAll relationships verified - data integrity confirmed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part3'></a>\n",
    "# Part 3: Exploratory Data Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Price Trend Visualization\n",
    "\n",
    "The first step in any time series analysis is to **visualize the data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock price history\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# 1. Closing Price\n",
    "axes[0].plot(df.index, df['Close'], color='steelblue', linewidth=1.5)\n",
    "axes[0].fill_between(df.index, df['Close'], alpha=0.3, color='steelblue')\n",
    "axes[0].set_title(f'{TICKER} Closing Price History', fontweight='bold', fontsize=14)\n",
    "axes[0].set_ylabel('Close Price ($)')\n",
    "axes[0].set_xlabel('Date')\n",
    "\n",
    "# Add annotation for price range\n",
    "axes[0].axhline(y=df['Close'].mean(), color='red', linestyle='--', \n",
    "                label=f\"Mean: ${df['Close'].mean():.2f}\")\n",
    "axes[0].legend(loc='upper left')\n",
    "\n",
    "# 2. Trading Volume\n",
    "colors = ['green' if df['Close'].iloc[i] >= df['Open'].iloc[i] else 'red' \n",
    "          for i in range(len(df))]\n",
    "axes[1].bar(df.index, df['Volume'], color=colors, alpha=0.7, width=1)\n",
    "axes[1].set_title('Trading Volume', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylabel('Volume')\n",
    "axes[1].set_xlabel('Date')\n",
    "\n",
    "# 3. Price Range (High - Low)\n",
    "df['Price_Range'] = df['High'] - df['Low']\n",
    "axes[2].plot(df.index, df['Price_Range'], color='purple', linewidth=1, alpha=0.8)\n",
    "axes[2].fill_between(df.index, df['Price_Range'], alpha=0.3, color='purple')\n",
    "axes[2].set_title('Daily Price Range (High - Low)', fontweight='bold', fontsize=14)\n",
    "axes[2].set_ylabel('Price Range ($)')\n",
    "axes[2].set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=\"*70)\n",
    "print(\"PRICE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Starting Price: ${df['Close'].iloc[0]:.2f}\")\n",
    "print(f\"Ending Price:   ${df['Close'].iloc[-1]:.2f}\")\n",
    "print(f\"Highest Price:  ${df['Close'].max():.2f} (on {df['Close'].idxmax().strftime('%Y-%m-%d')})\")\n",
    "print(f\"Lowest Price:   ${df['Close'].min():.2f} (on {df['Close'].idxmin().strftime('%Y-%m-%d')})\")\n",
    "print(f\"Total Return:   {((df['Close'].iloc[-1] / df['Close'].iloc[0]) - 1) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Returns Analysis\n",
    "\n",
    "### Why Analyze Returns Instead of Prices?\n",
    "\n",
    "| Metric | Formula | Why Use It |\n",
    "|--------|---------|------------|\n",
    "| **Price** | P(t) | Absolute value, hard to compare |\n",
    "| **Return** | (P(t) - P(t-1)) / P(t-1) | Normalized, stationary, comparable |\n",
    "| **Log Return** | ln(P(t) / P(t-1)) | Additive over time, symmetric |\n",
    "\n",
    "**Returns are more stationary than prices!** This helps ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate returns\n",
    "df['Daily_Return'] = df['Close'].pct_change() * 100  # Percentage return\n",
    "df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1)) * 100  # Log return\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RETURNS ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Plot returns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Daily Returns Time Series\n",
    "axes[0, 0].plot(df.index, df['Daily_Return'], color='steelblue', linewidth=0.8, alpha=0.8)\n",
    "axes[0, 0].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "axes[0, 0].fill_between(df.index, df['Daily_Return'], \n",
    "                        where=(df['Daily_Return'] > 0), color='green', alpha=0.3)\n",
    "axes[0, 0].fill_between(df.index, df['Daily_Return'], \n",
    "                        where=(df['Daily_Return'] < 0), color='red', alpha=0.3)\n",
    "axes[0, 0].set_title('Daily Returns Over Time', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Return (%)')\n",
    "\n",
    "# 2. Returns Distribution\n",
    "axes[0, 1].hist(df['Daily_Return'].dropna(), bins=50, color='steelblue', \n",
    "                edgecolor='black', alpha=0.7, density=True)\n",
    "axes[0, 1].axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "axes[0, 1].axvline(x=df['Daily_Return'].mean(), color='red', linestyle='--', \n",
    "                   linewidth=2, label=f\"Mean: {df['Daily_Return'].mean():.3f}%\")\n",
    "axes[0, 1].set_title('Distribution of Daily Returns', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Return (%)')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Box Plot\n",
    "axes[1, 0].boxplot(df['Daily_Return'].dropna(), vert=True, patch_artist=True,\n",
    "                   boxprops=dict(facecolor='lightblue', color='black'),\n",
    "                   medianprops=dict(color='red', linewidth=2))\n",
    "axes[1, 0].set_title('Returns Box Plot (Outliers = Extreme Days)', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Return (%)')\n",
    "\n",
    "# 4. QQ Plot (Check for Normality)\n",
    "from scipy import stats\n",
    "stats.probplot(df['Daily_Return'].dropna(), dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot (Check Normality)', fontweight='bold')\n",
    "axes[1, 1].get_lines()[0].set_markerfacecolor('steelblue')\n",
    "axes[1, 1].get_lines()[0].set_alpha(0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(\"\\nReturn Statistics:\")\n",
    "print(f\"  Mean Daily Return:     {df['Daily_Return'].mean():.4f}%\")\n",
    "print(f\"  Std Dev (Volatility):  {df['Daily_Return'].std():.4f}%\")\n",
    "print(f\"  Min Return (Worst Day): {df['Daily_Return'].min():.4f}%\")\n",
    "print(f\"  Max Return (Best Day):  {df['Daily_Return'].max():.4f}%\")\n",
    "print(f\"  Skewness:              {df['Daily_Return'].skew():.4f}\")\n",
    "print(f\"  Kurtosis:              {df['Daily_Return'].kurtosis():.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Negative skewness = More extreme negative returns (crash risk)\")\n",
    "print(\"  - High kurtosis = Fat tails (more extreme events than normal distribution)\")\n",
    "print(\"  - Q-Q plot deviation from line = Non-normal distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Autocorrelation Analysis\n",
    "\n",
    "### What is Autocorrelation?\n",
    "\n",
    "**Autocorrelation** measures how correlated a time series is with its past values.\n",
    "\n",
    "| Lag | Meaning | If Significant... |\n",
    "|-----|---------|-------------------|\n",
    "| Lag 1 | Today vs Yesterday | Recent past predicts near future |\n",
    "| Lag 5 | Today vs 5 days ago | Weekly patterns |\n",
    "| Lag 20 | Today vs 20 days ago | Monthly patterns |\n",
    "\n",
    "**For stock prices**: Usually low autocorrelation in returns (markets are efficient).\n",
    "But some patterns may exist!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation analysis\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Autocorrelation of Prices (usually high - trending)\n",
    "pd.plotting.autocorrelation_plot(df['Close'].dropna(), ax=axes[0])\n",
    "axes[0].set_title('Autocorrelation of PRICES (Expected: High)', fontweight='bold')\n",
    "axes[0].set_xlim(0, 50)\n",
    "\n",
    "# 2. Autocorrelation of Returns (usually low - efficient market)\n",
    "pd.plotting.autocorrelation_plot(df['Daily_Return'].dropna(), ax=axes[1])\n",
    "axes[1].set_title('Autocorrelation of RETURNS (Expected: Low)', fontweight='bold')\n",
    "axes[1].set_xlim(0, 50)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Prices show HIGH autocorrelation (trending behavior)\")\n",
    "print(\"- Returns show LOW autocorrelation (past returns don't predict future)\")\n",
    "print(\"- This is why we need TECHNICAL INDICATORS - they extract patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 3.4 Trend and Seasonality Analysis\n\n### What are Trend, Seasonality, and Residuals?\n\nEvery time series can be decomposed into three components:\n\n| Component | Definition | Example in Stock Data |\n|-----------|------------|----------------------|\n| **Trend** | Long-term direction (up/down) | Bull market = upward trend |\n| **Seasonality** | Repeating patterns at fixed intervals | Monday effect, January effect |\n| **Residuals** | Random noise after removing trend/season | Unpredictable fluctuations |\n\n### Types of Seasonality in Stock Markets:\n\n| Pattern | Period | Description |\n|---------|--------|-------------|\n| **Day-of-Week Effect** | Weekly | Mondays often negative, Fridays positive |\n| **Month-of-Year Effect** | Yearly | \"Sell in May\", January effect |\n| **Quarter-End Effect** | Quarterly | Window dressing by fund managers |\n| **Holiday Effect** | Variable | Pre-holiday rallies |\n\n### Additive vs Multiplicative Decomposition:\n\n| Type | Formula | Use When |\n|------|---------|----------|\n| **Additive** | Y = Trend + Seasonal + Residual | Seasonal variation is constant |\n| **Multiplicative** | Y = Trend \u00d7 Seasonal \u00d7 Residual | Seasonal variation scales with trend |\n\nStock prices typically use **multiplicative** (percentage changes scale with price level).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SEASONALITY ANALYSIS\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"SEASONALITY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create temporary columns for analysis\n",
    "df['Day_Name'] = df.index.day_name()\n",
    "df['Month_Name'] = df.index.month_name()\n",
    "df['Year'] = df.index.year\n",
    "\n",
    "# ============================================================\n",
    "# 1. DAY-OF-WEEK EFFECT\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1. DAY-OF-WEEK EFFECT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "The 'Monday Effect' hypothesis:\n",
    "- Mondays tend to have lower/negative returns\n",
    "- Fridays tend to have positive returns\n",
    "- Reason: Weekend news accumulation, investor psychology\n",
    "\"\"\")\n",
    "\n",
    "# Calculate average return by day of week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "daily_returns = df.groupby('Day_Name')['Daily_Return'].agg(['mean', 'std', 'count'])\n",
    "daily_returns = daily_returns.reindex(day_order)\n",
    "\n",
    "print(\"\\nAverage Return by Day of Week:\")\n",
    "print(daily_returns.round(4))\n",
    "\n",
    "# Visualize day-of-week effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of average returns\n",
    "colors = ['green' if x > 0 else 'red' for x in daily_returns['mean']]\n",
    "axes[0].bar(day_order, daily_returns['mean'], color=colors, edgecolor='black', alpha=0.7)\n",
    "axes[0].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "axes[0].set_title('Average Daily Return by Day of Week', fontweight='bold')\n",
    "axes[0].set_ylabel('Average Return (%)')\n",
    "axes[0].set_xlabel('Day of Week')\n",
    "for i, (day, row) in enumerate(daily_returns.iterrows()):\n",
    "    axes[0].text(i, row['mean'] + 0.01, f\"{row['mean']:.3f}%\", ha='center', fontweight='bold')\n",
    "\n",
    "# Box plot\n",
    "day_data = [df[df['Day_Name'] == day]['Daily_Return'].dropna() for day in day_order]\n",
    "bp = axes[1].boxplot(day_data, labels=day_order, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], ['#ff9999', '#99ff99', '#9999ff', '#ffff99', '#ff99ff']):\n",
    "    patch.set_facecolor(color)\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "axes[1].set_title('Return Distribution by Day of Week', fontweight='bold')\n",
    "axes[1].set_ylabel('Daily Return (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 2. MONTH-OF-YEAR EFFECT\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. MONTH-OF-YEAR EFFECT (Seasonality)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "Common Stock Market Seasonality Patterns:\n",
    "- 'January Effect': Small caps outperform in January\n",
    "- 'Sell in May': May-October underperforms Nov-April\n",
    "- 'Santa Rally': December often positive\n",
    "- 'September Effect': September often negative\n",
    "\"\"\")\n",
    "\n",
    "# Calculate monthly returns\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "               'July', 'August', 'September', 'October', 'November', 'December']\n",
    "monthly_returns = df.groupby('Month_Name')['Daily_Return'].agg(['mean', 'std', 'count'])\n",
    "monthly_returns = monthly_returns.reindex(month_order)\n",
    "\n",
    "print(\"\\nAverage Return by Month:\")\n",
    "print(monthly_returns.round(4))\n",
    "\n",
    "# Visualize monthly seasonality\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "colors = ['green' if x > 0 else 'red' for x in monthly_returns['mean']]\n",
    "bars = ax.bar(range(12), monthly_returns['mean'], color=colors, edgecolor='black', alpha=0.7)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax.axhline(y=monthly_returns['mean'].mean(), color='blue', linestyle='--', \n",
    "           label=f\"Overall Mean: {monthly_returns['mean'].mean():.3f}%\")\n",
    "ax.set_title('Average Daily Return by Month (Seasonality Pattern)', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('Average Return (%)')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_xticks(range(12))\n",
    "ax.set_xticklabels([m[:3] for m in month_order], rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Annotate best and worst months\n",
    "best_month = monthly_returns['mean'].idxmax()\n",
    "worst_month = monthly_returns['mean'].idxmin()\n",
    "ax.annotate(f'Best: {best_month}', xy=(month_order.index(best_month), monthly_returns.loc[best_month, 'mean']),\n",
    "            xytext=(10, 20), textcoords='offset points', fontweight='bold', color='green',\n",
    "            arrowprops=dict(arrowstyle='->', color='green'))\n",
    "ax.annotate(f'Worst: {worst_month}', xy=(month_order.index(worst_month), monthly_returns.loc[worst_month, 'mean']),\n",
    "            xytext=(10, -30), textcoords='offset points', fontweight='bold', color='red',\n",
    "            arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 3. TIME SERIES DECOMPOSITION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. TIME SERIES DECOMPOSITION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "Decomposition breaks the series into:\n",
    "1. TREND: Long-term direction\n",
    "2. SEASONAL: Repeating patterns (we use yearly = ~252 trading days)\n",
    "3. RESIDUAL: Random noise\n",
    "\"\"\")\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Use multiplicative decomposition (better for stock prices)\n",
    "# Period = 252 (approximate trading days per year)\n",
    "# Use a subset for cleaner visualization\n",
    "decomposition = seasonal_decompose(df['Close'].dropna(), model='multiplicative', period=252)\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n",
    "\n",
    "# Original\n",
    "axes[0].plot(df.index, df['Close'], color='black', linewidth=1)\n",
    "axes[0].set_title('Original Time Series', fontweight='bold')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "\n",
    "# Trend\n",
    "axes[1].plot(decomposition.trend.index, decomposition.trend, color='blue', linewidth=1.5)\n",
    "axes[1].set_title('Trend Component (Long-term Direction)', fontweight='bold')\n",
    "axes[1].set_ylabel('Trend')\n",
    "\n",
    "# Seasonal\n",
    "axes[2].plot(decomposition.seasonal.index, decomposition.seasonal, color='green', linewidth=1)\n",
    "axes[2].set_title('Seasonal Component (Yearly Pattern)', fontweight='bold')\n",
    "axes[2].set_ylabel('Seasonal Factor')\n",
    "axes[2].axhline(y=1, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Residual\n",
    "axes[3].plot(decomposition.resid.index, decomposition.resid, color='purple', linewidth=0.8, alpha=0.7)\n",
    "axes[3].set_title('Residual Component (Random Noise)', fontweight='bold')\n",
    "axes[3].set_ylabel('Residual')\n",
    "axes[3].axhline(y=1, color='red', linestyle='--', alpha=0.5)\n",
    "axes[3].set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - TREND shows the overall market direction (upward = bull market)\")\n",
    "print(\"  - SEASONAL shows yearly patterns (values > 1 = above average, < 1 = below)\")\n",
    "print(\"  - RESIDUAL is what's left (should be random if model is good)\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SEASONALITY SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest performing day: {daily_returns['mean'].idxmax()} ({daily_returns['mean'].max():.3f}%)\")\n",
    "print(f\"Worst performing day: {daily_returns['mean'].idxmin()} ({daily_returns['mean'].min():.3f}%)\")\n",
    "print(f\"Best performing month: {monthly_returns['mean'].idxmax()} ({monthly_returns['mean'].max():.3f}%)\")\n",
    "print(f\"Worst performing month: {monthly_returns['mean'].idxmin()} ({monthly_returns['mean'].min():.3f}%)\")\n",
    "\n",
    "# Clean up temporary columns\n",
    "df.drop(['Day_Name', 'Month_Name', 'Year'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part4'></a>\n",
    "# Part 4: Technical Indicators (Feature Engineering)\n",
    "\n",
    "---\n",
    "\n",
    "## Why Technical Indicators?\n",
    "\n",
    "Raw prices have **low predictive power**. Technical indicators:\n",
    "1. **Extract patterns** from noisy data\n",
    "2. **Encode domain knowledge** (what traders actually look at)\n",
    "3. **Create stationary features** from non-stationary prices\n",
    "4. **Capture momentum, trend, volatility**\n",
    "\n",
    "### Categories of Technical Indicators:\n",
    "\n",
    "| Category | Purpose | Examples |\n",
    "|----------|---------|----------|\n",
    "| **Trend** | Identify direction | SMA, EMA, MACD |\n",
    "| **Momentum** | Measure speed of change | RSI, Stochastic, ROC |\n",
    "| **Volatility** | Measure price variability | Bollinger Bands, ATR |\n",
    "| **Volume** | Confirm trends with volume | OBV, VWAP |\n",
    "\n",
    "## 4.1 Moving Averages (Trend Indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature DataFrame\n",
    "data = df.copy()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE ENGINEERING - TECHNICAL INDICATORS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# 1. MOVING AVERAGES (Trend Indicators)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1. MOVING AVERAGES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "What are Moving Averages?\n",
    "- Average of past N days' prices\n",
    "- Smooths out noise to reveal trend\n",
    "- Common periods: 5 (week), 20 (month), 50, 200 (long-term)\n",
    "\n",
    "Types:\n",
    "- SMA (Simple): Equal weight to all days\n",
    "- EMA (Exponential): More weight to recent days\n",
    "\"\"\")\n",
    "\n",
    "# Simple Moving Averages\n",
    "data['SMA_5'] = data['Close'].rolling(window=5).mean()   # 1 week\n",
    "data['SMA_10'] = data['Close'].rolling(window=10).mean() # 2 weeks\n",
    "data['SMA_20'] = data['Close'].rolling(window=20).mean() # 1 month\n",
    "data['SMA_50'] = data['Close'].rolling(window=50).mean() # ~2 months\n",
    "\n",
    "# Exponential Moving Averages\n",
    "data['EMA_12'] = data['Close'].ewm(span=12, adjust=False).mean()  # Standard for MACD\n",
    "data['EMA_26'] = data['Close'].ewm(span=26, adjust=False).mean()  # Standard for MACD\n",
    "\n",
    "# Price relative to Moving Averages (normalized)\n",
    "data['Price_to_SMA_20'] = data['Close'] / data['SMA_20']\n",
    "data['Price_to_SMA_50'] = data['Close'] / data['SMA_50']\n",
    "\n",
    "print(\"Created:\")\n",
    "print(\"  - SMA_5, SMA_10, SMA_20, SMA_50\")\n",
    "print(\"  - EMA_12, EMA_26\")\n",
    "print(\"  - Price_to_SMA_20, Price_to_SMA_50 (ratios)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Moving Averages\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot last 200 days\n",
    "plot_data = data.tail(200)\n",
    "\n",
    "ax.plot(plot_data.index, plot_data['Close'], label='Close', linewidth=2, color='black')\n",
    "ax.plot(plot_data.index, plot_data['SMA_20'], label='SMA 20', linewidth=1.5, \n",
    "        linestyle='--', color='blue')\n",
    "ax.plot(plot_data.index, plot_data['SMA_50'], label='SMA 50', linewidth=1.5, \n",
    "        linestyle='--', color='red')\n",
    "ax.plot(plot_data.index, plot_data['EMA_12'], label='EMA 12', linewidth=1.5, \n",
    "        linestyle=':', color='green')\n",
    "\n",
    "ax.set_title(f'{TICKER} with Moving Averages', fontweight='bold', fontsize=14)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price ($)')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTrading Signals from Moving Averages:\")\n",
    "print(\"  - Price > SMA: Bullish (uptrend)\")\n",
    "print(\"  - Price < SMA: Bearish (downtrend)\")\n",
    "print(\"  - SMA_20 crosses above SMA_50: Golden Cross (buy signal)\")\n",
    "print(\"  - SMA_20 crosses below SMA_50: Death Cross (sell signal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Momentum Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. MOMENTUM INDICATORS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. MOMENTUM INDICATORS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "What is Momentum?\n",
    "- Rate of change in price\n",
    "- Identifies overbought/oversold conditions\n",
    "- Helps predict potential reversals\n",
    "\"\"\")\n",
    "\n",
    "# RSI (Relative Strength Index)\n",
    "# Measures: Ratio of average gains to average losses\n",
    "# Range: 0-100\n",
    "# Interpretation: >70 = overbought, <30 = oversold\n",
    "\n",
    "def calculate_rsi(prices, period=14):\n",
    "    \"\"\"Calculate RSI (Relative Strength Index)\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "data['RSI'] = calculate_rsi(data['Close'], period=14)\n",
    "print(\"\\nRSI (Relative Strength Index):\")\n",
    "print(\"  - Formula: 100 - 100/(1 + RS), where RS = Avg Gain / Avg Loss\")\n",
    "print(\"  - Period: 14 days (standard)\")\n",
    "print(\"  - Range: 0 to 100\")\n",
    "print(\"  - Overbought: RSI > 70\")\n",
    "print(\"  - Oversold: RSI < 30\")\n",
    "\n",
    "# MACD (Moving Average Convergence Divergence)\n",
    "# Measures: Relationship between two EMAs\n",
    "# Components: MACD line, Signal line, Histogram\n",
    "\n",
    "data['MACD'] = data['EMA_12'] - data['EMA_26']  # MACD Line\n",
    "data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()  # Signal Line\n",
    "data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']  # Histogram\n",
    "\n",
    "print(\"\\nMACD (Moving Average Convergence Divergence):\")\n",
    "print(\"  - MACD Line: EMA_12 - EMA_26\")\n",
    "print(\"  - Signal Line: 9-day EMA of MACD Line\")\n",
    "print(\"  - Histogram: MACD - Signal\")\n",
    "print(\"  - Buy: MACD crosses above Signal\")\n",
    "print(\"  - Sell: MACD crosses below Signal\")\n",
    "\n",
    "# Rate of Change (ROC)\n",
    "data['ROC_5'] = ((data['Close'] - data['Close'].shift(5)) / data['Close'].shift(5)) * 100\n",
    "data['ROC_10'] = ((data['Close'] - data['Close'].shift(10)) / data['Close'].shift(10)) * 100\n",
    "\n",
    "print(\"\\nROC (Rate of Change):\")\n",
    "print(\"  - Formula: ((Price_today - Price_N_days_ago) / Price_N_days_ago) * 100\")\n",
    "print(\"  - Created: ROC_5, ROC_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RSI and MACD\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "plot_data = data.tail(200)\n",
    "\n",
    "# 1. Price\n",
    "axes[0].plot(plot_data.index, plot_data['Close'], color='black', linewidth=1.5)\n",
    "axes[0].set_title(f'{TICKER} Price', fontweight='bold')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "\n",
    "# 2. RSI\n",
    "axes[1].plot(plot_data.index, plot_data['RSI'], color='purple', linewidth=1.5)\n",
    "axes[1].axhline(y=70, color='red', linestyle='--', alpha=0.7, label='Overbought (70)')\n",
    "axes[1].axhline(y=30, color='green', linestyle='--', alpha=0.7, label='Oversold (30)')\n",
    "axes[1].axhline(y=50, color='gray', linestyle='-', alpha=0.3)\n",
    "axes[1].fill_between(plot_data.index, 30, 70, alpha=0.1, color='gray')\n",
    "axes[1].set_title('RSI (Relative Strength Index)', fontweight='bold')\n",
    "axes[1].set_ylabel('RSI')\n",
    "axes[1].set_ylim(0, 100)\n",
    "axes[1].legend(loc='upper right')\n",
    "\n",
    "# 3. MACD\n",
    "axes[2].plot(plot_data.index, plot_data['MACD'], color='blue', linewidth=1.5, label='MACD')\n",
    "axes[2].plot(plot_data.index, plot_data['MACD_Signal'], color='red', linewidth=1.5, label='Signal')\n",
    "axes[2].bar(plot_data.index, plot_data['MACD_Histogram'], \n",
    "            color=['green' if x >= 0 else 'red' for x in plot_data['MACD_Histogram']], \n",
    "            alpha=0.5, label='Histogram')\n",
    "axes[2].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[2].set_title('MACD (Moving Average Convergence Divergence)', fontweight='bold')\n",
    "axes[2].set_ylabel('MACD')\n",
    "axes[2].legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Volatility Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. VOLATILITY INDICATORS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. VOLATILITY INDICATORS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "What is Volatility?\n",
    "- Measure of price variability/risk\n",
    "- High volatility = Big price swings (opportunity + risk)\n",
    "- Low volatility = Stable prices\n",
    "\"\"\")\n",
    "\n",
    "# Bollinger Bands\n",
    "# Upper Band: SMA + 2*Std\n",
    "# Lower Band: SMA - 2*Std\n",
    "# When price touches bands, it may reverse\n",
    "\n",
    "data['BB_Middle'] = data['Close'].rolling(window=20).mean()\n",
    "bb_std = data['Close'].rolling(window=20).std()\n",
    "data['BB_Upper'] = data['BB_Middle'] + (bb_std * 2)\n",
    "data['BB_Lower'] = data['BB_Middle'] - (bb_std * 2)\n",
    "data['BB_Width'] = (data['BB_Upper'] - data['BB_Lower']) / data['BB_Middle'] * 100\n",
    "data['BB_Position'] = (data['Close'] - data['BB_Lower']) / (data['BB_Upper'] - data['BB_Lower'])\n",
    "\n",
    "print(\"\\nBollinger Bands:\")\n",
    "print(\"  - Middle: 20-day SMA\")\n",
    "print(\"  - Upper: Middle + 2 * StdDev\")\n",
    "print(\"  - Lower: Middle - 2 * StdDev\")\n",
    "print(\"  - BB_Width: Band width (volatility measure)\")\n",
    "print(\"  - BB_Position: Where price is within bands (0=lower, 1=upper)\")\n",
    "\n",
    "# Average True Range (ATR)\n",
    "# Measures average daily price range\n",
    "# Used for volatility and stop-loss placement\n",
    "\n",
    "high_low = data['High'] - data['Low']\n",
    "high_close = abs(data['High'] - data['Close'].shift())\n",
    "low_close = abs(data['Low'] - data['Close'].shift())\n",
    "true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "data['ATR'] = true_range.rolling(window=14).mean()\n",
    "data['ATR_Pct'] = (data['ATR'] / data['Close']) * 100  # As percentage of price\n",
    "\n",
    "print(\"\\nATR (Average True Range):\")\n",
    "print(\"  - True Range: max(High-Low, |High-PrevClose|, |Low-PrevClose|)\")\n",
    "print(\"  - ATR: 14-day moving average of True Range\")\n",
    "print(\"  - ATR_Pct: ATR as percentage of price\")\n",
    "\n",
    "# Rolling Volatility (Std of Returns)\n",
    "data['Volatility_5'] = data['Daily_Return'].rolling(window=5).std()\n",
    "data['Volatility_20'] = data['Daily_Return'].rolling(window=20).std()\n",
    "\n",
    "print(\"\\nRolling Volatility:\")\n",
    "print(\"  - Volatility_5: 5-day rolling std of returns\")\n",
    "print(\"  - Volatility_20: 20-day rolling std of returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Bollinger Bands\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "plot_data = data.tail(200)\n",
    "\n",
    "# 1. Bollinger Bands\n",
    "axes[0].plot(plot_data.index, plot_data['Close'], label='Close', color='black', linewidth=1.5)\n",
    "axes[0].plot(plot_data.index, plot_data['BB_Upper'], label='Upper Band', \n",
    "             color='red', linestyle='--', linewidth=1)\n",
    "axes[0].plot(plot_data.index, plot_data['BB_Middle'], label='Middle (SMA 20)', \n",
    "             color='blue', linestyle='--', linewidth=1)\n",
    "axes[0].plot(plot_data.index, plot_data['BB_Lower'], label='Lower Band', \n",
    "             color='green', linestyle='--', linewidth=1)\n",
    "axes[0].fill_between(plot_data.index, plot_data['BB_Upper'], plot_data['BB_Lower'], \n",
    "                     alpha=0.1, color='gray')\n",
    "axes[0].set_title(f'{TICKER} with Bollinger Bands', fontweight='bold', fontsize=14)\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].legend(loc='upper left')\n",
    "\n",
    "# 2. Band Width (Volatility)\n",
    "axes[1].plot(plot_data.index, plot_data['BB_Width'], color='purple', linewidth=1.5)\n",
    "axes[1].fill_between(plot_data.index, plot_data['BB_Width'], alpha=0.3, color='purple')\n",
    "axes[1].axhline(y=plot_data['BB_Width'].mean(), color='red', linestyle='--', \n",
    "                label=f\"Mean: {plot_data['BB_Width'].mean():.2f}%\")\n",
    "axes[1].set_title('Bollinger Band Width (Volatility Measure)', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylabel('BB Width (%)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBollinger Band Trading Signals:\")\n",
    "print(\"  - Price at Upper Band: Potentially overbought\")\n",
    "print(\"  - Price at Lower Band: Potentially oversold\")\n",
    "print(\"  - Bands Narrowing: Low volatility (breakout may follow)\")\n",
    "print(\"  - Bands Widening: High volatility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Lag Features and Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. LAG FEATURES\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. LAG FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "What are Lag Features?\n",
    "- Past values of the target/features\n",
    "- Captures autocorrelation patterns\n",
    "- Essential for time series ML\n",
    "\n",
    "Why use them?\n",
    "- Yesterday's price may predict today's price\n",
    "- Multiple lags capture different patterns\n",
    "\"\"\")\n",
    "\n",
    "# Lag features for Close price\n",
    "for lag in [1, 2, 3, 5, 7, 10]:\n",
    "    data[f'Close_Lag_{lag}'] = data['Close'].shift(lag)\n",
    "    data[f'Return_Lag_{lag}'] = data['Daily_Return'].shift(lag)\n",
    "\n",
    "print(\"Created Close and Return lags: 1, 2, 3, 5, 7, 10 days\")\n",
    "\n",
    "# ============================================================\n",
    "# 5. ADDITIONAL FEATURES\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5. ADDITIONAL FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Price-based features\n",
    "data['Open_Close_Pct'] = (data['Close'] - data['Open']) / data['Open'] * 100  # Intraday return\n",
    "data['High_Low_Pct'] = (data['High'] - data['Low']) / data['Low'] * 100  # Daily range %\n",
    "\n",
    "# Volume features\n",
    "data['Volume_SMA_20'] = data['Volume'].rolling(window=20).mean()\n",
    "data['Volume_Ratio'] = data['Volume'] / data['Volume_SMA_20']  # Volume spike detection\n",
    "\n",
    "# Time features (day of week, month)\n",
    "data['Day_of_Week'] = data.index.dayofweek  # 0=Monday, 4=Friday\n",
    "data['Month'] = data.index.month\n",
    "data['Quarter'] = data.index.quarter\n",
    "\n",
    "# Target variable: Next day's close price\n",
    "data['Target'] = data['Close'].shift(-1)  # What we want to predict!\n",
    "\n",
    "print(\"\\nAdditional features created:\")\n",
    "print(\"  - Open_Close_Pct: Intraday return\")\n",
    "print(\"  - High_Low_Pct: Daily range percentage\")\n",
    "print(\"  - Volume_SMA_20, Volume_Ratio: Volume patterns\")\n",
    "print(\"  - Day_of_Week, Month, Quarter: Calendar features\")\n",
    "print(\"  - Target: Next day's close (what we predict)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all features\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nTotal columns: {len(data.columns)}\")\n",
    "print(\"\\nFeatures by Category:\")\n",
    "\n",
    "trend_features = ['SMA_5', 'SMA_10', 'SMA_20', 'SMA_50', 'EMA_12', 'EMA_26', \n",
    "                  'Price_to_SMA_20', 'Price_to_SMA_50']\n",
    "momentum_features = ['RSI', 'MACD', 'MACD_Signal', 'MACD_Histogram', 'ROC_5', 'ROC_10']\n",
    "volatility_features = ['BB_Middle', 'BB_Upper', 'BB_Lower', 'BB_Width', 'BB_Position', \n",
    "                       'ATR', 'ATR_Pct', 'Volatility_5', 'Volatility_20']\n",
    "lag_features = [col for col in data.columns if 'Lag' in col]\n",
    "\n",
    "print(f\"\\n  Trend ({len(trend_features)}):     {trend_features}\")\n",
    "print(f\"\\n  Momentum ({len(momentum_features)}):  {momentum_features}\")\n",
    "print(f\"\\n  Volatility ({len(volatility_features)}): {volatility_features}\")\n",
    "print(f\"\\n  Lag ({len(lag_features)}):       {lag_features}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE DATA WITH ALL FEATURES\")\n",
    "print(\"=\"*70)\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part5'></a>\n",
    "# Part 5: Data Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 Handle Missing Values\n",
    "\n",
    "Technical indicators create NaN at the start (rolling window needs history)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HANDLING MISSING VALUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nBefore cleaning: {data.shape}\")\n",
    "print(f\"Total NaN values: {data.isnull().sum().sum()}\")\n",
    "\n",
    "# Show which columns have NaN\n",
    "nan_counts = data.isnull().sum()\n",
    "nan_cols = nan_counts[nan_counts > 0].sort_values(ascending=False)\n",
    "print(f\"\\nColumns with NaN (top 10):\")\n",
    "print(nan_cols.head(10))\n",
    "\n",
    "# Drop rows with NaN\n",
    "data_clean = data.dropna()\n",
    "\n",
    "print(f\"\\nAfter cleaning: {data_clean.shape}\")\n",
    "print(f\"Rows removed: {len(data) - len(data_clean)}\")\n",
    "print(f\"\\nWhy? Rolling windows (like SMA_50) need 50 days of history.\")\n",
    "print(f\"     Also, Target shift removes last row.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FEATURE SELECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Columns to exclude from features\n",
    "exclude_cols = ['Target', 'Adj Close']  # Target is what we predict, Adj Close is redundant\n",
    "\n",
    "# All other columns are features\n",
    "feature_cols = [col for col in data_clean.columns if col not in exclude_cols]\n",
    "\n",
    "X = data_clean[feature_cols]\n",
    "y = data_clean['Target']\n",
    "\n",
    "print(f\"\\nFeatures (X): {X.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")\n",
    "\n",
    "print(f\"\\nFeature columns ({len(feature_cols)}):\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Time Series Train-Test Split\n",
    "\n",
    "### CRITICAL: Why NOT Random Split?\n",
    "\n",
    "| Split Type | Method | Problem for Time Series |\n",
    "|------------|--------|------------------------|\n",
    "| **Random** | Shuffle, then split | Future data leaks into training! |\n",
    "| **Time-Based** | Split by date | Correct - no future leakage |\n",
    "\n",
    "**Data Leakage**: Using future information to predict the past = cheating!\n",
    "\n",
    "```\n",
    "WRONG (Random):    Train: [2020, 2023, 2019, 2022] | Test: [2021, 2024]\n",
    "RIGHT (Time):      Train: [2019, 2020, 2021, 2022] | Test: [2023, 2024]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TIME-BASED TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Split point: 80% train, 20% test\n",
    "split_idx = int(len(X) * 0.8)\n",
    "\n",
    "# Split chronologically (NOT randomly!)\n",
    "X_train = X.iloc[:split_idx]\n",
    "X_test = X.iloc[split_idx:]\n",
    "y_train = y.iloc[:split_idx]\n",
    "y_test = y.iloc[split_idx:]\n",
    "\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  Samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Period: {X_train.index[0].strftime('%Y-%m-%d')} to {X_train.index[-1].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  Samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Period: {X_test.index[0].strftime('%Y-%m-%d')} to {X_test.index[-1].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(\"\\nIMPORTANT: Train on PAST, test on FUTURE!\")\n",
    "print(\"          This simulates real-world prediction.\")\n",
    "\n",
    "# Visualize split\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.plot(y_train.index, y_train.values, color='blue', label='Training Data', linewidth=1.5)\n",
    "ax.plot(y_test.index, y_test.values, color='orange', label='Test Data', linewidth=1.5)\n",
    "ax.axvline(x=X_test.index[0], color='red', linestyle='--', linewidth=2, label='Split Point')\n",
    "ax.set_title('Time-Based Train-Test Split', fontweight='bold', fontsize=14)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price ($)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Feature Scaling\n",
    "\n",
    "### Why Scale Features?\n",
    "\n",
    "| Algorithm | Needs Scaling? | Why |\n",
    "|-----------|---------------|-----|\n",
    "| **Linear Models** | Yes | Gradient descent converges faster |\n",
    "| **KNN** | Yes | Distance-based, sensitive to scale |\n",
    "| **SVM** | Yes | Kernel calculations affected by scale |\n",
    "| **Decision Trees** | No | Split-based, scale-invariant |\n",
    "| **Random Forest** | No | Ensemble of trees |\n",
    "\n",
    "**We'll scale anyway** - it doesn't hurt tree models and helps others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FEATURE SCALING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use StandardScaler: transforms to mean=0, std=1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# IMPORTANT: Fit on training data ONLY, then transform both\n",
    "# This prevents data leakage from test set!\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit + Transform\n",
    "X_test_scaled = scaler.transform(X_test)         # Transform only (use train stats)\n",
    "\n",
    "# Convert to DataFrame for convenience\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"\\nBefore Scaling (Training Set - first 5 features):\")\n",
    "print(X_train.iloc[:, :5].describe().loc[['mean', 'std', 'min', 'max']].round(2))\n",
    "\n",
    "print(\"\\nAfter Scaling (Training Set - first 5 features):\")\n",
    "print(X_train_scaled.iloc[:, :5].describe().loc[['mean', 'std', 'min', 'max']].round(4))\n",
    "\n",
    "print(\"\\nScaling ensures:\")\n",
    "print(\"  - Mean \u2248 0 for all features\")\n",
    "print(\"  - Std \u2248 1 for all features\")\n",
    "print(\"  - No feature dominates due to scale differences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part6'></a>\n",
    "# Part 6: Model Selection & Training\n",
    "\n",
    "---\n",
    "\n",
    "## Why These Models?\n",
    "\n",
    "| Model | Type | Why Use for Stock Prediction |\n",
    "|-------|------|-----------------------------|\n",
    "| **Linear Regression** | Linear | Baseline, interpretable, fast |\n",
    "| **Ridge** | Regularized Linear | Handles multicollinearity (correlated features) |\n",
    "| **Lasso** | Regularized Linear | Feature selection (sets some coefficients to 0) |\n",
    "| **Random Forest** | Ensemble | Captures non-linear patterns, robust |\n",
    "| **Gradient Boosting** | Ensemble | Often best accuracy, sequential learning |\n",
    "| **SVR** | Kernel-based | Good for non-linear, few samples |\n",
    "| **KNN** | Instance-based | Similar days have similar outcomes |\n",
    "\n",
    "## Key Hyperparameters:\n",
    "\n",
    "| Model | Key Parameters | What They Control |\n",
    "|-------|---------------|-------------------|\n",
    "| **Ridge/Lasso** | alpha | Regularization strength (higher = simpler model) |\n",
    "| **Random Forest** | n_estimators, max_depth | Number of trees, tree complexity |\n",
    "| **Gradient Boosting** | n_estimators, learning_rate | Number of boosting rounds, step size |\n",
    "| **KNN** | n_neighbors | Number of similar samples to use |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with explanations\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL DEFINITIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': {\n",
    "        'model': LinearRegression(),\n",
    "        'description': 'Simple linear fit, no regularization',\n",
    "        'needs_scaling': True\n",
    "    },\n",
    "    'Ridge Regression': {\n",
    "        'model': Ridge(alpha=1.0, random_state=42),\n",
    "        'description': 'L2 regularization, handles multicollinearity',\n",
    "        'needs_scaling': True\n",
    "    },\n",
    "    'Lasso Regression': {\n",
    "        'model': Lasso(alpha=0.1, random_state=42),\n",
    "        'description': 'L1 regularization, feature selection',\n",
    "        'needs_scaling': True\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "        'description': '100 trees, max depth 10, parallel',\n",
    "        'needs_scaling': False\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42),\n",
    "        'description': '100 boosting rounds, learning rate 0.1',\n",
    "        'needs_scaling': False\n",
    "    },\n",
    "    'KNN Regressor': {\n",
    "        'model': KNeighborsRegressor(n_neighbors=10, weights='distance'),\n",
    "        'description': '10 neighbors, distance-weighted',\n",
    "        'needs_scaling': True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nModels to train:\")\n",
    "for i, (name, info) in enumerate(models.items(), 1):\n",
    "    print(f\"\\n{i}. {name}\")\n",
    "    print(f\"   {info['description']}\")\n",
    "    print(f\"   Needs scaling: {info['needs_scaling']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING ALL MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, info in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    model = info['model']\n",
    "    \n",
    "    # Use scaled or unscaled data based on model needs\n",
    "    if info['needs_scaling']:\n",
    "        X_tr, X_te = X_train_scaled, X_test_scaled\n",
    "    else:\n",
    "        X_tr, X_te = X_train, X_test\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_tr, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_train = model.predict(X_tr)\n",
    "    y_pred_test = model.predict(X_te)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'y_pred_train': y_pred_train,\n",
    "        'y_pred_test': y_pred_test,\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'train_mae': mean_absolute_error(y_train, y_pred_train),\n",
    "        'test_mae': mean_absolute_error(y_test, y_pred_test),\n",
    "        'train_r2': r2_score(y_train, y_pred_train),\n",
    "        'test_r2': r2_score(y_test, y_pred_test),\n",
    "        'test_mape': mean_absolute_percentage_error(y_test, y_pred_test) * 100\n",
    "    }\n",
    "    \n",
    "    r = results[name]\n",
    "    print(f\"  Train RMSE: ${r['train_rmse']:.2f} | Test RMSE: ${r['test_rmse']:.2f}\")\n",
    "    print(f\"  Train R\u00b2: {r['train_r2']:.4f} | Test R\u00b2: {r['test_r2']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"All models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part7'></a>\n",
    "# Part 7: Model Evaluation & Comparison\n",
    "\n",
    "---\n",
    "\n",
    "## Regression Metrics Explained\n",
    "\n",
    "| Metric | Formula | Interpretation | Good Value |\n",
    "|--------|---------|----------------|------------|\n",
    "| **RMSE** | \u221a(\u03a3(y-\u0177)\u00b2/n) | Error in original units ($) | Lower is better |\n",
    "| **MAE** | \u03a3|y-\u0177|/n | Average absolute error ($) | Lower is better |\n",
    "| **R\u00b2** | 1 - SS_res/SS_tot | Variance explained (0-1) | Closer to 1 |\n",
    "| **MAPE** | \u03a3|(y-\u0177)/y|/n \u00d7 100 | Percentage error | Lower is better |\n",
    "\n",
    "### Which Metric to Focus On?\n",
    "\n",
    "- **R\u00b2**: Good for comparing models (how much variance explained)\n",
    "- **RMSE**: Penalizes large errors more (sensitive to outliers)\n",
    "- **MAE**: More robust to outliers\n",
    "- **MAPE**: Scale-independent, easy to interpret\n",
    "\n",
    "## 7.1 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Train RMSE': [r['train_rmse'] for r in results.values()],\n",
    "    'Test RMSE': [r['test_rmse'] for r in results.values()],\n",
    "    'Train R\u00b2': [r['train_r2'] for r in results.values()],\n",
    "    'Test R\u00b2': [r['test_r2'] for r in results.values()],\n",
    "    'Test MAPE %': [r['test_mape'] for r in results.values()]\n",
    "}).sort_values('Test RMSE')\n",
    "\n",
    "comparison['Rank'] = range(1, len(comparison) + 1)\n",
    "comparison = comparison[['Rank', 'Model', 'Train RMSE', 'Test RMSE', 'Train R\u00b2', 'Test R\u00b2', 'Test MAPE %']]\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "best_model_name = comparison.iloc[0]['Model']\n",
    "print(f\"\\nBest Model (by Test RMSE): {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "sorted_models = comparison['Model'].tolist()\n",
    "test_rmse = comparison['Test RMSE'].tolist()\n",
    "test_r2 = comparison['Test R\u00b2'].tolist()\n",
    "test_mape = comparison['Test MAPE %'].tolist()\n",
    "\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(sorted_models)))\n",
    "\n",
    "# RMSE\n",
    "axes[0].barh(sorted_models[::-1], test_rmse[::-1], color=colors[::-1], edgecolor='black')\n",
    "axes[0].set_xlabel('Test RMSE ($)')\n",
    "axes[0].set_title('RMSE (Lower is Better)', fontweight='bold')\n",
    "for i, (m, v) in enumerate(zip(sorted_models[::-1], test_rmse[::-1])):\n",
    "    axes[0].text(v + 0.2, i, f'${v:.2f}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# R\u00b2\n",
    "colors_r2 = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(sorted_models)))\n",
    "axes[1].barh(sorted_models[::-1], test_r2[::-1], color=colors_r2[::-1], edgecolor='black')\n",
    "axes[1].set_xlabel('Test R\u00b2 Score')\n",
    "axes[1].set_title('R\u00b2 (Higher is Better)', fontweight='bold')\n",
    "axes[1].set_xlim(0, 1.05)\n",
    "for i, (m, v) in enumerate(zip(sorted_models[::-1], test_r2[::-1])):\n",
    "    axes[1].text(v + 0.01, i, f'{v:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# MAPE\n",
    "axes[2].barh(sorted_models[::-1], test_mape[::-1], color=colors[::-1], edgecolor='black')\n",
    "axes[2].set_xlabel('Test MAPE (%)')\n",
    "axes[2].set_title('MAPE (Lower is Better)', fontweight='bold')\n",
    "for i, (m, v) in enumerate(zip(sorted_models[::-1], test_mape[::-1])):\n",
    "    axes[2].text(v + 0.05, i, f'{v:.2f}%', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Overfitting Analysis\n",
    "\n",
    "### What is Overfitting?\n",
    "\n",
    "When a model learns the **training data too well** (including noise) and fails on new data.\n",
    "\n",
    "| Scenario | Train Error | Test Error | Problem |\n",
    "|----------|------------|------------|----------|\n",
    "| **Underfitting** | High | High | Model too simple |\n",
    "| **Good Fit** | Low | Low (similar) | Optimal complexity |\n",
    "| **Overfitting** | Very Low | High | Model too complex |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overfitting\n",
    "print(\"=\"*70)\n",
    "print(\"OVERFITTING ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(sorted_models))\n",
    "width = 0.35\n",
    "\n",
    "train_rmse = [results[m]['train_rmse'] for m in sorted_models]\n",
    "test_rmse = [results[m]['test_rmse'] for m in sorted_models]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, train_rmse, width, label='Train RMSE', color='steelblue', edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, test_rmse, width, label='Test RMSE', color='darkorange', edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('RMSE ($)')\n",
    "ax.set_title('Train vs Test RMSE - Overfitting Check', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sorted_models, rotation=15, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOverfitting Analysis:\")\n",
    "for name in sorted_models:\n",
    "    train = results[name]['train_rmse']\n",
    "    test = results[name]['test_rmse']\n",
    "    gap = test - train\n",
    "    gap_pct = (gap / train) * 100\n",
    "    \n",
    "    if gap_pct > 50:\n",
    "        status = \"OVERFITTING\"\n",
    "    elif gap_pct < 10:\n",
    "        status = \"Good generalization\"\n",
    "    else:\n",
    "        status = \"Slight overfitting\"\n",
    "    \n",
    "    print(f\"  {name}: Train={train:.2f}, Test={test:.2f}, Gap={gap_pct:.1f}% - {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Time Series Cross-Validation\n",
    "\n",
    "### Why Special CV for Time Series?\n",
    "\n",
    "Regular K-Fold CV randomly shuffles data - **BAD for time series!**\n",
    "\n",
    "**TimeSeriesSplit** preserves temporal order:\n",
    "```\n",
    "Fold 1: Train=[1,2]     Test=[3]\n",
    "Fold 2: Train=[1,2,3]   Test=[4]\n",
    "Fold 3: Train=[1,2,3,4] Test=[5]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TIME SERIES CROSS-VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 5-fold time series split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv_results = {}\n",
    "\n",
    "for name, info in models.items():\n",
    "    # Choose scaled or unscaled data\n",
    "    if info['needs_scaling']:\n",
    "        X_cv = X_train_scaled\n",
    "    else:\n",
    "        X_cv = X_train\n",
    "    \n",
    "    # Cross-validation\n",
    "    scores = cross_val_score(info['model'], X_cv, y_train, \n",
    "                            cv=tscv, scoring='neg_mean_squared_error')\n",
    "    rmse_scores = np.sqrt(-scores)\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'mean': rmse_scores.mean(),\n",
    "        'std': rmse_scores.std(),\n",
    "        'scores': rmse_scores\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Fold RMSE: {rmse_scores.round(2)}\")\n",
    "    print(f\"  Mean: ${rmse_scores.mean():.2f} (+/- ${rmse_scores.std()*2:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV results\n",
    "cv_sorted = dict(sorted(cv_results.items(), key=lambda x: x[1]['mean']))\n",
    "names_cv = list(cv_sorted.keys())\n",
    "means = [cv_sorted[n]['mean'] for n in names_cv]\n",
    "stds = [cv_sorted[n]['std'] for n in names_cv]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(names_cv)))\n",
    "bars = ax.barh(names_cv[::-1], means[::-1], xerr=stds[::-1],\n",
    "               color=colors[::-1], edgecolor='black', capsize=5)\n",
    "\n",
    "ax.set_xlabel('Cross-Validation RMSE ($)')\n",
    "ax.set_title('5-Fold Time Series Cross-Validation', fontweight='bold', fontsize=14)\n",
    "\n",
    "for bar, mean, std in zip(bars, means[::-1], stds[::-1]):\n",
    "    ax.text(bar.get_width() + std + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "            f'${mean:.2f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_cv_model = names_cv[0]\n",
    "print(f\"\\nBest Model (CV): {best_cv_model}\")\n",
    "print(f\"Mean CV RMSE: ${cv_sorted[best_cv_model]['mean']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part8'></a>\n",
    "# Part 8: Hyperparameter Tuning\n",
    "\n",
    "---\n",
    "\n",
    "## What is Hyperparameter Tuning?\n",
    "\n",
    "**Hyperparameters** are settings chosen BEFORE training (unlike model parameters learned during training).\n",
    "\n",
    "| Parameter Type | Example | Set By |\n",
    "|---------------|---------|--------|\n",
    "| **Model Parameters** | Weights, coefficients | Learned during training |\n",
    "| **Hyperparameters** | n_estimators, max_depth | Chosen before training |\n",
    "\n",
    "## Tuning the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Tune Random Forest (usually one of the best)\n",
    "print(\"\\nTuning Random Forest Regressor...\")\n",
    "print(\"\\nHyperparameter Grid:\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],      # Number of trees\n",
    "    'max_depth': [5, 10, 15, None],       # Tree depth (None = unlimited)\n",
    "    'min_samples_split': [2, 5, 10],      # Min samples to split a node\n",
    "    'min_samples_leaf': [1, 2, 4]         # Min samples in a leaf\n",
    "}\n",
    "\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "print(f\"\\nTotal combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "print(\"Using GridSearchCV with TimeSeriesSplit...\")\n",
    "\n",
    "# Grid Search with Time Series CV\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "tscv = TimeSeriesSplit(n_splits=3)  # Use 3 folds for speed\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TUNING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "best_rmse = np.sqrt(-grid_search.best_score_)\n",
    "print(f\"\\nBest CV RMSE: ${best_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tuned vs default\n",
    "print(\"=\"*70)\n",
    "print(\"TUNED MODEL PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Predict with tuned model\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_tuned = best_rf.predict(X_test)\n",
    "\n",
    "tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "tuned_r2 = r2_score(y_test, y_pred_tuned)\n",
    "tuned_mape = mean_absolute_percentage_error(y_test, y_pred_tuned) * 100\n",
    "\n",
    "default_rmse = results['Random Forest']['test_rmse']\n",
    "default_r2 = results['Random Forest']['test_r2']\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"  {'Metric':<15} {'Default':>15} {'Tuned':>15} {'Change':>15}\")\n",
    "print(f\"  {'-'*60}\")\n",
    "print(f\"  {'Test RMSE':<15} ${default_rmse:>14.2f} ${tuned_rmse:>14.2f} {(tuned_rmse-default_rmse)/default_rmse*100:>+14.1f}%\")\n",
    "print(f\"  {'Test R\u00b2':<15} {default_r2:>15.4f} {tuned_r2:>15.4f} {(tuned_r2-default_r2)*100:>+14.1f}%\")\n",
    "\n",
    "if tuned_rmse < default_rmse:\n",
    "    print(\"\\nTuning improved the model!\")\n",
    "else:\n",
    "    print(\"\\nDefault parameters were already good.\")\n",
    "\n",
    "# Add tuned model to results\n",
    "results['Random Forest (Tuned)'] = {\n",
    "    'model': best_rf,\n",
    "    'y_pred_test': y_pred_tuned,\n",
    "    'test_rmse': tuned_rmse,\n",
    "    'test_r2': tuned_r2,\n",
    "    'test_mape': tuned_mape\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part9'></a>\n",
    "# Part 9: Final Predictions & Visualization\n",
    "\n",
    "---\n",
    "\n",
    "## 9.1 Best Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on test RMSE\n",
    "best_model_name = min(results.items(), key=lambda x: x[1].get('test_rmse', float('inf')))[0]\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"FINAL PREDICTIONS - {best_model_name}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create predictions DataFrame\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual': y_test.values,\n",
    "    'Predicted': best_result['y_pred_test']\n",
    "}, index=y_test.index)\n",
    "\n",
    "predictions_df['Error'] = predictions_df['Predicted'] - predictions_df['Actual']\n",
    "predictions_df['Abs_Error'] = abs(predictions_df['Error'])\n",
    "predictions_df['Pct_Error'] = (predictions_df['Error'] / predictions_df['Actual']) * 100\n",
    "\n",
    "print(\"\\nLast 10 Predictions:\")\n",
    "print(predictions_df.tail(10).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Actual vs Predicted over time\n",
    "axes[0, 0].plot(predictions_df.index, predictions_df['Actual'], \n",
    "                label='Actual', color='black', linewidth=1.5)\n",
    "axes[0, 0].plot(predictions_df.index, predictions_df['Predicted'], \n",
    "                label='Predicted', color='red', linewidth=1.5, alpha=0.8)\n",
    "axes[0, 0].set_title(f'{TICKER} Actual vs Predicted - {best_model_name}', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Price ($)')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Scatter plot\n",
    "axes[0, 1].scatter(predictions_df['Actual'], predictions_df['Predicted'], \n",
    "                   alpha=0.5, color='steelblue', edgecolor='black')\n",
    "min_val = min(predictions_df['Actual'].min(), predictions_df['Predicted'].min())\n",
    "max_val = max(predictions_df['Actual'].max(), predictions_df['Predicted'].max())\n",
    "axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 1].set_title('Actual vs Predicted Scatter', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Actual Price ($)')\n",
    "axes[0, 1].set_ylabel('Predicted Price ($)')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Error distribution\n",
    "axes[1, 0].hist(predictions_df['Error'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].axvline(x=predictions_df['Error'].mean(), color='green', linestyle='--', \n",
    "                   linewidth=2, label=f\"Mean: ${predictions_df['Error'].mean():.2f}\")\n",
    "axes[1, 0].set_title('Prediction Error Distribution', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Error ($)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Error over time\n",
    "axes[1, 1].plot(predictions_df.index, predictions_df['Error'], color='purple', linewidth=1)\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "axes[1, 1].fill_between(predictions_df.index, predictions_df['Error'], \n",
    "                        where=(predictions_df['Error'] > 0), color='green', alpha=0.3)\n",
    "axes[1, 1].fill_between(predictions_df.index, predictions_df['Error'], \n",
    "                        where=(predictions_df['Error'] < 0), color='red', alpha=0.3)\n",
    "axes[1, 1].set_title('Prediction Error Over Time', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].set_ylabel('Error ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from tree-based model\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if 'Random Forest (Tuned)' in results:\n",
    "    model_for_importance = results['Random Forest (Tuned)']['model']\n",
    "elif 'Random Forest' in results:\n",
    "    model_for_importance = results['Random Forest']['model']\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': model_for_importance.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features:\")\n",
    "print(importance_df.head(15).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "top_20 = importance_df.head(20)\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_20)))\n",
    "\n",
    "bars = ax.barh(range(len(top_20)), top_20['Importance'].values, color=colors, edgecolor='black')\n",
    "ax.set_yticks(range(len(top_20)))\n",
    "ax.set_yticklabels(top_20['Feature'].values)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('Top 20 Feature Importance', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Lag features often dominate (yesterday's price predicts today's)\")\n",
    "print(\"- Technical indicators help capture momentum and trends\")\n",
    "print(\"- Calendar features (day/month) may show seasonal patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part10'></a>\n",
    "# Part 10: Summary and Conclusions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary dashboard\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "# 1. Model Comparison\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "model_names = [k for k in results.keys() if 'test_rmse' in results[k]]\n",
    "rmse_vals = [results[k]['test_rmse'] for k in model_names]\n",
    "sorted_idx = np.argsort(rmse_vals)\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(model_names)))\n",
    "bars = ax1.bar([model_names[i] for i in sorted_idx], [rmse_vals[i] for i in sorted_idx], \n",
    "               color=colors, edgecolor='black')\n",
    "ax1.set_ylabel('Test RMSE ($)')\n",
    "ax1.set_title('Model Performance Comparison', fontweight='bold', fontsize=14)\n",
    "ax1.tick_params(axis='x', rotation=15)\n",
    "for bar in bars:\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n",
    "             f'${bar.get_height():.2f}', ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# 2. Actual vs Predicted\n",
    "ax2 = fig.add_subplot(gs[1, :2])\n",
    "ax2.plot(predictions_df.index, predictions_df['Actual'], label='Actual', color='black', linewidth=1.5)\n",
    "ax2.plot(predictions_df.index, predictions_df['Predicted'], label='Predicted', color='red', linewidth=1.5, alpha=0.8)\n",
    "ax2.set_title(f'Best Model: {best_model_name}', fontweight='bold', fontsize=12)\n",
    "ax2.set_ylabel('Price ($)')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Error Distribution\n",
    "ax3 = fig.add_subplot(gs[1, 2])\n",
    "ax3.hist(predictions_df['Error'], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax3.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "ax3.set_title('Error Distribution', fontweight='bold', fontsize=12)\n",
    "ax3.set_xlabel('Error ($)')\n",
    "\n",
    "# 4. Top Features\n",
    "ax4 = fig.add_subplot(gs[2, 0])\n",
    "top_10 = importance_df.head(10)\n",
    "ax4.barh(top_10['Feature'][::-1], top_10['Importance'][::-1], \n",
    "         color=plt.cm.viridis(np.linspace(0.3, 0.9, 10))[::-1], edgecolor='black')\n",
    "ax4.set_xlabel('Importance')\n",
    "ax4.set_title('Top 10 Features', fontweight='bold', fontsize=12)\n",
    "\n",
    "# 5. Metrics Summary\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "ax5.axis('off')\n",
    "metrics_text = f\"\"\"\n",
    "BEST MODEL: {best_model_name}\n",
    "\n",
    "Test Metrics:\n",
    "  RMSE:  ${best_result['test_rmse']:.2f}\n",
    "  R2:    {best_result['test_r2']:.4f}\n",
    "  MAPE:  {best_result['test_mape']:.2f}%\n",
    "\n",
    "Dataset:\n",
    "  Stock: {TICKER}\n",
    "  Period: {START_DATE} to {END_DATE}\n",
    "  Train: {len(X_train)} days\n",
    "  Test: {len(X_test)} days\n",
    "  Features: {len(feature_cols)}\n",
    "\"\"\"\n",
    "ax5.text(0.1, 0.9, metrics_text, transform=ax5.transAxes, fontsize=10,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))\n",
    "\n",
    "# 6. Full Price History\n",
    "ax6 = fig.add_subplot(gs[2, 2])\n",
    "ax6.plot(df.index, df['Close'], color='steelblue', linewidth=1)\n",
    "ax6.fill_between(df.index, df['Close'], alpha=0.3)\n",
    "ax6.axvline(x=X_test.index[0], color='red', linestyle='--', label='Split')\n",
    "ax6.set_title(f'{TICKER} Full History', fontweight='bold', fontsize=12)\n",
    "ax6.set_ylabel('Price ($)')\n",
    "ax6.legend()\n",
    "\n",
    "plt.suptitle('STOCK PRICE PREDICTOR - SUMMARY DASHBOARD', fontweight='bold', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### 1. Time Series is Different\n",
    "\n",
    "| Aspect | What We Learned |\n",
    "|--------|----------------|\n",
    "| **Data Split** | Must be chronological, not random |\n",
    "| **Cross-Validation** | Use TimeSeriesSplit, not K-Fold |\n",
    "| **Feature Engineering** | Technical indicators encode domain knowledge |\n",
    "| **Lag Features** | Past values are crucial predictors |\n",
    "| **Seasonality** | Day-of-week and monthly patterns exist |\n",
    "\n",
    "### 2. Model Selection Insights\n",
    "\n",
    "| Finding | Explanation |\n",
    "|---------|-------------|\n",
    "| **Tree models work well** | Capture non-linear patterns without assuming relationships |\n",
    "| **Linear models are baseline** | Good for comparison, but may miss complex patterns |\n",
    "| **Tuning helps** | Hyperparameters can significantly improve performance |\n",
    "\n",
    "### 3. Feature Importance\n",
    "\n",
    "| Feature Type | Why Important |\n",
    "|-------------|---------------|\n",
    "| **Lag features** | Yesterday's price strongly predicts today's |\n",
    "| **Technical indicators** | Encode trader psychology and patterns |\n",
    "| **Volume features** | Confirm trends and signal reversals |\n",
    "\n",
    "### 4. Limitations & Disclaimers\n",
    "\n",
    "**This is for EDUCATIONAL purposes only!**\n",
    "\n",
    "- Past performance does NOT guarantee future results\n",
    "- Models cannot predict black swan events\n",
    "- Markets are influenced by news, sentiment, and unpredictable factors\n",
    "- Real trading requires risk management, not just prediction\n",
    "\n",
    "---\n",
    "\n",
    "## Checklist\n",
    "\n",
    "- [x] Downloaded stock data from Yahoo Finance\n",
    "- [x] Understood OHLCV data structure\n",
    "- [x] Explored price trends and returns\n",
    "- [x] Analyzed seasonality (day-of-week, monthly patterns)\n",
    "- [x] Performed time series decomposition (trend, seasonal, residual)\n",
    "- [x] Created technical indicators (SMA, EMA, RSI, MACD, Bollinger Bands)\n",
    "- [x] Engineered lag features\n",
    "- [x] Used time-based train-test split (no data leakage!)\n",
    "- [x] Trained multiple regression models\n",
    "- [x] Explained why each model was chosen\n",
    "- [x] Compared using RMSE, MAE, R\u00b2, MAPE\n",
    "- [x] Performed hyperparameter tuning\n",
    "- [x] Analyzed feature importance\n",
    "- [x] Visualized predictions\n",
    "\n",
    "---\n",
    "\n",
    "**End of Stock Price Predictor Tutorial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"STOCK PRICE PREDICTOR - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca DATASET\")\n",
    "print(f\"   Stock: {TICKER}\")\n",
    "print(f\"   Period: {START_DATE} to {END_DATE}\")\n",
    "print(f\"   Trading Days: {len(df)}\")\n",
    "print(f\"   Features Created: {len(feature_cols)}\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfc6 BEST MODEL\")\n",
    "print(f\"   Name: {best_model_name}\")\n",
    "print(f\"   Test RMSE: ${best_result['test_rmse']:.2f}\")\n",
    "print(f\"   Test R\u00b2: {best_result['test_r2']:.4f}\")\n",
    "print(f\"   Test MAPE: {best_result['test_mape']:.2f}%\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 ALL MODEL RMSE (Test)\")\n",
    "for name in sorted(results.keys(), key=lambda x: results[x].get('test_rmse', float('inf'))):\n",
    "    if 'test_rmse' in results[name]:\n",
    "        print(f\"   {name}: ${results[name]['test_rmse']:.2f}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udd11 TOP 5 IMPORTANT FEATURES\")\n",
    "for _, row in importance_df.head(5).iterrows():\n",
    "    print(f\"   {row['Feature']}: {row['Importance']*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROJECT COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n\u26a0\ufe0f  DISCLAIMER: This is for EDUCATIONAL purposes only.\")\n",
    "print(\"    Stock markets are unpredictable.\")\n",
    "print(\"    Do NOT use this for real trading decisions!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}