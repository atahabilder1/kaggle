{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model From Scratch - Complete Tutorial\n",
    "\n",
    "**Author:** Anik Tahabilder  \n",
    "**Project:** 14 of 22 - Kaggle ML Portfolio  \n",
    "**Difficulty:** 9/10 | **Learning Value:** 10/10\n",
    "\n",
    "---\n",
    "\n",
    "## What Will You Learn?\n",
    "\n",
    "This tutorial builds language models **from scratch**, progressing from simple to complex:\n",
    "\n",
    "| Part | Model | Complexity | Key Concept |\n",
    "|------|-------|------------|-------------|\n",
    "| 1 | N-gram Models | Basic | Count-based probability |\n",
    "| 2 | Neural LM | Intermediate | Word embeddings |\n",
    "| 3 | Attention | Advanced | Context-aware weights |\n",
    "| 4 | Transformer | Expert | Self-attention, parallelization |\n",
    "\n",
    "---\n",
    "\n",
    "## The Evolution of Language Models\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                    LANGUAGE MODEL EVOLUTION                              │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│   N-gram          Neural LM         Attention         Transformer       │\n",
    "│   (1990s)         (2003)            (2014)            (2017)            │\n",
    "│                                                                         │\n",
    "│   ┌─────┐         ┌─────┐          ┌─────┐           ┌─────┐           │\n",
    "│   │Count│  ───>   │ RNN │   ───>   │Attn │   ───>    │ ⚡  │           │\n",
    "│   │Based│         │LSTM │          │ RNN │           │Self │           │\n",
    "│   └─────┘         └─────┘          └─────┘           │Attn │           │\n",
    "│                                                       └─────┘           │\n",
    "│   P(w|history)    Hidden states    Weighted context   Parallel         │\n",
    "│   from counts     capture context  dynamic weights    attention        │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: What is a Language Model?](#part1)\n",
    "2. [Part 2: N-gram Language Models](#part2)\n",
    "3. [Part 3: Word Embeddings](#part3)\n",
    "4. [Part 4: Attention Mechanism](#part4)\n",
    "5. [Part 5: Transformer Architecture](#part5)\n",
    "6. [Part 6: Building Mini-GPT](#part6)\n",
    "7. [Part 7: Training & Text Generation](#part7)\n",
    "8. [Part 8: Summary & Comparison](#part8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part1'></a>\n",
    "# Part 1: What is a Language Model?\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Definition\n",
    "\n",
    "A **Language Model** assigns probabilities to sequences of words.\n",
    "\n",
    "```\n",
    "Given: \"The cat sat on the\"\n",
    "Predict: P(mat) = 0.3, P(floor) = 0.2, P(dog) = 0.01, ...\n",
    "```\n",
    "\n",
    "## 1.2 Why Language Models Matter\n",
    "\n",
    "| Application | How LM Helps |\n",
    "|-------------|-------------|\n",
    "| **Text Generation** | GPT, ChatGPT |\n",
    "| **Machine Translation** | Choose fluent translations |\n",
    "| **Speech Recognition** | Disambiguate similar sounds |\n",
    "| **Autocomplete** | Predict next word |\n",
    "| **Spelling Correction** | \"teh\" → \"the\" |\n",
    "\n",
    "## 1.3 The Core Problem\n",
    "\n",
    "**Goal:** Estimate P(w₁, w₂, ..., wₙ) - probability of a sentence\n",
    "\n",
    "Using **chain rule**:\n",
    "```\n",
    "P(w₁, w₂, w₃, w₄) = P(w₁) × P(w₂|w₁) × P(w₃|w₁,w₂) × P(w₄|w₁,w₂,w₃)\n",
    "```\n",
    "\n",
    "**Problem:** As sequence grows, conditioning history becomes intractable!\n",
    "\n",
    "**Solution:** Make simplifying assumptions (N-gram) or learn representations (Neural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP AND IMPORTS\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch for neural models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LANGUAGE MODEL FROM SCRATCH - TUTORIAL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"\\nAll libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD TEXT DATA\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING TEXT CORPUS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sample corpus for demonstration\n",
    "# Using famous quotes and simple sentences for clear learning\n",
    "\n",
    "corpus = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "A journey of a thousand miles begins with a single step.\n",
    "To be or not to be that is the question.\n",
    "All that glitters is not gold.\n",
    "The only thing we have to fear is fear itself.\n",
    "In the beginning was the word and the word was with god.\n",
    "It was the best of times it was the worst of times.\n",
    "The cat sat on the mat.\n",
    "The dog chased the cat around the house.\n",
    "I think therefore I am.\n",
    "Knowledge is power.\n",
    "Time flies like an arrow.\n",
    "The early bird catches the worm.\n",
    "Actions speak louder than words.\n",
    "Practice makes perfect.\n",
    "The pen is mightier than the sword.\n",
    "Where there is a will there is a way.\n",
    "Rome was not built in a day.\n",
    "When in Rome do as the Romans do.\n",
    "A picture is worth a thousand words.\n",
    "The grass is always greener on the other side.\n",
    "Birds of a feather flock together.\n",
    "Two heads are better than one.\n",
    "The apple does not fall far from the tree.\n",
    "You cannot judge a book by its cover.\n",
    "Every cloud has a silver lining.\n",
    "A stitch in time saves nine.\n",
    "Better late than never.\n",
    "Curiosity killed the cat.\n",
    "Fortune favors the bold.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and tokenize text.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation except periods (sentence boundaries)\n",
    "    text = re.sub(r'[^a-z\\s.]', '', text)\n",
    "    # Split into sentences\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    # Tokenize each sentence\n",
    "    tokenized = [s.split() for s in sentences]\n",
    "    return tokenized\n",
    "\n",
    "sentences = preprocess_text(corpus)\n",
    "\n",
    "# Create vocabulary\n",
    "all_words = [word for sent in sentences for word in sent]\n",
    "vocab = sorted(set(all_words))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "print(f\"Total words: {len(all_words)}\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"\\nSample sentences:\")\n",
    "for i, sent in enumerate(sentences[:5]):\n",
    "    print(f\"  {i+1}. {' '.join(sent)}\")\n",
    "\n",
    "print(f\"\\nFirst 20 vocabulary words:\")\n",
    "print(vocab[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part2'></a>\n",
    "# Part 2: N-gram Language Models\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 The N-gram Assumption\n",
    "\n",
    "**Markov Assumption:** The probability of a word depends only on the previous N-1 words.\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│                        N-GRAM MODELS                           │\n",
    "├────────────────────────────────────────────────────────────────┤\n",
    "│                                                                │\n",
    "│  Sentence: \"The cat sat on the mat\"                           │\n",
    "│                                                                │\n",
    "│  UNIGRAM (N=1): P(word) - Each word independent               │\n",
    "│  ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐             │\n",
    "│  │ the │ │ cat │ │ sat │ │ on  │ │ the │ │ mat │             │\n",
    "│  └─────┘ └─────┘ └─────┘ └─────┘ └─────┘ └─────┘             │\n",
    "│                                                                │\n",
    "│  BIGRAM (N=2): P(word | prev_word)                            │\n",
    "│  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐             │\n",
    "│  │the→cat │ │cat→sat │ │sat→on  │ │on→the  │ ...           │\n",
    "│  └─────────┘ └─────────┘ └─────────┘ └─────────┘             │\n",
    "│                                                                │\n",
    "│  TRIGRAM (N=3): P(word | prev_2_words)                        │\n",
    "│  ┌───────────────┐ ┌───────────────┐                          │\n",
    "│  │the,cat→sat   │ │cat,sat→on    │ ...                       │\n",
    "│  └───────────────┘ └───────────────┘                          │\n",
    "│                                                                │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## 2.2 N-gram Probability Formula\n",
    "\n",
    "| N-gram | Formula | Example |\n",
    "|--------|---------|--------|\n",
    "| Unigram | P(w) = C(w) / N | P(the) = 10/100 |\n",
    "| Bigram | P(w₂\\|w₁) = C(w₁,w₂) / C(w₁) | P(cat\\|the) = C(the,cat)/C(the) |\n",
    "| Trigram | P(w₃\\|w₁,w₂) = C(w₁,w₂,w₃) / C(w₁,w₂) | P(sat\\|the,cat) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# N-GRAM LANGUAGE MODEL FROM SCRATCH\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"N-GRAM LANGUAGE MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    \"\"\"\n",
    "    N-gram Language Model implementation.\n",
    "    \n",
    "    Supports:\n",
    "    - Unigram (n=1)\n",
    "    - Bigram (n=2)  \n",
    "    - Trigram (n=3)\n",
    "    - Any N-gram\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n=2):\n",
    "        self.n = n\n",
    "        self.ngram_counts = defaultdict(Counter)\n",
    "        self.context_counts = Counter()\n",
    "        self.vocab = set()\n",
    "        \n",
    "    def train(self, sentences):\n",
    "        \"\"\"\n",
    "        Train the N-gram model on sentences.\n",
    "        \n",
    "        For each sentence, we:\n",
    "        1. Add start tokens <s> for context\n",
    "        2. Count all n-grams\n",
    "        3. Count all (n-1)-gram contexts\n",
    "        \"\"\"\n",
    "        for sentence in sentences:\n",
    "            # Add start tokens\n",
    "            padded = ['<s>'] * (self.n - 1) + sentence + ['</s>']\n",
    "            \n",
    "            # Update vocabulary\n",
    "            self.vocab.update(sentence)\n",
    "            \n",
    "            # Count n-grams\n",
    "            for i in range(len(padded) - self.n + 1):\n",
    "                # Context is the first n-1 words\n",
    "                context = tuple(padded[i:i + self.n - 1])\n",
    "                # Target is the last word\n",
    "                word = padded[i + self.n - 1]\n",
    "                \n",
    "                self.ngram_counts[context][word] += 1\n",
    "                self.context_counts[context] += 1\n",
    "    \n",
    "    def probability(self, word, context):\n",
    "        \"\"\"\n",
    "        Calculate P(word | context).\n",
    "        \n",
    "        P(word | context) = Count(context, word) / Count(context)\n",
    "        \"\"\"\n",
    "        context = tuple(context[-(self.n-1):])  # Take last n-1 words\n",
    "        \n",
    "        if self.context_counts[context] == 0:\n",
    "            # Unseen context - return uniform probability\n",
    "            return 1 / len(self.vocab)\n",
    "        \n",
    "        count = self.ngram_counts[context][word]\n",
    "        total = self.context_counts[context]\n",
    "        \n",
    "        return count / total if count > 0 else 0\n",
    "    \n",
    "    def probability_smoothed(self, word, context, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Calculate P(word | context) with Laplace (add-alpha) smoothing.\n",
    "        \n",
    "        P(word | context) = (Count(context, word) + α) / (Count(context) + α × |V|)\n",
    "        \n",
    "        This prevents zero probabilities for unseen n-grams.\n",
    "        \"\"\"\n",
    "        context = tuple(context[-(self.n-1):])\n",
    "        \n",
    "        count = self.ngram_counts[context][word]\n",
    "        total = self.context_counts[context]\n",
    "        V = len(self.vocab) + 2  # +2 for <s> and </s>\n",
    "        \n",
    "        return (count + alpha) / (total + alpha * V)\n",
    "    \n",
    "    def generate(self, start_words=None, max_length=20, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate text using the trained model.\n",
    "        \n",
    "        Temperature:\n",
    "        - Low (0.1): More deterministic, picks high probability words\n",
    "        - High (2.0): More random, explores diverse options\n",
    "        \"\"\"\n",
    "        if start_words is None:\n",
    "            # Start with start tokens\n",
    "            current = ['<s>'] * (self.n - 1)\n",
    "        else:\n",
    "            current = ['<s>'] * (self.n - 1) + start_words\n",
    "        \n",
    "        generated = list(start_words) if start_words else []\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            context = tuple(current[-(self.n-1):])\n",
    "            \n",
    "            # Get probability distribution\n",
    "            if context not in self.ngram_counts:\n",
    "                # Random word if context unseen\n",
    "                next_word = random.choice(list(self.vocab))\n",
    "            else:\n",
    "                words = list(self.ngram_counts[context].keys())\n",
    "                counts = np.array(list(self.ngram_counts[context].values()), dtype=float)\n",
    "                \n",
    "                # Apply temperature\n",
    "                probs = counts ** (1 / temperature)\n",
    "                probs = probs / probs.sum()\n",
    "                \n",
    "                next_word = np.random.choice(words, p=probs)\n",
    "            \n",
    "            if next_word == '</s>':\n",
    "                break\n",
    "                \n",
    "            generated.append(next_word)\n",
    "            current.append(next_word)\n",
    "        \n",
    "        return generated\n",
    "    \n",
    "    def perplexity(self, sentences):\n",
    "        \"\"\"\n",
    "        Calculate perplexity on test sentences.\n",
    "        \n",
    "        Perplexity = 2^(-average log probability)\n",
    "        Lower perplexity = better model\n",
    "        \"\"\"\n",
    "        log_prob_sum = 0\n",
    "        word_count = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            padded = ['<s>'] * (self.n - 1) + sentence + ['</s>']\n",
    "            \n",
    "            for i in range(self.n - 1, len(padded)):\n",
    "                context = padded[i - self.n + 1:i]\n",
    "                word = padded[i]\n",
    "                \n",
    "                prob = self.probability_smoothed(word, context)\n",
    "                log_prob_sum += math.log2(prob) if prob > 0 else -100\n",
    "                word_count += 1\n",
    "        \n",
    "        return 2 ** (-log_prob_sum / word_count)\n",
    "\n",
    "print(\"NGramLanguageModel class created!\")\n",
    "print(\"\\nKey methods:\")\n",
    "print(\"  - train(sentences): Learn n-gram counts\")\n",
    "print(\"  - probability(word, context): Get P(word|context)\")\n",
    "print(\"  - generate(start_words): Generate new text\")\n",
    "print(\"  - perplexity(sentences): Evaluate model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN AND COMPARE N-GRAM MODELS\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING N-GRAM MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train models with different N\n",
    "models = {}\n",
    "for n in [1, 2, 3, 4]:\n",
    "    model = NGramLanguageModel(n=n)\n",
    "    model.train(sentences)\n",
    "    models[n] = model\n",
    "    print(f\"\\n{n}-gram model trained\")\n",
    "    print(f\"  Unique contexts: {len(model.context_counts)}\")\n",
    "\n",
    "# Demonstrate bigram probabilities\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BIGRAM PROBABILITIES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "bigram = models[2]\n",
    "\n",
    "# Show probabilities after \"the\"\n",
    "context = ('the',)\n",
    "print(f\"\\nAfter 'the', most likely words:\")\n",
    "if context in bigram.ngram_counts:\n",
    "    sorted_words = sorted(bigram.ngram_counts[context].items(), \n",
    "                         key=lambda x: x[1], reverse=True)\n",
    "    for word, count in sorted_words[:10]:\n",
    "        prob = bigram.probability(word, ['the'])\n",
    "        print(f\"  P({word}|the) = {count}/{bigram.context_counts[context]} = {prob:.3f}\")\n",
    "\n",
    "# Visualize bigram distribution\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BIGRAM VISUALIZATION\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top bigrams\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Top context words\n",
    "ax1 = axes[0]\n",
    "top_contexts = bigram.context_counts.most_common(15)\n",
    "contexts, counts = zip(*top_contexts)\n",
    "context_labels = [' '.join(c) for c in contexts]\n",
    "ax1.barh(context_labels, counts, color='steelblue', edgecolor='black')\n",
    "ax1.set_xlabel('Count')\n",
    "ax1.set_title('Most Common Bigram Contexts', fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Probability distribution after \"the\"\n",
    "ax2 = axes[1]\n",
    "context = ('the',)\n",
    "if context in bigram.ngram_counts:\n",
    "    sorted_words = sorted(bigram.ngram_counts[context].items(), \n",
    "                         key=lambda x: x[1], reverse=True)[:10]\n",
    "    words, counts = zip(*sorted_words)\n",
    "    total = sum(counts)\n",
    "    probs = [c/bigram.context_counts[context] for c in counts]\n",
    "    ax2.barh(words, probs, color='coral', edgecolor='black')\n",
    "    ax2.set_xlabel('P(word | \"the\")')\n",
    "    ax2.set_title('Probability Distribution After \"the\"', fontweight='bold')\n",
    "    ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEXT GENERATION WITH N-GRAMS\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"TEXT GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for n in [2, 3, 4]:\n",
    "    print(f\"\\n{n}-gram model generations:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i in range(3):\n",
    "        generated = models[n].generate(start_words=['the'], max_length=10, temperature=0.8)\n",
    "        print(f\"  {i+1}. {' '.join(generated)}\")\n",
    "\n",
    "# Show effect of temperature\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EFFECT OF TEMPERATURE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "trigram = models[3]\n",
    "for temp in [0.3, 1.0, 2.0]:\n",
    "    print(f\"\\nTemperature = {temp}:\")\n",
    "    for i in range(3):\n",
    "        generated = trigram.generate(start_words=['the'], max_length=8, temperature=temp)\n",
    "        print(f\"  {' '.join(generated)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL EVALUATION - PERPLEXITY\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL EVALUATION - PERPLEXITY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "PERPLEXITY EXPLAINED:\n",
    "=====================\n",
    "- Measures how \"surprised\" the model is by test data\n",
    "- Lower perplexity = better model\n",
    "- Perplexity of N means the model is as confused as choosing\n",
    "  uniformly among N words at each position\n",
    "\n",
    "Formula: PP(W) = 2^(-1/N × Σ log₂ P(wᵢ|context))\n",
    "\"\"\")\n",
    "\n",
    "# Calculate perplexity for different N\n",
    "test_sentences = sentences[::3]  # Every 3rd sentence as test\n",
    "\n",
    "perplexities = {}\n",
    "for n in [1, 2, 3, 4]:\n",
    "    pp = models[n].perplexity(test_sentences)\n",
    "    perplexities[n] = pp\n",
    "    print(f\"{n}-gram perplexity: {pp:.2f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(list(perplexities.keys()), list(perplexities.values()), \n",
    "        color='steelblue', edgecolor='black')\n",
    "plt.xlabel('N (N-gram order)')\n",
    "plt.ylabel('Perplexity (lower is better)')\n",
    "plt.title('Perplexity vs N-gram Order', fontweight='bold')\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInsight: Higher N generally gives lower perplexity on training data,\")\n",
    "print(\"but may overfit (high perplexity on unseen data).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part3'></a>\n",
    "# Part 3: Word Embeddings\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Problem with N-grams\n",
    "\n",
    "| Limitation | Description |\n",
    "|------------|-------------|\n",
    "| **Sparsity** | Most n-grams never seen in training |\n",
    "| **No Generalization** | \"cat\" and \"dog\" are equally different from \"car\" |\n",
    "| **Fixed Context** | Can't capture long-range dependencies |\n",
    "\n",
    "## 3.2 Word Embeddings Solution\n",
    "\n",
    "Instead of treating words as discrete symbols, represent them as **dense vectors**.\n",
    "\n",
    "```\n",
    "One-hot (sparse):              Embedding (dense):\n",
    "\"cat\" = [0,0,1,0,0,0,...]      \"cat\" = [0.2, -0.5, 0.8, 0.1, ...]\n",
    "\"dog\" = [0,0,0,1,0,0,...]      \"dog\" = [0.3, -0.4, 0.7, 0.2, ...]\n",
    "                                        ↑ Similar vectors!\n",
    "```\n",
    "\n",
    "## 3.3 Key Property: Semantic Similarity\n",
    "\n",
    "```\n",
    "king - man + woman ≈ queen\n",
    "Paris - France + Italy ≈ Rome\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# WORD EMBEDDINGS FROM SCRATCH\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"WORD EMBEDDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class SimpleEmbedding:\n",
    "    \"\"\"\n",
    "    Simple word embedding using co-occurrence matrix + SVD.\n",
    "    \n",
    "    Steps:\n",
    "    1. Build word-word co-occurrence matrix\n",
    "    2. Apply SVD to reduce dimensionality\n",
    "    3. Use left singular vectors as embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=50, window_size=2):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.embeddings = None\n",
    "        \n",
    "    def fit(self, sentences):\n",
    "        \"\"\"Build embeddings from sentences.\"\"\"\n",
    "        # Build vocabulary\n",
    "        all_words = [w for sent in sentences for w in sent]\n",
    "        vocab = sorted(set(all_words))\n",
    "        self.word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "        self.idx_to_word = {i: w for w, i in self.word_to_idx.items()}\n",
    "        V = len(vocab)\n",
    "        \n",
    "        # Build co-occurrence matrix\n",
    "        cooccurrence = np.zeros((V, V))\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                word_idx = self.word_to_idx[word]\n",
    "                \n",
    "                # Look at words in window\n",
    "                start = max(0, i - self.window_size)\n",
    "                end = min(len(sentence), i + self.window_size + 1)\n",
    "                \n",
    "                for j in range(start, end):\n",
    "                    if i != j:\n",
    "                        context_idx = self.word_to_idx[sentence[j]]\n",
    "                        cooccurrence[word_idx, context_idx] += 1\n",
    "        \n",
    "        # Apply log transform (like GloVe)\n",
    "        cooccurrence = np.log1p(cooccurrence)\n",
    "        \n",
    "        # SVD\n",
    "        U, S, Vt = np.linalg.svd(cooccurrence, full_matrices=False)\n",
    "        \n",
    "        # Use top-k singular vectors\n",
    "        k = min(self.embedding_dim, len(S))\n",
    "        self.embeddings = U[:, :k] * np.sqrt(S[:k])\n",
    "        \n",
    "        print(f\"Built embeddings: {self.embeddings.shape}\")\n",
    "        \n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"Get embedding vector for a word.\"\"\"\n",
    "        if word not in self.word_to_idx:\n",
    "            return None\n",
    "        return self.embeddings[self.word_to_idx[word]]\n",
    "    \n",
    "    def most_similar(self, word, top_k=5):\n",
    "        \"\"\"Find most similar words using cosine similarity.\"\"\"\n",
    "        if word not in self.word_to_idx:\n",
    "            return []\n",
    "        \n",
    "        word_vec = self.get_embedding(word)\n",
    "        \n",
    "        # Cosine similarity with all words\n",
    "        similarities = []\n",
    "        for other_word, idx in self.word_to_idx.items():\n",
    "            if other_word != word:\n",
    "                other_vec = self.embeddings[idx]\n",
    "                # Cosine similarity\n",
    "                sim = np.dot(word_vec, other_vec) / (np.linalg.norm(word_vec) * np.linalg.norm(other_vec) + 1e-8)\n",
    "                similarities.append((other_word, sim))\n",
    "        \n",
    "        return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "# Train embeddings\n",
    "emb = SimpleEmbedding(embedding_dim=30, window_size=2)\n",
    "emb.fit(sentences)\n",
    "\n",
    "# Show similar words\n",
    "print(\"\\nMost similar words:\")\n",
    "for word in ['the', 'cat', 'is', 'not']:\n",
    "    similar = emb.most_similar(word, top_k=5)\n",
    "    print(f\"  '{word}' → {[(w, f'{s:.3f}') for w, s in similar]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings with t-SNE\n",
    "print(\"=\"*70)\n",
    "print(\"EMBEDDING VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Get embeddings for visualization\n",
    "words_to_plot = list(emb.word_to_idx.keys())[:50]  # Top 50 words\n",
    "vectors = np.array([emb.get_embedding(w) for w in words_to_plot])\n",
    "\n",
    "# t-SNE reduction to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(15, len(words_to_plot)-1))\n",
    "vectors_2d = tsne.fit_transform(vectors)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.6, s=100)\n",
    "\n",
    "for i, word in enumerate(words_to_plot):\n",
    "    plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), fontsize=9)\n",
    "\n",
    "plt.title('Word Embeddings Visualization (t-SNE)', fontweight='bold', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Similar words should cluster together!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part4'></a>\n",
    "# Part 4: The Attention Mechanism\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 Why Attention?\n",
    "\n",
    "**Problem with RNNs/LSTMs:**\n",
    "- Information bottleneck: Entire sequence compressed into fixed-size vector\n",
    "- Long-range dependencies hard to capture\n",
    "- Sequential processing (slow)\n",
    "\n",
    "**Attention Solution:** \n",
    "Look at ALL positions and weight them by relevance!\n",
    "\n",
    "## 4.2 Attention Intuition\n",
    "\n",
    "```\n",
    "Query: \"What word should come next?\"\n",
    "\n",
    "Sentence: \"The cat sat on the ___\"\n",
    "\n",
    "Attention looks at all words and asks:\n",
    "- \"The\" → relevance = 0.1\n",
    "- \"cat\" → relevance = 0.3  (subject, might be relevant)\n",
    "- \"sat\" → relevance = 0.2  (action)\n",
    "- \"on\" → relevance = 0.3   (preposition, next word is object)\n",
    "- \"the\" → relevance = 0.1\n",
    "\n",
    "Weighted sum → context vector → predict \"mat\"\n",
    "```\n",
    "\n",
    "## 4.3 Attention Formula\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q × K^T / √d_k) × V\n",
    "\n",
    "Where:\n",
    "- Q (Query): What am I looking for?\n",
    "- K (Key): What do I contain?\n",
    "- V (Value): What information do I provide?\n",
    "- d_k: Dimension of keys (for scaling)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ATTENTION MECHANISM FROM SCRATCH\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"ATTENTION MECHANISM FROM SCRATCH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        query: (batch, seq_len, d_k)\n",
    "        key: (batch, seq_len, d_k)\n",
    "        value: (batch, seq_len, d_v)\n",
    "        mask: Optional mask for padding/future tokens\n",
    "    \n",
    "    Returns:\n",
    "        output: (batch, seq_len, d_v)\n",
    "        attention_weights: (batch, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = query.shape[-1]\n",
    "    \n",
    "    # Step 1: Compute attention scores\n",
    "    # Q × K^T → (batch, seq_len, seq_len)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    # This prevents softmax from having extremely small gradients\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Apply mask (optional)\n",
    "    # Set masked positions to -inf so softmax gives 0\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Step 4: Softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 5: Weighted sum of values\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Demonstrate attention\n",
    "print(\"\\nDemonstrating Attention Mechanism:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Example: 1 batch, 5 tokens, 4-dimensional embeddings\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "d_model = 4\n",
    "\n",
    "# Random embeddings for \"The cat sat on mat\"\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# In self-attention, Q=K=V come from same input\n",
    "query = key = value = x\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(query, key, value)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nAttention weights (each row sums to 1):\")\n",
    "print(attn_weights[0].detach().numpy().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize attention weights\nprint(\"=\"*70)\nprint(\"ATTENTION VISUALIZATION\")\nprint(\"=\"*70)\n\nwords = ['The', 'cat', 'sat', 'on', 'mat']\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Attention heatmap\nax1 = axes[0]\nsns.heatmap(attn_weights[0].detach().numpy(), \n            xticklabels=words, yticklabels=words,\n            annot=True, fmt='.2f', cmap='Blues', ax=ax1)\nax1.set_title('Attention Weights', fontweight='bold')\nax1.set_xlabel('Key (attending to)')\nax1.set_ylabel('Query (attending from)')\n\n# Causal mask attention (for language models)\nax2 = axes[1]\n\n# Create causal mask (lower triangular)\ncausal_mask = torch.tril(torch.ones(seq_len, seq_len))\n_, causal_attn = scaled_dot_product_attention(query, key, value, mask=causal_mask)\n\nsns.heatmap(causal_attn[0].detach().numpy(), \n            xticklabels=words, yticklabels=words,\n            annot=True, fmt='.2f', cmap='Blues', ax=ax2)\nax2.set_title('Causal Attention (Can\\'t See Future)', fontweight='bold')\nax2.set_xlabel('Key')\nax2.set_ylabel('Query')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nNote: Causal attention only looks at previous tokens (lower triangle).\")\nprint(\"This is essential for language models - can't see future words!\")"
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# REAL-WORLD EXAMPLE: COREFERENCE RESOLUTION\n# ============================================================\nprint(\"=\"*70)\nprint(\"ATTENTION FOR COREFERENCE RESOLUTION\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nCOREFERENCE RESOLUTION:\n=======================\nWhen we see a pronoun like \"it\", attention helps determine what it refers to.\n\nExample sentence:\n\"The animal didn't cross the street because it was too tired.\"\n\nQuestion: What does \"it\" refer to?\n- \"it\" could refer to \"animal\" or \"street\"\n- Context tells us: \"it was too tired\" → animals get tired, streets don't\n- So \"it\" = \"animal\"\n\nThis is exactly what attention learns to do!\n\"\"\")\n\n# Simulate attention weights for coreference\ncoref_words = ['The', 'animal', 'didn\\'t', 'cross', 'the', 'street', 'because', 'it', 'was', 'too', 'tired']\n\n# Simulated attention weights when \"it\" is the query\n# \"it\" should attend strongly to \"animal\" (0.63) and somewhat to \"street\" (0.31)\nit_attention = np.array([0.00, 0.63, 0.00, 0.02, 0.00, 0.31, 0.02, 0.00, 0.01, 0.00, 0.01])\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Bar chart of attention weights\nax1 = axes[0]\ncolors = ['#90EE90' if w in ['animal', 'it'] else '#ADD8E6' if w == 'street' else 'lightgray' \n          for w in coref_words]\nbars = ax1.bar(coref_words, it_attention, color=colors, edgecolor='black')\nax1.set_ylabel('Attention Weight')\nax1.set_title('What does \"it\" attend to?', fontweight='bold', fontsize=12)\nax1.set_xticklabels(coref_words, rotation=45, ha='right')\n\n# Highlight the key relationship\nax1.annotate('', xy=(1, 0.63), xytext=(7, 0.4),\n            arrowprops=dict(arrowstyle='->', color='blue', lw=2))\nax1.text(4, 0.55, '\"it\" → \"animal\"', fontsize=11, fontweight='bold', color='blue')\n\n# Formula explanation\nax2 = axes[1]\nax2.axis('off')\nax2.text(0.5, 0.9, 'Softmax Attention Calculation', fontsize=14, fontweight='bold', \n         ha='center', transform=ax2.transAxes)\n\nformula_text = '''\nFor query word \"it\", attention weights are:\n\n           exp(score_i)\nW_it = ─────────────────────\n        Σ exp(score_j)\n\nResult for \"it\":\n\n┌────────┬────────┬────────┬────────┬────────┬────────┐\n│  The   │ animal │ didn't │ cross  │  the   │ street │\n├────────┼────────┼────────┼────────┼────────┼────────┤\n│  0.00  │  0.63  │  0.00  │  0.02  │  0.00  │  0.31  │\n└────────┴────────┴────────┴────────┴────────┴────────┘\n              ↑                                   ↑\n         HIGH weight                         Some weight\n      (correct referent)                  (plausible but wrong)\n\nThe model learned that \"it\" most likely refers to \"animal\"\nbecause they share semantic features (animate, can be tired).\n'''\nax2.text(0.1, 0.7, formula_text, fontsize=10, family='monospace',\n         transform=ax2.transAxes, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey Insight:\")\nprint(\"  - Attention weights sum to 1 (softmax)\")\nprint(\"  - High weight (0.63) on 'animal' = strong reference\")\nprint(\"  - Lower weight (0.31) on 'street' = possible but less likely\")\nprint(\"  - This is how transformers understand pronoun references!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part5'></a>\n",
    "# Part 5: Transformer Architecture\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 The Transformer (\"Attention Is All You Need\")\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                         TRANSFORMER BLOCK                                │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│    Input                                                                │\n",
    "│      │                                                                  │\n",
    "│      ▼                                                                  │\n",
    "│  ┌────────────────────┐                                                 │\n",
    "│  │ Multi-Head         │  ← Self-attention with multiple \"perspectives\"│\n",
    "│  │ Self-Attention     │                                                 │\n",
    "│  └─────────┬──────────┘                                                 │\n",
    "│            │                                                            │\n",
    "│      ┌─────┴─────┐                                                      │\n",
    "│      │  Add & Norm│  ← Residual connection + Layer Normalization       │\n",
    "│      └─────┬─────┘                                                      │\n",
    "│            │                                                            │\n",
    "│  ┌─────────▼─────────┐                                                  │\n",
    "│  │ Feed-Forward      │  ← Position-wise fully connected                │\n",
    "│  │ Network           │    FFN(x) = ReLU(xW₁ + b₁)W₂ + b₂              │\n",
    "│  └─────────┬─────────┘                                                  │\n",
    "│            │                                                            │\n",
    "│      ┌─────┴─────┐                                                      │\n",
    "│      │  Add & Norm│                                                     │\n",
    "│      └─────┬─────┘                                                      │\n",
    "│            │                                                            │\n",
    "│      Output                                                             │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## 5.2 Key Components\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| **Multi-Head Attention** | Multiple attention \"heads\" capture different relationships |\n",
    "| **Position Encoding** | Add position information (attention has no inherent order) |\n",
    "| **Feed-Forward Network** | Add non-linearity, process each position |\n",
    "| **Layer Normalization** | Stabilize training |\n",
    "| **Residual Connections** | Help gradient flow, enable deep networks |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRANSFORMER COMPONENTS FROM SCRATCH\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"TRANSFORMER COMPONENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding using sine and cosine functions.\n",
    "    \n",
    "    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \n",
    "    This gives each position a unique encoding and allows\n",
    "    the model to learn relative positions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Compute the div term\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sin to even indices, cos to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension and register as buffer\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "print(\"1. PositionalEncoding created\")\n",
    "print(\"   - Adds position information to embeddings\")\n",
    "print(\"   - Uses sin/cos functions for smooth position representation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention.\n",
    "    \n",
    "    Instead of one attention, use multiple \"heads\" that each\n",
    "    learn different aspects of relationships:\n",
    "    - Head 1: Might learn syntactic relationships\n",
    "    - Head 2: Might learn semantic relationships\n",
    "    - Head 3: Might learn positional relationships\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split the last dimension into (num_heads, d_k).\"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)  # (batch, num_heads, seq_len, d_k)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Combine heads back.\"\"\"\n",
    "        batch_size, _, seq_len, _ = x.size()\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        return x.view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \"\"\"\n",
    "        # Linear projections\n",
    "        Q = self.split_heads(self.W_q(x))\n",
    "        K = self.split_heads(self.W_k(x))\n",
    "        V = self.split_heads(self.W_v(x))\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Combine heads and project\n",
    "        output = self.W_o(self.combine_heads(context))\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "print(\"\\n2. MultiHeadAttention created\")\n",
    "print(\"   - Multiple attention heads in parallel\")\n",
    "print(\"   - Each head learns different relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network.\n",
    "    \n",
    "    FFN(x) = max(0, xW₁ + b₁)W₂ + b₂\n",
    "    \n",
    "    - Applied independently to each position\n",
    "    - Adds non-linearity and increases model capacity\n",
    "    - Inner dimension typically 4x the model dimension\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "print(\"\\n3. FeedForward Network created\")\n",
    "print(\"   - Two linear layers with ReLU activation\")\n",
    "print(\"   - Expands then contracts dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer Block.\n",
    "    \n",
    "    Architecture:\n",
    "    x → MultiHead Attention → Add & Norm → FFN → Add & Norm → output\n",
    "         └──────────────────────┘       └──────────────┘\n",
    "              residual                    residual\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-head attention with residual\n",
    "        attn_output, attn_weights = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "print(\"\\n4. TransformerBlock created\")\n",
    "print(\"   - Combines attention + FFN with residuals\")\n",
    "print(\"   - Layer normalization for stable training\")\n",
    "\n",
    "# Test the transformer block\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING TRANSFORMER BLOCK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "block = TransformerBlock(d_model, num_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Create causal mask\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "output, attn = block(x, mask)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention shape: {attn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part6'></a>\n",
    "# Part 6: Building Mini-GPT\n",
    "\n",
    "---\n",
    "\n",
    "## 6.1 GPT Architecture\n",
    "\n",
    "GPT (Generative Pre-trained Transformer) is a **decoder-only** transformer:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                      MINI-GPT ARCHITECTURE                   │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│    Input Tokens: [The, cat, sat, on, the]                  │\n",
    "│           │                                                 │\n",
    "│           ▼                                                 │\n",
    "│    ┌─────────────────┐                                      │\n",
    "│    │ Token Embedding │  → Each token → d_model vector      │\n",
    "│    └────────┬────────┘                                      │\n",
    "│             │                                               │\n",
    "│             ▼                                               │\n",
    "│    ┌─────────────────┐                                      │\n",
    "│    │ Position Embed  │  → Add position information         │\n",
    "│    └────────┬────────┘                                      │\n",
    "│             │                                               │\n",
    "│             ▼                                               │\n",
    "│    ┌─────────────────┐                                      │\n",
    "│    │ Transformer     │                                      │\n",
    "│    │ Block × N       │  → N stacked transformer blocks     │\n",
    "│    └────────┬────────┘                                      │\n",
    "│             │                                               │\n",
    "│             ▼                                               │\n",
    "│    ┌─────────────────┐                                      │\n",
    "│    │ Output Linear   │  → Project to vocabulary size       │\n",
    "│    └────────┬────────┘                                      │\n",
    "│             │                                               │\n",
    "│             ▼                                               │\n",
    "│    Output Logits: [vocab_size] for each position           │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MINI-GPT LANGUAGE MODEL\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"BUILDING MINI-GPT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Mini-GPT: A simplified GPT-style language model.\n",
    "    \n",
    "    Components:\n",
    "    1. Token embeddings: Map tokens to vectors\n",
    "    2. Position embeddings: Add position information\n",
    "    3. Transformer blocks: Self-attention + FFN\n",
    "    4. Output layer: Map to vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=128, num_heads=4, \n",
    "                 num_layers=4, max_seq_len=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights (important for training stability).\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input token indices (batch, seq_len)\n",
    "            targets: Target token indices for loss calculation\n",
    "        \n",
    "        Returns:\n",
    "            logits: Output logits (batch, seq_len, vocab_size)\n",
    "            loss: Cross-entropy loss if targets provided\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Token embeddings\n",
    "        x = self.token_embedding(x)  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Create causal mask (can't see future tokens)\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device))\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, seq_len)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x, _ = block(x, mask)\n",
    "        \n",
    "        # Final layer norm\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = self.output(x)  # (batch, seq_len, vocab_size)\n",
    "        \n",
    "        # Calculate loss if targets provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1)\n",
    "            )\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate new tokens autoregressively.\n",
    "        \n",
    "        Args:\n",
    "            idx: Starting token indices (batch, seq_len)\n",
    "            max_new_tokens: Number of tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "            top_k: If set, only sample from top-k tokens\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop to max_seq_len\n",
    "            idx_cond = idx if idx.size(1) <= self.max_seq_len else idx[:, -self.max_seq_len:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature  # Last position only\n",
    "            \n",
    "            # Top-k sampling\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            \n",
    "            # Sample\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "print(\"MiniGPT class created!\")\n",
    "print(\"\\nArchitecture:\")\n",
    "print(\"  - Token Embedding → Position Encoding\")\n",
    "print(\"  - N × Transformer Blocks (Multi-Head Attention + FFN)\")\n",
    "print(\"  - Layer Norm → Output Projection\")\n",
    "print(\"\\nKey Features:\")\n",
    "print(\"  - Causal masking (can't see future)\")\n",
    "print(\"  - Autoregressive generation\")\n",
    "print(\"  - Temperature + top-k sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Model hyperparameters\n",
    "config = {\n",
    "    'vocab_size': len(vocab) + 2,  # +2 for <pad> and <eos>\n",
    "    'd_model': 64,\n",
    "    'num_heads': 4,\n",
    "    'num_layers': 3,\n",
    "    'max_seq_len': 32,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "model = MiniGPT(**config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Compare to GPT models\n",
    "print(\"\\nFor reference (GPT model sizes):\")\n",
    "print(\"  GPT-1: 117M parameters\")\n",
    "print(\"  GPT-2: 1.5B parameters\")\n",
    "print(\"  GPT-3: 175B parameters\")\n",
    "print(\"  GPT-4: ~1.7T parameters (estimated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part7'></a>\n",
    "# Part 7: Training & Text Generation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PREPARE TRAINING DATA\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARING TRAINING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Add special tokens to vocabulary\n",
    "special_tokens = ['<pad>', '<eos>']\n",
    "full_vocab = special_tokens + vocab\n",
    "word_to_idx = {w: i for i, w in enumerate(full_vocab)}\n",
    "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "\n",
    "PAD_IDX = word_to_idx['<pad>']\n",
    "EOS_IDX = word_to_idx['<eos>']\n",
    "\n",
    "print(f\"Vocabulary size: {len(full_vocab)}\")\n",
    "print(f\"PAD token index: {PAD_IDX}\")\n",
    "print(f\"EOS token index: {EOS_IDX}\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for language model training.\"\"\"\n",
    "    \n",
    "    def __init__(self, sentences, word_to_idx, seq_len=32):\n",
    "        self.data = []\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Convert to indices\n",
    "            indices = [word_to_idx.get(w, PAD_IDX) for w in sentence]\n",
    "            indices.append(EOS_IDX)\n",
    "            \n",
    "            # Pad or truncate\n",
    "            if len(indices) < seq_len + 1:\n",
    "                indices = indices + [PAD_IDX] * (seq_len + 1 - len(indices))\n",
    "            else:\n",
    "                indices = indices[:seq_len + 1]\n",
    "            \n",
    "            self.data.append(indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx]\n",
    "        x = torch.tensor(seq[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(seq[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TextDataset(sentences, word_to_idx, seq_len=config['max_seq_len'])\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "print(f\"Batches: {len(dataloader)}\")\n",
    "\n",
    "# Show sample\n",
    "x, y = dataset[0]\n",
    "print(f\"\\nSample input shape: {x.shape}\")\n",
    "print(f\"Sample target shape: {y.shape}\")\n",
    "print(f\"\\nSample input (first 10): {[idx_to_word[i.item()] for i in x[:10]]}\")\n",
    "print(f\"Sample target (first 10): {[idx_to_word[i.item()] for i in y[:10]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN THE MODEL\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING MINI-GPT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Reinitialize model with correct vocab size\n",
    "config['vocab_size'] = len(full_vocab)\n",
    "model = MiniGPT(**config).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, loss = model(batch_x, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, color='steelblue', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEXT GENERATION WITH MINI-GPT\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"TEXT GENERATION WITH MINI-GPT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def generate_text(model, start_words, max_tokens=15, temperature=1.0, top_k=10):\n",
    "    \"\"\"Generate text from starting words.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert start words to indices\n",
    "    start_indices = [word_to_idx.get(w, PAD_IDX) for w in start_words]\n",
    "    idx = torch.tensor([start_indices], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Generate\n",
    "    output = model.generate(idx, max_tokens, temperature=temperature, top_k=top_k)\n",
    "    \n",
    "    # Convert back to words\n",
    "    words = [idx_to_word[i.item()] for i in output[0]]\n",
    "    \n",
    "    # Stop at EOS or PAD\n",
    "    result = []\n",
    "    for w in words:\n",
    "        if w in ['<eos>', '<pad>']:\n",
    "            break\n",
    "        result.append(w)\n",
    "    \n",
    "    return ' '.join(result)\n",
    "\n",
    "# Generate with different prompts\n",
    "prompts = [\n",
    "    ['the'],\n",
    "    ['the', 'cat'],\n",
    "    ['a'],\n",
    "    ['is'],\n",
    "]\n",
    "\n",
    "print(\"\\nGenerated text (temperature=0.8):\")\n",
    "print(\"-\" * 50)\n",
    "for prompt in prompts:\n",
    "    for i in range(2):\n",
    "        text = generate_text(model, prompt, max_tokens=12, temperature=0.8)\n",
    "        print(f\"  '{' '.join(prompt)}' → {text}\")\n",
    "    print()\n",
    "\n",
    "# Show effect of temperature\n",
    "print(\"\\nEffect of temperature:\")\n",
    "print(\"-\" * 50)\n",
    "for temp in [0.3, 0.7, 1.0, 1.5]:\n",
    "    text = generate_text(model, ['the'], max_tokens=10, temperature=temp)\n",
    "    print(f\"  Temperature {temp}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZE ATTENTION PATTERNS\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"ATTENTION PATTERN VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def get_attention_weights(model, text):\n",
    "    \"\"\"Get attention weights for a text sequence.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    words = text.split()\n",
    "    indices = [word_to_idx.get(w, PAD_IDX) for w in words]\n",
    "    x = torch.tensor([indices], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Get attention from first block\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        emb = model.token_embedding(x)\n",
    "        emb = model.pos_encoding(emb)\n",
    "        \n",
    "        # Get attention from first block\n",
    "        mask = torch.tril(torch.ones(len(words), len(words), device=device))\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "        _, attn = model.blocks[0](emb, mask)\n",
    "    \n",
    "    return attn[0].cpu().numpy(), words\n",
    "\n",
    "# Visualize attention for a sentence\n",
    "test_text = \"the cat sat on the mat\"\n",
    "attn_weights, words = get_attention_weights(model, test_text)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for head in range(min(4, attn_weights.shape[0])):\n",
    "    ax = axes[head]\n",
    "    sns.heatmap(attn_weights[head], xticklabels=words, yticklabels=words,\n",
    "                ax=ax, cmap='Blues', annot=True, fmt='.2f', cbar=False)\n",
    "    ax.set_title(f'Head {head+1}', fontweight='bold')\n",
    "    ax.set_xlabel('Key')\n",
    "    if head == 0:\n",
    "        ax.set_ylabel('Query')\n",
    "\n",
    "plt.suptitle(f'Attention Patterns for \"{test_text}\"', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEach head learns different attention patterns!\")\n",
    "print(\"  - Some heads might focus on nearby words (local)\")\n",
    "print(\"  - Some heads might focus on specific word types (global)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part8'></a>\n",
    "# Part 8: Summary & Comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL COMPARISON\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"LANGUAGE MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Unigram', 'Bigram', 'Trigram', 'Mini-GPT'],\n",
    "    'Parameters': ['~100', '~10K', '~100K', f'{total_params:,}'],\n",
    "    'Context': ['None', '1 word', '2 words', f'{config[\"max_seq_len\"]} words'],\n",
    "    'Strengths': [\n",
    "        'Simple, fast',\n",
    "        'Local patterns',\n",
    "        'Better local context',\n",
    "        'Long-range dependencies'\n",
    "    ],\n",
    "    'Weaknesses': [\n",
    "        'No context',\n",
    "        'Very limited context',\n",
    "        'Limited context, sparse',\n",
    "        'Needs more data'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"LANGUAGE MODEL FROM SCRATCH - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "WHAT WE LEARNED:\n",
    "================\n",
    "\n",
    "1. N-GRAM MODELS:\n",
    "   ┌─────────────────────────────────────────────────┐\n",
    "   │ P(word | context) = Count(context, word)        │\n",
    "   │                     ─────────────────────       │\n",
    "   │                     Count(context)              │\n",
    "   └─────────────────────────────────────────────────┘\n",
    "   - Simple counting approach\n",
    "   - Fixed context window\n",
    "   - Sparsity problems\n",
    "\n",
    "2. WORD EMBEDDINGS:\n",
    "   - Dense vector representations\n",
    "   - Capture semantic similarity\n",
    "   - king - man + woman ≈ queen\n",
    "\n",
    "3. ATTENTION MECHANISM:\n",
    "   ┌─────────────────────────────────────────────────┐\n",
    "   │ Attention(Q, K, V) = softmax(QK^T / √d_k) × V   │\n",
    "   └─────────────────────────────────────────────────┘\n",
    "   - Dynamic, content-based weighting\n",
    "   - Every position can attend to every other\n",
    "   - Enables long-range dependencies\n",
    "\n",
    "4. TRANSFORMER:\n",
    "   ┌──────────────────────────────────┐\n",
    "   │ Input → Embedding + Position     │\n",
    "   │       ↓                          │\n",
    "   │ Multi-Head Attention + FFN (×N)  │\n",
    "   │       ↓                          │\n",
    "   │ Output Projection                │\n",
    "   └──────────────────────────────────┘\n",
    "   - Parallel processing (no recurrence)\n",
    "   - Multi-head attention captures different patterns\n",
    "   - Foundation of GPT, BERT, etc.\n",
    "\n",
    "KEY EQUATIONS:\n",
    "==============\n",
    "\n",
    "Bigram: P(w₂|w₁) = C(w₁,w₂) / C(w₁)\n",
    "\n",
    "Attention: softmax(QK^T / √d_k) × V\n",
    "\n",
    "Perplexity: PP = 2^(-1/N × Σlog₂P(wᵢ|context))\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nMODERN LLMs BUILT ON THESE FOUNDATIONS:\")\n",
    "print(\"  - GPT-4: Transformer decoder, 1.7T parameters\")\n",
    "print(\"  - Claude: Constitutional AI, transformer-based\")\n",
    "print(\"  - LLaMA: Open-source transformer\")\n",
    "print(\"  - BERT: Transformer encoder, bidirectional\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm & Method Taxonomy\n",
    "\n",
    "### Language Model Types\n",
    "\n",
    "| Type | Method | Context | Training |\n",
    "|------|--------|---------|----------|\n",
    "| **Statistical** | N-gram | Fixed N-1 words | Count-based |\n",
    "| **Neural (RNN)** | LSTM/GRU | Sequential, variable | Backprop through time |\n",
    "| **Neural (Transformer)** | Self-attention | Full sequence | Standard backprop |\n",
    "\n",
    "### Attention Variants\n",
    "\n",
    "| Variant | Description | Use Case |\n",
    "|---------|-------------|----------|\n",
    "| **Self-attention** | Q=K=V from same sequence | GPT, BERT |\n",
    "| **Cross-attention** | Q from decoder, K,V from encoder | Translation |\n",
    "| **Causal attention** | Mask future tokens | Language modeling |\n",
    "| **Multi-head** | Multiple parallel attention heads | Better representations |\n",
    "\n",
    "### Transformer Architectures\n",
    "\n",
    "| Architecture | Attention Type | Use Case |\n",
    "|--------------|----------------|----------|\n",
    "| **Encoder-only** | Bidirectional | BERT (classification) |\n",
    "| **Decoder-only** | Causal | GPT (generation) |\n",
    "| **Encoder-Decoder** | Cross-attention | T5 (translation) |\n",
    "\n",
    "### Sampling Strategies\n",
    "\n",
    "| Strategy | Description | Effect |\n",
    "|----------|-------------|--------|\n",
    "| **Greedy** | Always pick highest prob | Deterministic, repetitive |\n",
    "| **Temperature** | Scale logits before softmax | Low=focused, High=diverse |\n",
    "| **Top-k** | Sample from top k tokens | Balanced diversity |\n",
    "| **Top-p (nucleus)** | Sample from cumulative p% | Adaptive diversity |\n",
    "\n",
    "---\n",
    "\n",
    "## Checklist\n",
    "\n",
    "- [x] Understand N-gram probability calculation\n",
    "- [x] Know why embeddings are better than one-hot\n",
    "- [x] Can explain attention mechanism (Q, K, V)\n",
    "- [x] Understand transformer architecture\n",
    "- [x] Know difference between encoder/decoder transformers\n",
    "- [x] Can implement basic transformer from scratch\n",
    "- [x] Understand perplexity as evaluation metric\n",
    "\n",
    "---\n",
    "\n",
    "**End of Language Model From Scratch Tutorial**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}