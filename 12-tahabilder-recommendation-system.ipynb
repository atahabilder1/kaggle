{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System - Complete Tutorial\n",
    "\n",
    "**Author:** Anik Tahabilder  \n",
    "**Project:** 12 of 22 - Kaggle ML Portfolio  \n",
    "**Dataset:** MovieLens  \n",
    "**Difficulty:** 6/10 | **Learning Value:** 8/10\n",
    "\n",
    "---\n",
    "\n",
    "## What Will You Learn?\n",
    "\n",
    "This tutorial teaches **Recommendation Systems from fundamentals to implementation**.\n",
    "\n",
    "| Topic | What You'll Understand |\n",
    "|-------|------------------------|\n",
    "| **Types of Recommenders** | Content-based vs Collaborative Filtering |\n",
    "| **Similarity Metrics** | Cosine, Pearson, Jaccard - when to use each |\n",
    "| **User-based CF** | \"Users like you also liked...\" |\n",
    "| **Item-based CF** | \"Because you liked X, try Y...\" |\n",
    "| **Matrix Factorization** | SVD for latent features |\n",
    "| **Cold Start Problem** | Handling new users/items |\n",
    "| **Evaluation Metrics** | RMSE, MAE, Precision@K, Recall@K |\n",
    "\n",
    "---\n",
    "\n",
    "## The Recommendation Problem\n",
    "\n",
    "```\n",
    "USER-ITEM INTERACTION MATRIX\n",
    "\n",
    "              Movie1  Movie2  Movie3  Movie4  Movie5\n",
    "           ┌────────────────────────────────────────┐\n",
    "   User1   │   5       3       ?       1       ?   │\n",
    "   User2   │   4       ?       ?       1       ?   │\n",
    "   User3   │   1       1       ?       5       4   │\n",
    "   User4   │   ?       ?       5       4       ?   │\n",
    "           └────────────────────────────────────────┘\n",
    "                        ↓\n",
    "           GOAL: Predict the \"?\" values!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: Types of Recommendation Systems](#part1)\n",
    "2. [Part 2: Similarity Metrics](#part2)\n",
    "3. [Part 3: Collaborative Filtering Theory](#part3)\n",
    "4. [Part 4: Matrix Factorization (SVD)](#part4)\n",
    "5. [Part 5: Dataset Loading & EDA](#part5)\n",
    "6. [Part 6: User-Based Collaborative Filtering](#part6)\n",
    "7. [Part 7: Item-Based Collaborative Filtering](#part7)\n",
    "8. [Part 8: SVD-Based Recommendations](#part8)\n",
    "9. [Part 9: Evaluation & Comparison](#part9)\n",
    "10. [Part 10: Summary & Key Takeaways](#part10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n<a id='part1'></a>\n# Part 1: Types of Recommendation Systems\n\n---\n\n## 1.1 The Three Main Approaches\n\n| Approach | How It Works | Example | When to Use |\n|----------|--------------|--------|-------------|\n| **Content-Based** | Recommend items similar to what user liked | \"You liked Action movies, here's more Action\" | Have good item features, new items |\n| **Collaborative Filtering** | Recommend based on similar users/items | \"Users like you also watched...\" | Have user-item interactions |\n| **Hybrid** | Combine both approaches | Netflix, Spotify, Amazon | Production systems |\n\n---\n\n## 1.2 Content-Based Filtering\n\n```\nUSER PROFILE                    ITEM FEATURES                RECOMMENDATION\n┌─────────────┐                ┌─────────────┐               ┌─────────────┐\n│ Likes:      │                │ Movie X:    │               │             │\n│ - Action    │   MATCH        │ - Action    │   HIGH        │ Recommend   │\n│ - Sci-Fi    │ ──────────>    │ - Sci-Fi    │ ──────────>   │  Movie X!   │\n│ - 2hr+      │                │ - 2hr 15min │   SCORE       │             │\n└─────────────┘                └─────────────┘               └─────────────┘\n```\n\n### When to Choose Content-Based?\n\n| Scenario | Why Content-Based Works |\n|----------|------------------------|\n| **New items (no ratings yet)** | Can recommend based on features alone |\n| **Rich item metadata** | News articles, job postings have good descriptions |\n| **User privacy important** | Doesn't need other users' data |\n| **Transparent recommendations** | Can explain \"because it has genre X\" |\n\n| Pros | Cons |\n|------|------|\n| No cold start for items | Limited diversity (filter bubble) |\n| Transparent recommendations | Needs good item features |\n| User independence | Can't discover unexpected preferences |\n\n---\n\n## 1.3 Collaborative Filtering\n\n```\nUSER-BASED CF                              ITEM-BASED CF\n\"Users similar to you liked...\"            \"Items similar to what you liked...\"\n\n┌─────┐                                    ┌─────┐\n│You  │──likes──> Movie A                  │You  │──likes──> Movie A\n└─────┘                                    └─────┘              │\n   │ similar                                                    │ similar\n   v                                                            v\n┌─────┐                                                    ┌─────────┐\n│Bob  │──likes──> Movie A, Movie B                         │ Movie B │\n└─────┘              │                                     └─────────┘\n                     v                                          │\n            Recommend Movie B!                      Recommend Movie B!\n```\n\n### When to Choose Collaborative Filtering?\n\n| Scenario | Why CF Works |\n|----------|-------------|\n| **Enough user-item interactions** | Can find meaningful patterns |\n| **Items lack good features** | Music, movies hard to describe |\n| **Want to discover hidden patterns** | Users might like unexpected things |\n| **E-commerce, streaming** | Behavior matters more than content |\n\n---\n\n## 1.4 User-Based vs Item-Based: Decision Guide\n\n| Criteria | User-Based | Item-Based | Better Choice |\n|----------|------------|------------|---------------|\n| **# Users >> # Items** | Slow (O(m²)) | Fast | Item-Based |\n| **# Items >> # Users** | Fast | Slow (O(n²)) | User-Based |\n| **User behavior changes often** | Recalculate often | Stable | Item-Based |\n| **New items frequently** | Handles better | Cold start | User-Based |\n| **Explainability** | \"Users like you...\" | \"Because you liked X...\" | Both good |\n| **Industry standard** | Academic | Amazon, Netflix | Item-Based |\n\n**Key insight**: Item-Based CF is preferred in production because:\n1. Items are more stable than users\n2. Precomputed similarities can be cached\n3. Better explainability for users\n\n---\n\n## 1.5 Challenges and Solutions\n\n| Challenge | Description | Solution |\n|-----------|-------------|----------|\n| **Cold Start** | New user/item with no history | Content-based fallback, ask preferences, popularity-based |\n| **Sparsity** | 95%+ of matrix is empty | Matrix factorization (SVD) captures patterns |\n| **Scalability** | Millions of users/items | Approximate nearest neighbors, clustering |\n| **Diversity** | Avoid \"filter bubbles\" | Exploration-exploitation, diversity re-ranking |\n| **Implicit feedback** | No explicit ratings | Use clicks, views, time spent |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP AND IMPORTS\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy import sparse\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RECOMMENDATION SYSTEM - TUTORIAL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(\"\\nAll libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n<a id='part2'></a>\n# Part 2: Similarity Metrics\n\n---\n\n## 2.1 Why Similarity Matters\n\nCollaborative filtering relies on finding **similar users** or **similar items**.\n\nThe choice of similarity metric significantly affects recommendations!\n\n---\n\n## 2.2 Common Similarity Metrics\n\n### Cosine Similarity\n\n$$\\text{cosine}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} = \\frac{\\sum a_i b_i}{\\sqrt{\\sum a_i^2} \\sqrt{\\sum b_i^2}}$$\n\n| Property | Description |\n|----------|-------------|\n| **Range** | -1 to 1 (0 to 1 for positive values) |\n| **Measures** | Angle between vectors |\n| **Best for** | When magnitude doesn't matter |\n| **Ignores** | Missing values (use only co-rated items) |\n\n### Pearson Correlation\n\n$$\\text{pearson}(A, B) = \\frac{\\sum (a_i - \\bar{a})(b_i - \\bar{b})}{\\sqrt{\\sum (a_i - \\bar{a})^2} \\sqrt{\\sum (b_i - \\bar{b})^2}}$$\n\n| Property | Description |\n|----------|-------------|\n| **Range** | -1 to 1 |\n| **Measures** | Linear correlation |\n| **Best for** | When users have different rating scales |\n| **Handles** | Mean-centering automatically |\n\n### Jaccard Similarity\n\n$$\\text{jaccard}(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n\n| Property | Description |\n|----------|-------------|\n| **Range** | 0 to 1 |\n| **Measures** | Set overlap |\n| **Best for** | Binary data (liked/not liked) |\n| **Ignores** | Rating values |\n\n---\n\n## 2.3 Which Metric to Choose?\n\n| Scenario | Best Metric | Why |\n|----------|-------------|-----|\n| **Explicit ratings (1-5 stars)** | Pearson | Handles different rating scales |\n| **Implicit feedback (clicks, views)** | Cosine | Magnitude = engagement strength |\n| **Binary data (liked/not liked)** | Jaccard | Designed for sets |\n| **Sparse data** | Cosine or Pearson | Both handle missing values |\n| **User-User similarity** | Pearson | Users have different biases |\n| **Item-Item similarity** | Cosine | Items don't have \"bias\" |\n\n### Why Pearson for User-Based CF?\n\nUsers have different rating behaviors:\n- User A: Rates everything 4-5 stars (generous)\n- User B: Rates everything 2-3 stars (harsh)\n\nBoth might have the **same preferences** but different scales. Pearson correlation **normalizes** by subtracting the mean, capturing the relative pattern, not absolute values.\n\n---\n\n## 2.4 Comparison Table\n\n| Metric | Range | Handles Bias | Uses Values | Sparse Data | Time Complexity |\n|--------|-------|--------------|-------------|-------------|-----------------|\n| **Cosine** | 0 to 1 | No | Yes | Good | O(d) |\n| **Pearson** | -1 to 1 | Yes | Yes | Good | O(d) |\n| **Jaccard** | 0 to 1 | N/A | No | Excellent | O(d) |\n| **Euclidean** | 0 to ∞ | No | Yes | Poor | O(d) |\n\n*d = number of co-rated items*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SIMILARITY METRICS IMPLEMENTATION\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"SIMILARITY METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    # Only use co-rated items (both non-zero)\n",
    "    mask = (a != 0) & (b != 0)\n",
    "    if mask.sum() == 0:\n",
    "        return 0\n",
    "    a_masked, b_masked = a[mask], b[mask]\n",
    "    dot = np.dot(a_masked, b_masked)\n",
    "    norm = np.linalg.norm(a_masked) * np.linalg.norm(b_masked)\n",
    "    return dot / norm if norm > 0 else 0\n",
    "\n",
    "def pearson_sim(a, b):\n",
    "    \"\"\"Compute Pearson correlation between two vectors.\"\"\"\n",
    "    # Only use co-rated items\n",
    "    mask = (a != 0) & (b != 0)\n",
    "    if mask.sum() < 2:  # Need at least 2 co-rated items\n",
    "        return 0\n",
    "    a_masked, b_masked = a[mask], b[mask]\n",
    "    # Mean-center\n",
    "    a_centered = a_masked - np.mean(a_masked)\n",
    "    b_centered = b_masked - np.mean(b_masked)\n",
    "    # Correlation\n",
    "    num = np.dot(a_centered, b_centered)\n",
    "    denom = np.linalg.norm(a_centered) * np.linalg.norm(b_centered)\n",
    "    return num / denom if denom > 0 else 0\n",
    "\n",
    "def jaccard_sim(a, b):\n",
    "    \"\"\"Compute Jaccard similarity (for binary/implicit feedback).\"\"\"\n",
    "    a_set = set(np.where(a > 0)[0])\n",
    "    b_set = set(np.where(b > 0)[0])\n",
    "    intersection = len(a_set & b_set)\n",
    "    union = len(a_set | b_set)\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "# Example\n",
    "print(\"\\nExample: Comparing Two Users' Ratings\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# User ratings for 5 movies (0 = not rated)\n",
    "user_a = np.array([5, 4, 0, 1, 2])  # Likes movies 1,2 | Dislikes 4,5\n",
    "user_b = np.array([4, 5, 0, 2, 1])  # Similar to A\n",
    "user_c = np.array([1, 2, 0, 5, 4])  # Opposite taste to A\n",
    "\n",
    "print(f\"User A ratings: {user_a}\")\n",
    "print(f\"User B ratings: {user_b}\")\n",
    "print(f\"User C ratings: {user_c}\")\n",
    "\n",
    "print(f\"\\nSimilarities (A vs B - Similar tastes):\")\n",
    "print(f\"  Cosine:  {cosine_sim(user_a, user_b):.4f}\")\n",
    "print(f\"  Pearson: {pearson_sim(user_a, user_b):.4f}\")\n",
    "print(f\"  Jaccard: {jaccard_sim(user_a, user_b):.4f}\")\n",
    "\n",
    "print(f\"\\nSimilarities (A vs C - Opposite tastes):\")\n",
    "print(f\"  Cosine:  {cosine_sim(user_a, user_c):.4f}\")\n",
    "print(f\"  Pearson: {pearson_sim(user_a, user_c):.4f}\")\n",
    "print(f\"  Jaccard: {jaccard_sim(user_a, user_c):.4f}\")\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"  - Pearson captures the negative correlation for opposite tastes\")\n",
    "print(\"  - Cosine is always positive (doesn't catch opposite tastes well)\")\n",
    "print(\"  - Jaccard only looks at what they rated, not how\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part3'></a>\n",
    "# Part 3: Collaborative Filtering Theory\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 User-Based Collaborative Filtering\n",
    "\n",
    "### Algorithm:\n",
    "\n",
    "1. **Build user-item matrix** (rows=users, cols=items)\n",
    "2. **Find similar users** to target user\n",
    "3. **Predict rating** as weighted average of similar users' ratings\n",
    "\n",
    "### Prediction Formula:\n",
    "\n",
    "$$\\hat{r}_{u,i} = \\bar{r}_u + \\frac{\\sum_{v \\in N(u)} \\text{sim}(u,v) \\cdot (r_{v,i} - \\bar{r}_v)}{\\sum_{v \\in N(u)} |\\text{sim}(u,v)|}$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{r}_{u,i}$ = predicted rating of user u for item i\n",
    "- $\\bar{r}_u$ = average rating of user u\n",
    "- $N(u)$ = neighbors (similar users) of u who rated item i\n",
    "- $\\text{sim}(u,v)$ = similarity between users u and v\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 Item-Based Collaborative Filtering\n",
    "\n",
    "### Algorithm:\n",
    "\n",
    "1. **Build item-item similarity matrix**\n",
    "2. **For prediction**: Find items similar to those user liked\n",
    "3. **Predict rating** as weighted average based on item similarities\n",
    "\n",
    "### Prediction Formula:\n",
    "\n",
    "$$\\hat{r}_{u,i} = \\frac{\\sum_{j \\in N(i)} \\text{sim}(i,j) \\cdot r_{u,j}}{\\sum_{j \\in N(i)} |\\text{sim}(i,j)|}$$\n",
    "\n",
    "Where:\n",
    "- $N(i)$ = items similar to item i that user u has rated\n",
    "- $\\text{sim}(i,j)$ = similarity between items i and j\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 User-Based vs Item-Based Comparison\n",
    "\n",
    "| Aspect | User-Based | Item-Based |\n",
    "|--------|------------|------------|\n",
    "| **Precompute** | User similarities | Item similarities |\n",
    "| **Scales better** | Few users | Few items |\n",
    "| **Stability** | Changes often (user behavior) | More stable |\n",
    "| **Explainability** | \"Users like you...\" | \"Because you liked X...\" |\n",
    "| **Cold Start** | New user problem | New item problem |\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Neighborhood Selection (K)\n",
    "\n",
    "| K Value | Effect |\n",
    "|---------|--------|\n",
    "| **Small K (5-10)** | More personalized, may be noisy |\n",
    "| **Large K (50+)** | Smoother, less personalized |\n",
    "| **Optimal** | Usually 20-50, tune via cross-validation |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part4'></a>\n",
    "# Part 4: Matrix Factorization (SVD)\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 The Idea\n",
    "\n",
    "Decompose the user-item matrix into latent factors:\n",
    "\n",
    "```\n",
    "USER-ITEM MATRIX              =    USER FACTORS    ×    ITEM FACTORS\n",
    "     (m × n)                         (m × k)              (k × n)\n",
    "\n",
    "┌─────────────────┐           ┌─────────┐         ┌─────────────────┐\n",
    "│ 5  3  ?  1  ?   │           │ u1 u2   │         │ i1 i2 i3 i4 i5 │\n",
    "│ 4  ?  ?  1  ?   │     =     │ u1 u2   │    ×    │ i1 i2 i3 i4 i5 │\n",
    "│ 1  1  ?  5  4   │           │ u1 u2   │         └─────────────────┘\n",
    "│ ?  ?  5  4  ?   │           │ u1 u2   │              k factors\n",
    "└─────────────────┘           └─────────┘\n",
    "                               k factors\n",
    "```\n",
    "\n",
    "## 4.2 SVD (Singular Value Decomposition)\n",
    "\n",
    "$$R = U \\Sigma V^T$$\n",
    "\n",
    "| Matrix | Shape | Meaning |\n",
    "|--------|-------|--------|\n",
    "| **R** | m × n | Original ratings matrix |\n",
    "| **U** | m × k | User latent factors |\n",
    "| **Σ** | k × k | Singular values (importance) |\n",
    "| **V^T** | k × n | Item latent factors |\n",
    "\n",
    "## 4.3 What Are Latent Factors?\n",
    "\n",
    "Hidden features that explain preferences:\n",
    "\n",
    "| Factor | Could Represent |\n",
    "|--------|----------------|\n",
    "| Factor 1 | Action vs Drama preference |\n",
    "| Factor 2 | Old vs New movies |\n",
    "| Factor 3 | Mainstream vs Indie |\n",
    "| ... | Other hidden patterns |\n",
    "\n",
    "## 4.4 Prediction with SVD\n",
    "\n",
    "$$\\hat{r}_{u,i} = \\mu + b_u + b_i + q_i^T p_u$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ = global average rating\n",
    "- $b_u$ = user bias (some users rate higher)\n",
    "- $b_i$ = item bias (some movies are rated higher)\n",
    "- $q_i$ = item latent vector\n",
    "- $p_u$ = user latent vector\n",
    "\n",
    "## 4.5 Key Parameters\n",
    "\n",
    "| Parameter | Description | Typical Value |\n",
    "|-----------|-------------|---------------|\n",
    "| **k (n_factors)** | Number of latent factors | 20-100 |\n",
    "| **Learning rate** | For SGD optimization | 0.005-0.01 |\n",
    "| **Regularization** | Prevent overfitting | 0.02-0.1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n<a id='part5'></a>\n# Part 5: Dataset Loading & EDA\n\n---\n\n## 5.1 Dataset Information\n\n| Attribute | Value |\n|-----------|-------|\n| **Kaggle Dataset** | `movie-recommendation-system` |\n| **Kaggle Path** | `/kaggle/input/movie-recommendation-system` |\n\n---\n\n## 5.2 Interview Question: Why Use This Dataset?\n\n| Reason | Explanation |\n|--------|-------------|\n| **Real-world data** | Actual user ratings, not synthetic |\n| **Benchmark standard** | MovieLens is industry standard for RecSys |\n| **Good sparsity** | ~95% sparse, realistic challenge |\n| **Multiple evaluation** | Can compare methods fairly |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# LOAD DATASET FROM KAGGLE\n# ============================================================\nprint(\"=\"*70)\nprint(\"LOADING MOVIE RECOMMENDATION DATASET\")\nprint(\"=\"*70)\n\n# ============================================================\n# KAGGLE PATH CONFIGURATION\n# ============================================================\n# Dataset: /kaggle/input/movie-recommendation-system\n\nUSE_KAGGLE = os.path.exists('/kaggle/input')\n\n# Possible paths for the dataset\nPOSSIBLE_PATHS = [\n    # Primary: movie-recommendation-system dataset\n    '/kaggle/input/movie-recommendation-system/ratings.csv',\n    '/kaggle/input/movie-recommendation-system/rating.csv',\n    '/kaggle/input/movie-recommendation-system',\n    # Alternative MovieLens paths\n    '/kaggle/input/movielens-100k-dataset/ml-100k/u.data',\n    '/kaggle/input/movielens-small-latest-dataset/ratings.csv',\n]\n\nratings = None\nmovies = None\n\nif USE_KAGGLE:\n    # First, let's check what files exist in the dataset\n    base_path = '/kaggle/input/movie-recommendation-system'\n    if os.path.exists(base_path):\n        print(f\"Found dataset at: {base_path}\")\n        print(f\"Files in dataset: {os.listdir(base_path)}\")\n        \n        # Try to find ratings file\n        for filename in os.listdir(base_path):\n            filepath = os.path.join(base_path, filename)\n            if 'rating' in filename.lower() and filename.endswith('.csv'):\n                try:\n                    ratings = pd.read_csv(filepath)\n                    print(f\"Loaded ratings from: {filename}\")\n                    break\n                except Exception as e:\n                    print(f\"Error loading {filename}: {e}\")\n        \n        # Try to find movies file for titles\n        for filename in os.listdir(base_path):\n            filepath = os.path.join(base_path, filename)\n            if 'movie' in filename.lower() and filename.endswith('.csv') and 'rating' not in filename.lower():\n                try:\n                    movies = pd.read_csv(filepath)\n                    print(f\"Loaded movies from: {filename}\")\n                    break\n                except Exception as e:\n                    print(f\"Error loading {filename}: {e}\")\n    \n    # If still no ratings, try alternative paths\n    if ratings is None:\n        for path in POSSIBLE_PATHS:\n            if os.path.exists(path):\n                try:\n                    if path.endswith('.csv'):\n                        ratings = pd.read_csv(path)\n                    elif 'u.data' in path:\n                        ratings = pd.read_csv(path, sep='\\t', \n                                            names=['user_id', 'movie_id', 'rating', 'timestamp'])\n                    print(f\"Loaded ratings from: {path}\")\n                    break\n                except Exception as e:\n                    continue\n\n# Standardize column names\nif ratings is not None:\n    ratings.columns = [c.lower().replace(' ', '_') for c in ratings.columns]\n    # Handle various column naming conventions\n    col_mapping = {\n        'userid': 'user_id', 'user': 'user_id',\n        'movieid': 'movie_id', 'movie': 'movie_id', 'item_id': 'movie_id',\n        'rate': 'rating', 'score': 'rating'\n    }\n    ratings.rename(columns=col_mapping, inplace=True)\n\n# Fallback: Create synthetic dataset\nif ratings is None:\n    print(\"\\nNo Kaggle dataset found. Creating synthetic MovieLens-like data...\")\n    print(\"(Add 'movie-recommendation-system' dataset in Kaggle for real data)\")\n    \n    np.random.seed(42)\n    n_users = 500\n    n_movies = 200\n    n_ratings = 50000\n    \n    # Create user preferences (latent factors)\n    user_preferences = np.random.randn(n_users, 5)\n    movie_features = np.random.randn(n_movies, 5)\n    \n    ratings_list = []\n    for _ in range(n_ratings):\n        u = np.random.randint(1, n_users + 1)\n        m = np.random.randint(1, n_movies + 1)\n        base_rating = np.dot(user_preferences[u-1], movie_features[m-1])\n        rating = np.clip(base_rating + 3 + np.random.randn() * 0.5, 1, 5)\n        rating = round(rating * 2) / 2\n        ratings_list.append([u, m, rating, 0])\n    \n    ratings = pd.DataFrame(ratings_list, \n                          columns=['user_id', 'movie_id', 'rating', 'timestamp'])\n    ratings = ratings.drop_duplicates(subset=['user_id', 'movie_id'], keep='last')\n\nprint(f\"\\n\" + \"=\"*50)\nprint(\"DATASET SUMMARY\")\nprint(\"=\"*50)\nprint(f\"Columns: {list(ratings.columns)}\")\nprint(f\"Total ratings: {len(ratings):,}\")\nprint(f\"Unique users: {ratings['user_id'].nunique():,}\")\nprint(f\"Unique movies: {ratings['movie_id'].nunique():,}\")\nprint(f\"Rating range: {ratings['rating'].min()} - {ratings['rating'].max()}\")\nprint(f\"Average rating: {ratings['rating'].mean():.2f}\")\nprint(f\"\\nSparsity: {100 * (1 - len(ratings) / (ratings['user_id'].nunique() * ratings['movie_id'].nunique())):.2f}%\")\n\nprint(\"\\nFirst few ratings:\")\nprint(ratings.head(10))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPLORATORY DATA ANALYSIS\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Rating distribution\n",
    "ax1 = axes[0, 0]\n",
    "ratings['rating'].value_counts().sort_index().plot(kind='bar', ax=ax1, color='steelblue', edgecolor='black')\n",
    "ax1.set_xlabel('Rating')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Rating Distribution', fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 2. Ratings per user\n",
    "ax2 = axes[0, 1]\n",
    "ratings_per_user = ratings.groupby('user_id').size()\n",
    "ax2.hist(ratings_per_user, bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(ratings_per_user.median(), color='red', linestyle='--', label=f'Median: {ratings_per_user.median():.0f}')\n",
    "ax2.set_xlabel('Number of Ratings')\n",
    "ax2.set_ylabel('Number of Users')\n",
    "ax2.set_title('Ratings per User', fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Ratings per movie\n",
    "ax3 = axes[1, 0]\n",
    "ratings_per_movie = ratings.groupby('movie_id').size()\n",
    "ax3.hist(ratings_per_movie, bins=50, color='green', edgecolor='black', alpha=0.7)\n",
    "ax3.axvline(ratings_per_movie.median(), color='red', linestyle='--', label=f'Median: {ratings_per_movie.median():.0f}')\n",
    "ax3.set_xlabel('Number of Ratings')\n",
    "ax3.set_ylabel('Number of Movies')\n",
    "ax3.set_title('Ratings per Movie', fontweight='bold')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Average rating per user\n",
    "ax4 = axes[1, 1]\n",
    "avg_rating_per_user = ratings.groupby('user_id')['rating'].mean()\n",
    "ax4.hist(avg_rating_per_user, bins=30, color='purple', edgecolor='black', alpha=0.7)\n",
    "ax4.axvline(avg_rating_per_user.mean(), color='red', linestyle='--', label=f'Mean: {avg_rating_per_user.mean():.2f}')\n",
    "ax4.set_xlabel('Average Rating')\n",
    "ax4.set_ylabel('Number of Users')\n",
    "ax4.set_title('User Rating Bias', fontweight='bold')\n",
    "ax4.legend()\n",
    "\n",
    "plt.suptitle('MovieLens Dataset Analysis', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(f\"  - Most users rate {int(ratings_per_user.median())}-{int(ratings_per_user.quantile(0.75))} movies\")\n",
    "print(f\"  - Some movies are very popular, most have few ratings (long tail)\")\n",
    "print(f\"  - Users have different rating biases (some rate high, some low)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PREPARE DATA FOR RECOMMENDATIONS\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Filter users and movies with minimum ratings (reduces sparsity)\n",
    "min_user_ratings = 10\n",
    "min_movie_ratings = 10\n",
    "\n",
    "# Filter users\n",
    "user_counts = ratings['user_id'].value_counts()\n",
    "valid_users = user_counts[user_counts >= min_user_ratings].index\n",
    "\n",
    "# Filter movies\n",
    "movie_counts = ratings['movie_id'].value_counts()\n",
    "valid_movies = movie_counts[movie_counts >= min_movie_ratings].index\n",
    "\n",
    "# Apply filters\n",
    "ratings_filtered = ratings[\n",
    "    (ratings['user_id'].isin(valid_users)) & \n",
    "    (ratings['movie_id'].isin(valid_movies))\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nAfter filtering (min {min_user_ratings} ratings):\")\n",
    "print(f\"  Ratings: {len(ratings_filtered):,}\")\n",
    "print(f\"  Users: {ratings_filtered['user_id'].nunique():,}\")\n",
    "print(f\"  Movies: {ratings_filtered['movie_id'].nunique():,}\")\n",
    "\n",
    "# Create train/test split\n",
    "train_data, test_data = train_test_split(ratings_filtered, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTrain/Test split:\")\n",
    "print(f\"  Train: {len(train_data):,} ratings\")\n",
    "print(f\"  Test:  {len(test_data):,} ratings\")\n",
    "\n",
    "# Create user-item matrix\n",
    "print(\"\\nCreating user-item matrix...\")\n",
    "\n",
    "# Get unique users and movies\n",
    "users = sorted(train_data['user_id'].unique())\n",
    "movies = sorted(train_data['movie_id'].unique())\n",
    "\n",
    "# Create mappings\n",
    "user_to_idx = {u: i for i, u in enumerate(users)}\n",
    "idx_to_user = {i: u for u, i in user_to_idx.items()}\n",
    "movie_to_idx = {m: i for i, m in enumerate(movies)}\n",
    "idx_to_movie = {i: m for m, i in movie_to_idx.items()}\n",
    "\n",
    "n_users = len(users)\n",
    "n_movies = len(movies)\n",
    "\n",
    "# Create matrix\n",
    "user_item_matrix = np.zeros((n_users, n_movies))\n",
    "for _, row in train_data.iterrows():\n",
    "    u_idx = user_to_idx.get(row['user_id'])\n",
    "    m_idx = movie_to_idx.get(row['movie_id'])\n",
    "    if u_idx is not None and m_idx is not None:\n",
    "        user_item_matrix[u_idx, m_idx] = row['rating']\n",
    "\n",
    "print(f\"\\nUser-Item Matrix shape: {user_item_matrix.shape}\")\n",
    "print(f\"Sparsity: {100 * (1 - np.count_nonzero(user_item_matrix) / user_item_matrix.size):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part6'></a>\n",
    "# Part 6: User-Based Collaborative Filtering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# USER-BASED COLLABORATIVE FILTERING\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"USER-BASED COLLABORATIVE FILTERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "Algorithm:\n",
    "==========\n",
    "1. Compute similarity between all user pairs\n",
    "2. For prediction: Find K most similar users who rated the item\n",
    "3. Predict = weighted average of neighbors' ratings\n",
    "\n",
    "Key Parameters:\n",
    "  - K (n_neighbors): Number of similar users to consider\n",
    "  - Similarity metric: Pearson (recommended) or Cosine\n",
    "\"\"\")\n",
    "\n",
    "class UserBasedCF:\n",
    "    \"\"\"\n",
    "    User-Based Collaborative Filtering from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_neighbors=20, similarity='pearson'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - n_neighbors: Number of similar users to use\n",
    "        - similarity: 'pearson' or 'cosine'\n",
    "        \"\"\"\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.similarity = similarity\n",
    "        self.user_sim_matrix = None\n",
    "        self.user_means = None\n",
    "        self.matrix = None\n",
    "    \n",
    "    def fit(self, user_item_matrix):\n",
    "        \"\"\"\n",
    "        Compute user-user similarity matrix.\n",
    "        \"\"\"\n",
    "        self.matrix = user_item_matrix.copy()\n",
    "        n_users = self.matrix.shape[0]\n",
    "        \n",
    "        # Compute user means (for non-zero ratings only)\n",
    "        self.user_means = np.zeros(n_users)\n",
    "        for i in range(n_users):\n",
    "            rated = self.matrix[i] > 0\n",
    "            if rated.sum() > 0:\n",
    "                self.user_means[i] = self.matrix[i, rated].mean()\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        print(f\"Computing {n_users}x{n_users} user similarity matrix...\")\n",
    "        self.user_sim_matrix = np.zeros((n_users, n_users))\n",
    "        \n",
    "        for i in range(n_users):\n",
    "            for j in range(i, n_users):\n",
    "                if i == j:\n",
    "                    self.user_sim_matrix[i, j] = 1.0\n",
    "                else:\n",
    "                    if self.similarity == 'pearson':\n",
    "                        sim = pearson_sim(self.matrix[i], self.matrix[j])\n",
    "                    else:\n",
    "                        sim = cosine_sim(self.matrix[i], self.matrix[j])\n",
    "                    self.user_sim_matrix[i, j] = sim\n",
    "                    self.user_sim_matrix[j, i] = sim\n",
    "        \n",
    "        print(\"Done!\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, user_idx, item_idx):\n",
    "        \"\"\"\n",
    "        Predict rating for a user-item pair.\n",
    "        \"\"\"\n",
    "        # Get users who rated this item\n",
    "        item_ratings = self.matrix[:, item_idx]\n",
    "        rated_mask = item_ratings > 0\n",
    "        \n",
    "        if not rated_mask.any():\n",
    "            return self.user_means[user_idx]  # Fallback to user mean\n",
    "        \n",
    "        # Get similarities to users who rated this item\n",
    "        similarities = self.user_sim_matrix[user_idx, rated_mask]\n",
    "        ratings = item_ratings[rated_mask]\n",
    "        means = self.user_means[rated_mask]\n",
    "        \n",
    "        # Get top K neighbors\n",
    "        if len(similarities) > self.n_neighbors:\n",
    "            top_k_idx = np.argsort(similarities)[-self.n_neighbors:]\n",
    "            similarities = similarities[top_k_idx]\n",
    "            ratings = ratings[top_k_idx]\n",
    "            means = means[top_k_idx]\n",
    "        \n",
    "        # Compute weighted average (with mean centering)\n",
    "        sim_sum = np.abs(similarities).sum()\n",
    "        if sim_sum == 0:\n",
    "            return self.user_means[user_idx]\n",
    "        \n",
    "        # Prediction = user_mean + weighted sum of deviations\n",
    "        deviation = ratings - means\n",
    "        pred = self.user_means[user_idx] + np.dot(similarities, deviation) / sim_sum\n",
    "        \n",
    "        # Clip to valid rating range\n",
    "        return np.clip(pred, 1, 5)\n",
    "    \n",
    "    def recommend(self, user_idx, n_recommendations=10):\n",
    "        \"\"\"\n",
    "        Get top N recommendations for a user.\n",
    "        \"\"\"\n",
    "        # Get items user hasn't rated\n",
    "        unrated_mask = self.matrix[user_idx] == 0\n",
    "        unrated_items = np.where(unrated_mask)[0]\n",
    "        \n",
    "        # Predict ratings for all unrated items\n",
    "        predictions = []\n",
    "        for item_idx in unrated_items:\n",
    "            pred = self.predict(user_idx, item_idx)\n",
    "            predictions.append((item_idx, pred))\n",
    "        \n",
    "        # Sort by predicted rating\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return predictions[:n_recommendations]\n",
    "\n",
    "# Train User-Based CF\n",
    "print(\"\\nTraining User-Based CF...\")\n",
    "print(f\"  n_neighbors: 20\")\n",
    "print(f\"  Similarity: Pearson\")\n",
    "\n",
    "user_cf = UserBasedCF(n_neighbors=20, similarity='pearson')\n",
    "user_cf.fit(user_item_matrix)\n",
    "\n",
    "print(\"\\nUser-Based CF ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize user similarity matrix\n",
    "print(\"=\"*70)\n",
    "print(\"USER SIMILARITY MATRIX (Sample)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show subset of similarity matrix\n",
    "n_show = min(30, n_users)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(user_cf.user_sim_matrix[:n_show, :n_show], \n",
    "            cmap='RdYlBu_r', center=0, ax=ax,\n",
    "            xticklabels=5, yticklabels=5)\n",
    "ax.set_title(f'User-User Similarity Matrix (First {n_show} Users)', fontweight='bold')\n",
    "ax.set_xlabel('User Index')\n",
    "ax.set_ylabel('User Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Red = Similar users (positive correlation)\")\n",
    "print(\"  - Blue = Dissimilar users (negative correlation)\")\n",
    "print(\"  - Diagonal = Self-similarity (always 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part7'></a>\n",
    "# Part 7: Item-Based Collaborative Filtering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ITEM-BASED COLLABORATIVE FILTERING\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"ITEM-BASED COLLABORATIVE FILTERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "Algorithm:\n",
    "==========\n",
    "1. Compute similarity between all item pairs\n",
    "2. For prediction: Find items similar to those user rated\n",
    "3. Predict = weighted average based on item similarities\n",
    "\n",
    "Why Item-Based is Often Preferred:\n",
    "  - Items are more stable than users\n",
    "  - Precomputed similarities can be reused\n",
    "  - Better explainability (\"Because you liked X...\")\n",
    "\"\"\")\n",
    "\n",
    "class ItemBasedCF:\n",
    "    \"\"\"\n",
    "    Item-Based Collaborative Filtering from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_neighbors=20, similarity='cosine'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - n_neighbors: Number of similar items to use\n",
    "        - similarity: 'cosine' or 'pearson'\n",
    "        \"\"\"\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.similarity = similarity\n",
    "        self.item_sim_matrix = None\n",
    "        self.matrix = None\n",
    "    \n",
    "    def fit(self, user_item_matrix):\n",
    "        \"\"\"\n",
    "        Compute item-item similarity matrix.\n",
    "        \"\"\"\n",
    "        self.matrix = user_item_matrix.copy()\n",
    "        n_items = self.matrix.shape[1]\n",
    "        \n",
    "        # Transpose for item-based operations\n",
    "        item_matrix = self.matrix.T  # Now rows are items\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        print(f\"Computing {n_items}x{n_items} item similarity matrix...\")\n",
    "        self.item_sim_matrix = np.zeros((n_items, n_items))\n",
    "        \n",
    "        for i in range(n_items):\n",
    "            for j in range(i, n_items):\n",
    "                if i == j:\n",
    "                    self.item_sim_matrix[i, j] = 1.0\n",
    "                else:\n",
    "                    if self.similarity == 'pearson':\n",
    "                        sim = pearson_sim(item_matrix[i], item_matrix[j])\n",
    "                    else:\n",
    "                        sim = cosine_sim(item_matrix[i], item_matrix[j])\n",
    "                    self.item_sim_matrix[i, j] = sim\n",
    "                    self.item_sim_matrix[j, i] = sim\n",
    "        \n",
    "        print(\"Done!\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, user_idx, item_idx):\n",
    "        \"\"\"\n",
    "        Predict rating for a user-item pair.\n",
    "        \"\"\"\n",
    "        # Get items this user has rated\n",
    "        user_ratings = self.matrix[user_idx]\n",
    "        rated_mask = user_ratings > 0\n",
    "        \n",
    "        if not rated_mask.any():\n",
    "            return 3.0  # Fallback to neutral rating\n",
    "        \n",
    "        # Get similarities to items user has rated\n",
    "        similarities = self.item_sim_matrix[item_idx, rated_mask]\n",
    "        ratings = user_ratings[rated_mask]\n",
    "        \n",
    "        # Get top K similar items\n",
    "        if len(similarities) > self.n_neighbors:\n",
    "            top_k_idx = np.argsort(similarities)[-self.n_neighbors:]\n",
    "            similarities = similarities[top_k_idx]\n",
    "            ratings = ratings[top_k_idx]\n",
    "        \n",
    "        # Only use positive similarities\n",
    "        pos_mask = similarities > 0\n",
    "        if not pos_mask.any():\n",
    "            return ratings.mean() if len(ratings) > 0 else 3.0\n",
    "        \n",
    "        similarities = similarities[pos_mask]\n",
    "        ratings = ratings[pos_mask]\n",
    "        \n",
    "        # Weighted average\n",
    "        pred = np.dot(similarities, ratings) / similarities.sum()\n",
    "        \n",
    "        return np.clip(pred, 1, 5)\n",
    "    \n",
    "    def recommend(self, user_idx, n_recommendations=10):\n",
    "        \"\"\"\n",
    "        Get top N recommendations for a user.\n",
    "        \"\"\"\n",
    "        # Get items user hasn't rated\n",
    "        unrated_mask = self.matrix[user_idx] == 0\n",
    "        unrated_items = np.where(unrated_mask)[0]\n",
    "        \n",
    "        # Predict ratings for all unrated items\n",
    "        predictions = []\n",
    "        for item_idx in unrated_items:\n",
    "            pred = self.predict(user_idx, item_idx)\n",
    "            predictions.append((item_idx, pred))\n",
    "        \n",
    "        # Sort by predicted rating\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return predictions[:n_recommendations]\n",
    "    \n",
    "    def get_similar_items(self, item_idx, n=5):\n",
    "        \"\"\"\n",
    "        Get most similar items to a given item.\n",
    "        \"\"\"\n",
    "        similarities = self.item_sim_matrix[item_idx]\n",
    "        # Exclude self\n",
    "        similarities[item_idx] = -1\n",
    "        top_idx = np.argsort(similarities)[-n:][::-1]\n",
    "        return [(idx, similarities[idx]) for idx in top_idx]\n",
    "\n",
    "# Train Item-Based CF\n",
    "print(\"\\nTraining Item-Based CF...\")\n",
    "print(f\"  n_neighbors: 20\")\n",
    "print(f\"  Similarity: Cosine\")\n",
    "\n",
    "item_cf = ItemBasedCF(n_neighbors=20, similarity='cosine')\n",
    "item_cf.fit(user_item_matrix)\n",
    "\n",
    "print(\"\\nItem-Based CF ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize item similarity matrix\n",
    "print(\"=\"*70)\n",
    "print(\"ITEM SIMILARITY MATRIX (Sample)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show subset of similarity matrix\n",
    "n_show = min(30, n_movies)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(item_cf.item_sim_matrix[:n_show, :n_show], \n",
    "            cmap='YlOrRd', ax=ax,\n",
    "            xticklabels=5, yticklabels=5)\n",
    "ax.set_title(f'Item-Item Similarity Matrix (First {n_show} Movies)', fontweight='bold')\n",
    "ax.set_xlabel('Movie Index')\n",
    "ax.set_ylabel('Movie Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show example of similar items\n",
    "print(\"\\nExample: Most similar movies to Movie 0:\")\n",
    "similar = item_cf.get_similar_items(0, n=5)\n",
    "for idx, sim in similar:\n",
    "    print(f\"  Movie {idx}: similarity = {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part8'></a>\n",
    "# Part 8: SVD-Based Recommendations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SVD-BASED RECOMMENDATIONS\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"SVD-BASED RECOMMENDATIONS (Matrix Factorization)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "SVD Approach:\n",
    "=============\n",
    "1. Fill missing values (e.g., with mean)\n",
    "2. Apply SVD: R = U × Σ × V^T\n",
    "3. Keep top k singular values (dimensionality reduction)\n",
    "4. Reconstruct matrix for predictions\n",
    "\n",
    "Key Parameter: k (number of latent factors)\n",
    "  - Small k: Simple model, may underfit\n",
    "  - Large k: Complex model, may overfit\n",
    "  - Typical: 20-100 factors\n",
    "\"\"\")\n",
    "\n",
    "class SVDRecommender:\n",
    "    \"\"\"\n",
    "    SVD-based recommendation system.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_factors=50):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - n_factors: Number of latent factors to keep\n",
    "        \"\"\"\n",
    "        self.n_factors = n_factors\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        self.user_means = None\n",
    "        self.global_mean = None\n",
    "        self.predictions = None\n",
    "    \n",
    "    def fit(self, user_item_matrix):\n",
    "        \"\"\"\n",
    "        Fit SVD model.\n",
    "        \"\"\"\n",
    "        matrix = user_item_matrix.copy()\n",
    "        \n",
    "        # Compute means\n",
    "        self.global_mean = matrix[matrix > 0].mean()\n",
    "        self.user_means = np.zeros(matrix.shape[0])\n",
    "        for i in range(matrix.shape[0]):\n",
    "            rated = matrix[i] > 0\n",
    "            if rated.sum() > 0:\n",
    "                self.user_means[i] = matrix[i, rated].mean()\n",
    "            else:\n",
    "                self.user_means[i] = self.global_mean\n",
    "        \n",
    "        # Fill missing values with user mean\n",
    "        matrix_filled = matrix.copy()\n",
    "        for i in range(matrix.shape[0]):\n",
    "            matrix_filled[i, matrix[i] == 0] = self.user_means[i]\n",
    "        \n",
    "        # Mean-center the matrix\n",
    "        matrix_centered = matrix_filled - self.user_means.reshape(-1, 1)\n",
    "        \n",
    "        # Apply SVD\n",
    "        print(f\"Applying SVD with k={self.n_factors} factors...\")\n",
    "        U, sigma, Vt = svds(sparse.csr_matrix(matrix_centered), k=self.n_factors)\n",
    "        \n",
    "        # Store factors\n",
    "        sigma = np.diag(sigma)\n",
    "        self.user_factors = U\n",
    "        self.sigma = sigma\n",
    "        self.item_factors = Vt\n",
    "        \n",
    "        # Reconstruct matrix for predictions\n",
    "        self.predictions = U @ sigma @ Vt + self.user_means.reshape(-1, 1)\n",
    "        \n",
    "        # Clip predictions to valid range\n",
    "        self.predictions = np.clip(self.predictions, 1, 5)\n",
    "        \n",
    "        print(\"Done!\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, user_idx, item_idx):\n",
    "        \"\"\"\n",
    "        Predict rating for a user-item pair.\n",
    "        \"\"\"\n",
    "        return self.predictions[user_idx, item_idx]\n",
    "    \n",
    "    def recommend(self, user_idx, original_matrix, n_recommendations=10):\n",
    "        \"\"\"\n",
    "        Get top N recommendations for a user.\n",
    "        \"\"\"\n",
    "        # Get items user hasn't rated\n",
    "        unrated_mask = original_matrix[user_idx] == 0\n",
    "        \n",
    "        # Get predictions for unrated items\n",
    "        user_predictions = self.predictions[user_idx].copy()\n",
    "        user_predictions[~unrated_mask] = -np.inf  # Exclude rated items\n",
    "        \n",
    "        # Get top N\n",
    "        top_items = np.argsort(user_predictions)[-n_recommendations:][::-1]\n",
    "        \n",
    "        return [(item, self.predictions[user_idx, item]) for item in top_items]\n",
    "\n",
    "# Train SVD Recommender\n",
    "print(\"\\nTraining SVD Recommender...\")\n",
    "print(f\"  n_factors: 50\")\n",
    "\n",
    "svd_rec = SVDRecommender(n_factors=50)\n",
    "svd_rec.fit(user_item_matrix)\n",
    "\n",
    "print(\"\\nSVD Recommender ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latent factors\n",
    "print(\"=\"*70)\n",
    "print(\"SVD LATENT FACTORS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# User factors (first 2 dimensions)\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(svd_rec.user_factors[:, 0], svd_rec.user_factors[:, 1], \n",
    "           alpha=0.5, c='steelblue', s=20)\n",
    "ax1.set_xlabel('Latent Factor 1')\n",
    "ax1.set_ylabel('Latent Factor 2')\n",
    "ax1.set_title('Users in Latent Space', fontweight='bold')\n",
    "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Item factors (first 2 dimensions)\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(svd_rec.item_factors[0, :], svd_rec.item_factors[1, :], \n",
    "           alpha=0.5, c='coral', s=20)\n",
    "ax2.set_xlabel('Latent Factor 1')\n",
    "ax2.set_ylabel('Latent Factor 2')\n",
    "ax2.set_title('Items in Latent Space', fontweight='bold')\n",
    "ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.suptitle('SVD Latent Factor Visualization (First 2 Factors)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Each point represents a user/item in latent space\")\n",
    "print(\"  - Close points have similar tastes/characteristics\")\n",
    "print(\"  - Factors could represent genres, eras, popularity, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part9'></a>\n",
    "# Part 9: Evaluation & Comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATION METRICS\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "Rating Prediction Metrics:\n",
    "==========================\n",
    "\n",
    "| Metric | Formula | Interpretation |\n",
    "|--------|---------|----------------|\n",
    "| RMSE   | √(Σ(r - r̂)²/n) | Lower is better, penalizes large errors |\n",
    "| MAE    | Σ|r - r̂|/n     | Lower is better, average error |\n",
    "\n",
    "Ranking Metrics:\n",
    "================\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| Precision@K | % of top-K recommendations that are relevant |\n",
    "| Recall@K    | % of relevant items in top-K |\n",
    "| NDCG@K      | Normalized discounted cumulative gain |\n",
    "\"\"\")\n",
    "\n",
    "def evaluate_model(model, test_data, user_to_idx, movie_to_idx, model_name, is_svd=False):\n",
    "    \"\"\"\n",
    "    Evaluate a recommendation model on test data.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    for _, row in test_data.iterrows():\n",
    "        u_idx = user_to_idx.get(row['user_id'])\n",
    "        m_idx = movie_to_idx.get(row['movie_id'])\n",
    "        \n",
    "        if u_idx is not None and m_idx is not None:\n",
    "            pred = model.predict(u_idx, m_idx)\n",
    "            predictions.append(pred)\n",
    "            actuals.append(row['rating'])\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'n_predictions': len(predictions)\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"\\nEvaluating models on test set...\")\n",
    "print(\"(This may take a minute)\\n\")\n",
    "\n",
    "# Sample test data for faster evaluation\n",
    "test_sample = test_data.sample(min(5000, len(test_data)), random_state=42)\n",
    "\n",
    "results = []\n",
    "\n",
    "# User-Based CF\n",
    "print(\"Evaluating User-Based CF...\")\n",
    "results.append(evaluate_model(user_cf, test_sample, user_to_idx, movie_to_idx, \"User-Based CF\"))\n",
    "\n",
    "# Item-Based CF\n",
    "print(\"Evaluating Item-Based CF...\")\n",
    "results.append(evaluate_model(item_cf, test_sample, user_to_idx, movie_to_idx, \"Item-Based CF\"))\n",
    "\n",
    "# SVD\n",
    "print(\"Evaluating SVD...\")\n",
    "results.append(evaluate_model(svd_rec, test_sample, user_to_idx, movie_to_idx, \"SVD (k=50)\"))\n",
    "\n",
    "# Add baseline (global mean)\n",
    "global_mean = train_data['rating'].mean()\n",
    "baseline_rmse = np.sqrt(mean_squared_error(test_sample['rating'], [global_mean] * len(test_sample)))\n",
    "baseline_mae = mean_absolute_error(test_sample['rating'], [global_mean] * len(test_sample))\n",
    "results.append({'Model': 'Baseline (Mean)', 'RMSE': baseline_rmse, 'MAE': baseline_mae, 'n_predictions': len(test_sample)})\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('RMSE')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "print(\"=\"*70)\n",
    "print(\"RESULTS VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "models = results_df['Model']\n",
    "colors = ['steelblue', 'coral', 'green', 'gray']\n",
    "\n",
    "# RMSE comparison\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.barh(models, results_df['RMSE'], color=colors, edgecolor='black')\n",
    "ax1.set_xlabel('RMSE (Lower is Better)')\n",
    "ax1.set_title('RMSE Comparison', fontweight='bold')\n",
    "ax1.axvline(x=baseline_rmse, color='red', linestyle='--', alpha=0.5, label='Baseline')\n",
    "for bar, val in zip(bars1, results_df['RMSE']):\n",
    "    ax1.text(val + 0.02, bar.get_y() + bar.get_height()/2, f'{val:.3f}', va='center')\n",
    "\n",
    "# MAE comparison\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.barh(models, results_df['MAE'], color=colors, edgecolor='black')\n",
    "ax2.set_xlabel('MAE (Lower is Better)')\n",
    "ax2.set_title('MAE Comparison', fontweight='bold')\n",
    "ax2.axvline(x=baseline_mae, color='red', linestyle='--', alpha=0.5, label='Baseline')\n",
    "for bar, val in zip(bars2, results_df['MAE']):\n",
    "    ax2.text(val + 0.02, bar.get_y() + bar.get_height()/2, f'{val:.3f}', va='center')\n",
    "\n",
    "plt.suptitle('Recommendation Model Comparison', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_model = results_df.iloc[0]['Model']\n",
    "best_rmse = results_df.iloc[0]['RMSE']\n",
    "improvement = ((baseline_rmse - best_rmse) / baseline_rmse) * 100\n",
    "\n",
    "print(f\"\\nBest Model: {best_model}\")\n",
    "print(f\"RMSE Improvement over Baseline: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example recommendations\n",
    "print(\"=\"*70)\n",
    "print(\"EXAMPLE RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Pick a random user\n",
    "example_user = 0\n",
    "\n",
    "print(f\"\\nRecommendations for User {example_user}:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Get user's highly rated items\n",
    "user_ratings = user_item_matrix[example_user]\n",
    "high_rated = np.where(user_ratings >= 4)[0][:5]\n",
    "print(f\"\\nItems user rated highly (4+):\")\n",
    "for item_idx in high_rated:\n",
    "    print(f\"  Movie {idx_to_movie[item_idx]}: {user_ratings[item_idx]:.1f} stars\")\n",
    "\n",
    "# Get recommendations from each model\n",
    "print(f\"\\nTop 5 Recommendations:\")\n",
    "\n",
    "print(\"\\n[User-Based CF]\")\n",
    "user_recs = user_cf.recommend(example_user, n_recommendations=5)\n",
    "for item_idx, pred in user_recs:\n",
    "    print(f\"  Movie {idx_to_movie[item_idx]}: predicted {pred:.2f} stars\")\n",
    "\n",
    "print(\"\\n[Item-Based CF]\")\n",
    "item_recs = item_cf.recommend(example_user, n_recommendations=5)\n",
    "for item_idx, pred in item_recs:\n",
    "    print(f\"  Movie {idx_to_movie[item_idx]}: predicted {pred:.2f} stars\")\n",
    "\n",
    "print(\"\\n[SVD]\")\n",
    "svd_recs = svd_rec.recommend(example_user, user_item_matrix, n_recommendations=5)\n",
    "for item_idx, pred in svd_recs:\n",
    "    print(f\"  Movie {idx_to_movie[item_idx]}: predicted {pred:.2f} stars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part10'></a>\n",
    "# Part 10: Summary & Key Takeaways\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"RECOMMENDATION SYSTEM - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "WHAT WE LEARNED:\n",
    "================\n",
    "\n",
    "1. TYPES OF RECOMMENDATION SYSTEMS:\n",
    "   ┌─────────────────┬──────────────────────────────────────┐\n",
    "   │ Type            │ How It Works                         │\n",
    "   ├─────────────────┼──────────────────────────────────────┤\n",
    "   │ Content-Based   │ Match item features to user profile  │\n",
    "   │ Collaborative   │ Use similar users/items              │\n",
    "   │ Hybrid          │ Combine both approaches              │\n",
    "   └─────────────────┴──────────────────────────────────────┘\n",
    "\n",
    "2. COLLABORATIVE FILTERING:\n",
    "   ┌─────────────────┬─────────────┬─────────────────────────┐\n",
    "   │ Method          │ Similarity  │ Best For                │\n",
    "   ├─────────────────┼─────────────┼─────────────────────────┤\n",
    "   │ User-Based CF   │ User-User   │ Few users, many items   │\n",
    "   │ Item-Based CF   │ Item-Item   │ Many users, stable      │\n",
    "   │ SVD             │ Latent      │ Large, sparse matrices  │\n",
    "   └─────────────────┴─────────────┴─────────────────────────┘\n",
    "\n",
    "3. SIMILARITY METRICS:\n",
    "   - Cosine: Good for sparse data, ignores magnitude\n",
    "   - Pearson: Handles user bias, captures correlation\n",
    "   - Jaccard: For binary data (liked/not liked)\n",
    "\n",
    "4. KEY PARAMETERS TO TUNE:\n",
    "   - K (neighbors): 20-50 typically works well\n",
    "   - n_factors (SVD): 20-100 latent dimensions\n",
    "   - Similarity metric: Pearson for users, Cosine for items\n",
    "\n",
    "5. EVALUATION METRICS:\n",
    "   - RMSE: Root Mean Squared Error (penalizes large errors)\n",
    "   - MAE: Mean Absolute Error (average error)\n",
    "   - Precision@K, Recall@K: For ranking quality\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nMODEL COMPARISON SUMMARY:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Algorithm Taxonomy & Model Selection Guide\n\n### When to Use Which Model?\n\n| Scenario | Recommended Model | Why | Alternatives |\n|----------|-------------------|-----|--------------|\n| **Small dataset (<100K ratings)** | User-Based CF | Fast enough, captures user nuance | Item-Based CF |\n| **Large dataset (>1M ratings)** | SVD or Item-Based CF | Scales better | ALS, Neural CF |\n| **Very sparse (>99% empty)** | SVD | Handles sparsity via latent factors | ALS |\n| **Real-time recommendations** | Item-Based CF (precomputed) | Fast lookup | Content-based |\n| **New items frequently** | Content-Based or Hybrid | No cold start | User-Based CF |\n| **Implicit feedback (clicks)** | ALS, BPR | Designed for implicit | Item-Based with Jaccard |\n| **Need explainability** | Item-Based CF | \"Because you liked X...\" | Content-based |\n\n---\n\n## Parameter Tuning Guide\n\n### User-Based & Item-Based CF\n\n| Parameter | Range | Effect | How to Tune |\n|-----------|-------|--------|-------------|\n| **K (neighbors)** | 5-100 | More K = smoother, less personalized | Grid search, start with 20-50 |\n| **Similarity metric** | Pearson/Cosine | Pearson handles bias | Pearson for users, Cosine for items |\n| **Min co-rated items** | 1-10 | Higher = more reliable similarity | Set based on sparsity |\n\n### SVD / Matrix Factorization\n\n| Parameter | Range | Effect | How to Tune |\n|-----------|-------|--------|-------------|\n| **n_factors (k)** | 10-200 | More = captures more patterns, risk overfitting | Cross-validation, typically 50-100 |\n| **Learning rate** | 0.001-0.1 | Higher = faster but unstable | Start 0.005, reduce if diverges |\n| **Regularization (λ)** | 0.01-0.5 | Higher = prevents overfitting | Increase if train >> test error |\n| **Epochs** | 10-100 | More = better fit, slower | Early stopping |\n\n---\n\n## Deep Dive: Common Problems & Solutions\n\n### Cold Start Problem\n\nCold start = new users or items with no interaction history.\n\n| Type | Problem | Solutions |\n|------|---------|-----------|\n| **New User** | No ratings to find similar users | Ask for preferences, use demographics, popularity-based |\n| **New Item** | No one has rated it yet | Content-based features, editorial curation |\n\n**Best approach**: Use a hybrid system - content-based for new items, popularity for new users, then transition to collaborative filtering as we gather data.\n\n---\n\n### Why SVD Works for Sparse Data\n\n1. **Dimensionality reduction**: SVD compresses the sparse matrix into dense latent factors\n2. **Generalization**: Latent factors capture patterns across missing entries\n3. **Implicit matrix completion**: Reconstructed matrix fills in missing values\n\n```\nSparse Matrix (95% empty)    →    SVD    →    Dense Factors\n                                                    ↓\n                                            Reconstruct full matrix\n```\n\n---\n\n### Scaling to Millions of Users\n\n| Strategy | Description |\n|----------|-------------|\n| **Precomputation** | Compute similarities offline, store in cache |\n| **Approximate NN** | Use LSH, Annoy, FAISS for fast similarity search |\n| **Clustering** | Group similar users, recommend within clusters |\n| **Model compression** | Reduce latent factors, quantization |\n| **Distributed computing** | Spark MLlib for ALS at scale |\n\n---\n\n### Evaluation Metrics Deep Dive\n\n| Metric | Type | What It Measures | Use When |\n|--------|------|------------------|----------|\n| **RMSE** | Rating prediction | Prediction accuracy | Explicit ratings |\n| **MAE** | Rating prediction | Average error | Less sensitive to outliers |\n| **Precision@K** | Ranking | % of top-K that are relevant | Implicit feedback |\n| **Recall@K** | Ranking | % of relevant in top-K | Coverage matters |\n| **NDCG** | Ranking | Position-aware quality | Order matters |\n| **Coverage** | Diversity | % of catalog recommended | Avoid popularity bias |\n\n---\n\n### Handling Popularity Bias\n\nPopularity bias = popular items get recommended more, reinforcing their popularity.\n\n| Solution | Description |\n|----------|-------------|\n| **Inverse propensity weighting** | Weight less popular items higher |\n| **Calibration** | Match recommendation distribution to user history |\n| **MMR (Maximal Marginal Relevance)** | Balance relevance with diversity |\n| **Exploration-exploitation** | ε-greedy, Thompson sampling |\n\n---\n\n## Complete Algorithm Comparison\n\n| Algorithm | Type | Complexity | Scalability | Cold Start | Explainable | Best For |\n|-----------|------|------------|-------------|------------|-------------|----------|\n| **User-Based CF** | Memory | O(m²n) | Poor | User problem | Yes | Small datasets |\n| **Item-Based CF** | Memory | O(mn²) | Medium | Item problem | Yes | E-commerce |\n| **SVD** | Model | O(mnk) | Good | Both problems | No | Sparse data |\n| **ALS** | Model | O(mnk) | Excellent | Both problems | No | Implicit feedback |\n| **Content-Based** | Feature | O(nd) | Good | No | Yes | New items |\n| **Neural CF** | Deep Learning | Varies | Excellent | Both problems | No | Large data |\n| **Hybrid** | Combined | Varies | Varies | Reduced | Partial | Production |\n\n---\n\n## Key Libraries\n\n| Library | What It's For | Key Algorithms |\n|---------|---------------|----------------|\n| **Surprise** | Research, prototyping | SVD, KNN, NMF |\n| **LightFM** | Hybrid recommendations | Factorization machines |\n| **Implicit** | Implicit feedback | ALS, BPR |\n| **TensorFlow Recommenders** | Deep learning | Neural CF, Two-Tower |\n| **Spark MLlib** | Distributed computing | ALS at scale |\n\n---\n\n## Checklist\n\n- [x] Understood Content-Based vs Collaborative Filtering trade-offs\n- [x] Know when to use User-Based vs Item-Based CF\n- [x] Understand similarity metrics (Cosine, Pearson, Jaccard) and when to use each\n- [x] Can explain SVD and why it handles sparsity\n- [x] Know how to handle cold start problem\n- [x] Understand evaluation metrics (RMSE, MAE, Precision@K, NDCG)\n- [x] Can discuss scalability solutions\n- [x] Know parameter tuning strategies\n\n---\n\n**End of Recommendation System Tutorial**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}